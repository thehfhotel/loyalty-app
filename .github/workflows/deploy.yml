name: Optimized CI/CD Pipeline with Security & Testing

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:

permissions:
  contents: write  # For GitHub Pages deployment
  pages: write
  id-token: write
  actions: write   # For workflow management
  pull-requests: write  # For PR comments on test results

env:
  NODE_VERSION: 24
  CACHE_VERSION: v2

jobs:
  # =============================================================================
  # PHASE 0: SHARED WORKSPACE PREPARATION (NEW - 30-60 seconds)
  # =============================================================================

  prepare-workspace:
    name: "üöÄ Workspace Preparation"
    runs-on: self-hosted
    timeout-minutes: 2
    outputs:
      workspace-ready: ${{ steps.setup-complete.outputs.ready }}
      backend-cache-hit: ${{ steps.backend-cache.outputs.cache-hit }}
      frontend-cache-hit: ${{ steps.frontend-cache.outputs.cache-hit }}

    steps:
      - name: "üì• Checkout code (shared for all jobs)"
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          clean: true

      - name: "‚ö° Setup Node.js"
        uses: actions/setup-node@v6
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: "üóëÔ∏è Clean up old cache versions"
        run: |
          echo "üóëÔ∏è Removing old cache versions..."
          # Remove all cache directories except current version
          find /tmp/runner-cache -maxdepth 1 -type d ! -name "${{ env.CACHE_VERSION }}" ! -name "runner-cache" ! -name "workspace" -exec rm -rf {} + 2>/dev/null || true
          echo "‚úÖ Old cache versions cleaned up"

      - name: "üßπ Reset Playwright artifacts (shared workspace)"
        run: |
          echo "üßπ Clearing stale Playwright and Allure artifacts..."
          rm -rf allure-results test-results backend/allure-results frontend/allure-results 2>/dev/null || true
          mkdir -p allure-results test-results backend/allure-results frontend/allure-results
          echo "‚úÖ Fresh artifact directories prepared for downstream jobs"

      - name: "üì¶ Setup versioned cache directories"
        run: |
          mkdir -p /tmp/runner-cache/${{ env.CACHE_VERSION }}/npm
          mkdir -p /tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules
          mkdir -p /tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules
          mkdir -p /tmp/runner-cache/${{ env.CACHE_VERSION }}/root-node_modules
          echo "‚úÖ Cache directories prepared (version: ${{ env.CACHE_VERSION }})"

      - name: "üì¶ Restore backend dependencies with hash validation"
        id: backend-cache
        run: |
          CACHE_DIR="/tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules"
          LOCK_HASH=$(sha256sum backend/package-lock.json | cut -d' ' -f1)

          if [ -f "$CACHE_DIR/.lock-hash" ] && [ -d "$CACHE_DIR/node_modules" ]; then
            CACHED_HASH=$(cat "$CACHE_DIR/.lock-hash")
            if [ "$LOCK_HASH" = "$CACHED_HASH" ]; then
              echo "‚úÖ Cache valid - restoring backend node_modules (hash: ${LOCK_HASH:0:12}...)"
              cp -r "$CACHE_DIR/node_modules" ./backend/
              echo "cache-hit=true" >> $GITHUB_OUTPUT
            else
              echo "‚ö†Ô∏è package-lock.json changed - cache invalidated"
              echo "   Old: ${CACHED_HASH:0:12}... ‚Üí New: ${LOCK_HASH:0:12}..."
              rm -rf "$CACHE_DIR"
              mkdir -p "$CACHE_DIR"
              echo "cache-hit=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ùå No valid cache found for backend"
            echo "cache-hit=false" >> $GITHUB_OUTPUT
          fi

      - name: "üì¶ Install backend dependencies"
        if: steps.backend-cache.outputs.cache-hit != 'true'
        working-directory: ./backend
        run: |
          echo "üì¶ Installing backend dependencies (fresh install)..."
          npm ci --cache /tmp/runner-cache/${{ env.CACHE_VERSION }}/npm --prefer-offline --include=dev

          CACHE_DIR="/tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules"
          LOCK_HASH=$(sha256sum package-lock.json | cut -d' ' -f1)

          echo "üíæ Saving backend node_modules to cache..."
          rm -rf "$CACHE_DIR/node_modules"
          cp -r node_modules "$CACHE_DIR/"
          echo "$LOCK_HASH" > "$CACHE_DIR/.lock-hash"
          echo "‚úÖ Cache saved with hash: ${LOCK_HASH:0:12}..."

      - name: "üì¶ Restore frontend dependencies with hash validation"
        id: frontend-cache
        run: |
          CACHE_DIR="/tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules"
          LOCK_HASH=$(sha256sum frontend/package-lock.json | cut -d' ' -f1)

          if [ -f "$CACHE_DIR/.lock-hash" ] && [ -d "$CACHE_DIR/node_modules" ]; then
            CACHED_HASH=$(cat "$CACHE_DIR/.lock-hash")
            if [ "$LOCK_HASH" = "$CACHED_HASH" ]; then
              echo "‚úÖ Cache valid - restoring frontend node_modules (hash: ${LOCK_HASH:0:12}...)"
              cp -r "$CACHE_DIR/node_modules" ./frontend/
              echo "cache-hit=true" >> $GITHUB_OUTPUT
            else
              echo "‚ö†Ô∏è package-lock.json changed - cache invalidated"
              echo "   Old: ${CACHED_HASH:0:12}... ‚Üí New: ${LOCK_HASH:0:12}..."
              rm -rf "$CACHE_DIR"
              mkdir -p "$CACHE_DIR"
              echo "cache-hit=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ùå No valid cache found for frontend"
            echo "cache-hit=false" >> $GITHUB_OUTPUT
          fi

      - name: "üì¶ Install frontend dependencies"
        if: steps.frontend-cache.outputs.cache-hit != 'true'
        working-directory: ./frontend
        run: |
          echo "üì¶ Installing frontend dependencies (fresh install)..."
          npm ci --cache /tmp/runner-cache/${{ env.CACHE_VERSION }}/npm --prefer-offline

          CACHE_DIR="/tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules"
          LOCK_HASH=$(sha256sum package-lock.json | cut -d' ' -f1)

          echo "üíæ Saving frontend node_modules to cache..."
          rm -rf "$CACHE_DIR/node_modules"
          cp -r node_modules "$CACHE_DIR/"
          echo "$LOCK_HASH" > "$CACHE_DIR/.lock-hash"
          echo "‚úÖ Cache saved with hash: ${LOCK_HASH:0:12}..."

      - name: "üì¶ Restore root dependencies with hash validation"
        id: root-cache
        run: |
          CACHE_DIR="/tmp/runner-cache/${{ env.CACHE_VERSION }}/root-node_modules"
          LOCK_HASH=$(sha256sum package-lock.json | cut -d' ' -f1)

          if [ -f "$CACHE_DIR/.lock-hash" ] && [ -d "$CACHE_DIR/node_modules" ]; then
            CACHED_HASH=$(cat "$CACHE_DIR/.lock-hash")
            if [ "$LOCK_HASH" = "$CACHED_HASH" ]; then
              echo "‚úÖ Cache valid - restoring root node_modules (hash: ${LOCK_HASH:0:12}...)"
              cp -r "$CACHE_DIR/node_modules" ./
              echo "cache-hit=true" >> $GITHUB_OUTPUT
            else
              echo "‚ö†Ô∏è package-lock.json changed - cache invalidated"
              echo "   Old: ${CACHED_HASH:0:12}... ‚Üí New: ${LOCK_HASH:0:12}..."
              rm -rf "$CACHE_DIR"
              mkdir -p "$CACHE_DIR"
              echo "cache-hit=false" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ùå No valid cache found for root"
            echo "cache-hit=false" >> $GITHUB_OUTPUT
          fi

      - name: "üì¶ Install root dependencies (Playwright)"
        if: steps.root-cache.outputs.cache-hit != 'true'
        run: |
          echo "üì¶ Installing root dependencies (Playwright - fresh install)..."
          npm ci --cache /tmp/runner-cache/${{ env.CACHE_VERSION }}/npm --prefer-offline

          CACHE_DIR="/tmp/runner-cache/${{ env.CACHE_VERSION }}/root-node_modules"
          LOCK_HASH=$(sha256sum package-lock.json | cut -d' ' -f1)

          echo "‚úÖ Root dependencies installed (includes @playwright/test)"
          echo "üíæ Saving root node_modules to cache..."
          rm -rf "$CACHE_DIR/node_modules"
          cp -r node_modules "$CACHE_DIR/"
          echo "$LOCK_HASH" > "$CACHE_DIR/.lock-hash"
          echo "‚úÖ Cache saved with hash: ${LOCK_HASH:0:12}..."

      - name: "üîß Generate Prisma client (shared)"
        working-directory: ./backend
        run: |
          echo "üîß Generating Prisma client for all jobs..."
          npm run db:generate

          if [ ! -d "src/generated/prisma" ]; then
            echo "‚ùå Prisma client generation failed"
            exit 1
          fi

          echo "‚úÖ Prisma client generated successfully"

      - name: "üìä Workspace preparation complete"
        id: setup-complete
        run: |
          echo "‚úÖ Workspace preparation completed"
          echo "Backend cache hit: ${{ steps.backend-cache.outputs.cache-hit }}"
          echo "Frontend cache hit: ${{ steps.frontend-cache.outputs.cache-hit }}"
          echo "ready=true" >> $GITHUB_OUTPUT

      - name: "üì§ Copy workspace to shared location"
        run: |
          SHARED_WORKSPACE="/tmp/runner-cache/workspace/${{ github.run_id }}"
          echo "üì¶ Creating shared workspace directory at: $SHARED_WORKSPACE"
          mkdir -p "$SHARED_WORKSPACE"

          echo "üìã Copying workspace files (excluding .git, node_modules, and backup files)..."
          rsync -av --exclude='.git' \
                    --exclude='node_modules' \
                    --exclude='backend/node_modules' \
                    --exclude='frontend/node_modules' \
                    --exclude='*.backup' \
                    --exclude='*.spec.ts.backup' \
                    --exclude='*.test.ts.backup' \
                    . "$SHARED_WORKSPACE/"

          echo "‚úÖ Workspace copied to shared location"
          echo "workspace-path=$SHARED_WORKSPACE" >> $GITHUB_OUTPUT

  # =============================================================================
  # PHASE 1: PARALLEL VALIDATION & SECURITY (3-4 minutes)
  # =============================================================================

  # Job 1A: Code Quality & Security Analysis (Parallel)
  security-analysis:
    name: "üîí Security & Code Quality"
    runs-on: self-hosted
    needs: prepare-workspace
    timeout-minutes: 5
    outputs:
      security-passed: ${{ steps.security-results.outputs.passed }}
      audit-passed: ${{ steps.security-audit.outputs.passed }}

    steps:
      - name: "üì• Copy shared workspace from local storage"
        run: |
          SHARED_WORKSPACE="/tmp/runner-cache/workspace/${{ github.run_id }}"
          echo "üì¶ Copying workspace from shared location: $SHARED_WORKSPACE"

          if [ ! -d "$SHARED_WORKSPACE" ]; then
            echo "‚ùå CRITICAL: Shared workspace not found at $SHARED_WORKSPACE"
            echo "This job depends on the prepare-workspace job completing successfully"
            exit 1
          fi

          echo "üìã Copying workspace files..."
          rsync -av "$SHARED_WORKSPACE/" .

          echo "‚úÖ Workspace copied from shared location"

      - name: "‚ö° Setup Node.js"
        uses: actions/setup-node@v6
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: "üì¶ Restore backend dependencies from local cache"
        id: backend-cache-restore
        run: |
          if [ -d "/tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/node_modules" ]; then
            echo "‚úÖ Restoring backend node_modules from local cache..."
            cp -r /tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/node_modules ./backend/
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå CRITICAL: Backend cache missing - should have been prepared by prepare-workspace job"
            exit 1
          fi

      - name: "üì¶ Restore frontend dependencies from local cache"
        id: frontend-cache-restore
        run: |
          if [ -d "/tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/node_modules" ]; then
            echo "‚úÖ Restoring frontend node_modules from local cache..."
            cp -r /tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/node_modules ./frontend/
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå CRITICAL: Frontend cache missing - should have been prepared by prepare-workspace job"
            exit 1
          fi

      - name: "üîç TypeScript type checking - Backend (validation)"
        working-directory: ./backend
        run: npm run typecheck

      - name: "üîç TypeScript type checking - Frontend (validation)"
        working-directory: ./frontend
        run: npm run typecheck

      - name: "üîí Security linting (ESLint + Security rules)"
        working-directory: ./backend
        run: |
          echo "üîí Running security linting with ESLint..."
          npm run lint:security 2>&1 | tee lint-output.txt

          # Extract error count from summary line (e.g., "‚úñ 166 problems (5 errors, 161 warnings)")
          ERROR_COUNT=$(grep -oP '‚úñ.*\(\K[0-9]+(?= error)' lint-output.txt || echo "0")

          if [ "$ERROR_COUNT" -gt 0 ]; then
            echo "‚ùå ESLint found $ERROR_COUNT errors - build must fail"
            cat lint-output.txt | grep "error"
            rm -f lint-output.txt
            exit 1
          else
            echo "‚úÖ No ESLint errors found (warnings are acceptable)"
            rm -f lint-output.txt
          fi

      - name: "üõ°Ô∏è Security audit (npm audit)"
        working-directory: ./backend
        run: |
          echo "üõ°Ô∏è Running npm audit (allowed exception: GHSA-5j98-mcp5-4vw2)..."
          set +e
          npm audit --audit-level=moderate --json > audit-report.json
          AUDIT_EXIT=$?
          set -e

          if [ "$AUDIT_EXIT" -eq 0 ]; then
            echo "‚úÖ npm audit passed with no vulnerabilities"
            rm -f audit-report.json
            exit 0
          fi

          echo "‚ö†Ô∏è npm audit reported vulnerabilities, evaluating against approved exceptions..."
          if node <<'EOF'
            const fs = require('fs');
            const reportPath = 'audit-report.json';
            const allowedCves = ['GHSA-5j98-mcp5-4vw2'];
            const data = JSON.parse(fs.readFileSync(reportPath, 'utf8'));
            const vulnerabilities = data.vulnerabilities || {};
            const memo = new Map();

            const hasAllowedAdvisory = (item) => {
              const id = item.id || item.source || '';
              const url = item.url || '';
              return allowedCves.some((allowed) => String(id).includes(allowed) || url.includes(allowed));
            };

            const isPackageAllowed = (pkg, seen = new Set()) => {
              if (memo.has(pkg)) {
                return memo.get(pkg);
              }
              if (seen.has(pkg)) {
                return true;
              }
              const entry = vulnerabilities[pkg];
              if (!entry) {
                memo.set(pkg, false);
                return false;
              }
              const via = entry.via || [];
              if (via.length === 0) {
                memo.set(pkg, false);
                return false;
              }

              seen.add(pkg);
              let allowed = true;
              for (const v of via) {
                if (typeof v === 'string') {
                  if (!isPackageAllowed(v, seen)) {
                    allowed = false;
                    break;
                  }
                } else if (!hasAllowedAdvisory(v)) {
                  allowed = false;
                  break;
                }
              }
              seen.delete(pkg);
              memo.set(pkg, allowed);
              return allowed;
            };

            const unapproved = Object.keys(vulnerabilities).filter((pkg) => !isPackageAllowed(pkg));

            if (unapproved.length === 0) {
              console.log('‚ö†Ô∏è npm audit only reported allowed advisories (GHSA-5j98-mcp5-4vw2). Accepted risk documented.');
              process.exit(0);
            }

            console.error('‚ùå Unapproved advisories found:');
            console.error(JSON.stringify(unapproved, null, 2));
            process.exit(1);
          EOF
          then
            rm -f audit-report.json
            exit 0
          else
            echo "‚ùå npm audit detected unapproved vulnerabilities. See audit-report.json for details."
            cat audit-report.json
            rm -f audit-report.json
            exit 1
          fi

      - name: "üîí Run custom security validation"
        working-directory: ./backend
        run: node scripts/validate-security.js

      - name: "üõ°Ô∏è Test integrity validation"
        run: |
          echo "üõ°Ô∏è Running test integrity validation..."
          if [ -f "./scripts/validate-test-integrity.sh" ]; then
            ./scripts/validate-test-integrity.sh
          else
            echo "‚ö†Ô∏è Test integrity script not found - skipping"
          fi

      - name: "üîê Environment & Secrets Validation"
        id: environment-validation
        run: |
          echo "üîê Validating environment configuration and secrets..."
          if [ -f "./scripts/validate-environment.sh" ]; then
            # Run in build mode (skip runtime checks like Redis/DB connectivity)
            if ./scripts/validate-environment.sh build; then
              echo "‚úÖ Environment validation passed"
              echo "env-validation-passed=true" >> $GITHUB_OUTPUT
            else
              echo "‚ùå Environment validation failed - check for exposed secrets or misconfiguration"
              echo "env-validation-passed=false" >> $GITHUB_OUTPUT
              exit 1
            fi
          else
            echo "‚ö†Ô∏è Environment validation script not found - skipping"
            echo "env-validation-passed=skipped" >> $GITHUB_OUTPUT
          fi

      - name: "üîê Enhanced Security Audit (20-User Prototype)"
        id: security-audit
        run: |
          echo "üîê Running comprehensive security audit..."
          if ./scripts/security-audit.sh; then
            echo "‚úÖ Security audit passed"
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Security audit failed - see details above"
            echo "passed=false" >> $GITHUB_OUTPUT
            exit 1
          fi

      - name: "üìä Security results summary (reporting only)"
        id: security-results
        if: always()
        run: |
          echo "Security analysis completed"
          echo "passed=true" >> $GITHUB_OUTPUT

  # Job 1B: Unit Tests (Parallel with Integration Tests)
  unit-tests:
    name: "üß™ Unit Tests"
    runs-on: self-hosted
    needs: [prepare-workspace, security-analysis]
    timeout-minutes: 4
    # Prevent multiple unit test jobs from running concurrently (queue instead)
    concurrency:
      group: unit-tests
      cancel-in-progress: false
    outputs:
      tests-passed: ${{ steps.test-results.outputs.passed }}
      coverage-percent: ${{ steps.test-results.outputs.coverage }}

    steps:
      - name: "üì• Copy shared workspace from local storage"
        run: |
          SHARED_WORKSPACE="/tmp/runner-cache/workspace/${{ github.run_id }}"
          echo "üì¶ Copying workspace from shared location: $SHARED_WORKSPACE"

          if [ ! -d "$SHARED_WORKSPACE" ]; then
            echo "‚ùå CRITICAL: Shared workspace not found at $SHARED_WORKSPACE"
            echo "This job depends on the prepare-workspace job completing successfully"
            exit 1
          fi

          echo "üìã Copying workspace files..."
          rsync -av "$SHARED_WORKSPACE/" .

          echo "‚úÖ Workspace copied from shared location"

      - name: "‚ö° Setup Node.js"
        uses: actions/setup-node@v6
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: "üì¶ Restore backend dependencies from local cache"
        id: backend-cache-restore
        run: |
          if [ -d "/tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/node_modules" ]; then
            echo "‚úÖ Restoring backend node_modules from local cache..."
            cp -r /tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/node_modules ./backend/
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå CRITICAL: Backend cache missing - should have been prepared by prepare-workspace job"
            exit 1
          fi

      - name: "üì¶ Restore frontend dependencies from local cache"
        id: frontend-cache-restore
        run: |
          if [ -d "/tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/node_modules" ]; then
            echo "‚úÖ Restoring frontend node_modules from local cache..."
            cp -r /tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/node_modules ./frontend/
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå CRITICAL: Frontend cache missing - should have been prepared by prepare-workspace job"
            exit 1
          fi

      - name: "üê≥ Start isolated test services"
        run: |
          # Use unique project name and ports to avoid conflicts with dev/E2E tests
          export COMPOSE_PROJECT_NAME="loyalty-unit-tests"
          export POSTGRES_PORT="5438"
          export REDIS_PORT="6383"

          # Start test services with isolated configuration
          echo "üöÄ Starting isolated test services on custom ports..."

          # Create temporary compose file for unit tests with isolated ports
          cat > docker-compose.unit-test.yml << 'EOF'
          version: '3.8'
          services:
            postgres:
              image: postgres:15-alpine
              container_name: loyalty_postgres_unit
              environment:
                POSTGRES_USER: loyalty
                POSTGRES_PASSWORD: loyalty_password
                POSTGRES_DB: loyalty_db
              ports:
                - "5438:5432"
              volumes:
                - loyalty_postgres_unit_data:/var/lib/postgresql/data
              healthcheck:
                test: ["CMD-SHELL", "pg_isready -U loyalty -d loyalty_db"]
                interval: 5s
                timeout: 5s
                retries: 5

            redis:
              image: redis:7-alpine
              container_name: loyalty_redis_unit
              ports:
                - "6383:6379"
              volumes:
                - loyalty_redis_unit_data:/data
              healthcheck:
                test: ["CMD", "redis-cli", "ping"]
                interval: 5s
                timeout: 3s
                retries: 5

          volumes:
            loyalty_postgres_unit_data:
            loyalty_redis_unit_data:
          EOF

          # Start isolated test services
          docker compose -f docker-compose.unit-test.yml up -d

          # Wait for services to be ready with health checks
          echo "‚è≥ Waiting for isolated test services to be ready..."
          timeout 60 bash -c 'until docker compose -f docker-compose.unit-test.yml exec -T postgres pg_isready -U loyalty -d loyalty_db; do echo "Waiting for isolated postgres..."; sleep 2; done'
          timeout 30 bash -c 'until docker compose -f docker-compose.unit-test.yml exec -T redis redis-cli ping | grep -q PONG; do echo "Waiting for isolated redis..."; sleep 2; done'

          # Configure isolated test database
          echo "üóÑÔ∏è Setting up isolated test database..."
          docker exec loyalty_postgres_unit psql -U loyalty -d postgres -c "DROP DATABASE IF EXISTS loyalty_test_db;" || true
          docker exec loyalty_postgres_unit psql -U loyalty -d postgres -c "DROP USER IF EXISTS loyalty_test;" || true
          docker exec loyalty_postgres_unit psql -U loyalty -d postgres -c "CREATE USER loyalty_test WITH PASSWORD 'test_password';" || true
          docker exec loyalty_postgres_unit psql -U loyalty -d postgres -c "CREATE DATABASE loyalty_test_db OWNER loyalty_test;" || true
          docker exec loyalty_postgres_unit psql -U loyalty -d postgres -c "GRANT ALL PRIVILEGES ON DATABASE loyalty_test_db TO loyalty_test;" || true

          # Enable UUID extension in isolated test database
          docker exec loyalty_postgres_unit psql -U loyalty_test -d loyalty_test_db -c "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";" || true

          echo "‚úÖ Isolated test services are ready on ports 5438 (postgres) and 6383 (redis)"
      
      - name: "üîß Setup isolated test environment"
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://loyalty_test:test_password@localhost:5438/loyalty_test_db
          REDIS_URL: redis://localhost:6383
          NODE_ENV: test
          JWT_SECRET: test-jwt-secret-for-testing-only-must-be-64-chars-minimum-xxxxxx
          JWT_REFRESH_SECRET: test-refresh-secret-for-testing-only-must-be-64-chars-minimum-xx
          SESSION_SECRET: test-session-secret-for-testing-only-min-32-chars
        run: |
          # Generate Prisma client for tests
          npm run db:generate
          
          # Validate Prisma client was generated successfully
          if [ ! -d "src/generated/prisma" ]; then
            echo "‚ùå Prisma client generation failed - directory not found"
            exit 1
          fi
          
          if [ ! -f "src/generated/prisma/index.js" ]; then
            echo "‚ùå Prisma client generation failed - index.js not found"
            exit 1
          fi
          
          echo "‚úÖ Prisma client validated successfully"
          
          # Run database migrations for test DB
          npm run db:migrate:deploy
      
      - name: "üß™ Run unit tests (isolated)"
        id: run-tests
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://loyalty_test:test_password@localhost:5438/loyalty_test_db
          REDIS_URL: redis://localhost:6383
          NODE_ENV: test
          JWT_SECRET: test-jwt-secret-for-testing-only-must-be-64-chars-minimum-xxxxxx
          JWT_REFRESH_SECRET: test-refresh-secret-for-testing-only-must-be-64-chars-minimum-xx
          SESSION_SECRET: test-session-secret-for-testing-only-min-32-chars
        run: npm run test:unit -- --coverage --passWithNoTests

      - name: "üìä Upload coverage report"
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: coverage-report-${{ github.run_id }}
          path: backend/coverage/
          retention-days: 30
          if-no-files-found: warn
          compression-level: 6

      - name: "üìä Upload unit test Allure results"
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: allure-results-unit-${{ github.run_id }}
          path: backend/allure-results/
          retention-days: 7
          if-no-files-found: warn

      - name: "üìà Test results summary (reporting only)"
        id: test-results
        if: always()
        run: |
          # Capture test outcome as shell variable
          TEST_OUTCOME="${{ steps.run-tests.outcome }}"

          # Check if tests actually passed
          if [ "$TEST_OUTCOME" == "success" ]; then
            echo "‚úÖ Unit tests passed"
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Unit tests failed"
            echo "passed=false" >> $GITHUB_OUTPUT
          fi

          # Extract and display coverage metrics
          if [ -f backend/coverage/coverage-summary.json ]; then
            # Extract coverage metrics from JSON summary (accurate totals across all files)
            LINES_FOUND=$(jq '.total.lines.total' backend/coverage/coverage-summary.json)
            LINES_HIT=$(jq '.total.lines.covered' backend/coverage/coverage-summary.json)
            COVERAGE_PERCENT=$(jq '.total.lines.pct' backend/coverage/coverage-summary.json)

            # Additional metrics for comprehensive reporting
            STATEMENTS_TOTAL=$(jq '.total.statements.total' backend/coverage/coverage-summary.json)
            STATEMENTS_COVERED=$(jq '.total.statements.covered' backend/coverage/coverage-summary.json)
            STATEMENTS_PCT=$(jq '.total.statements.pct' backend/coverage/coverage-summary.json)

            FUNCTIONS_TOTAL=$(jq '.total.functions.total' backend/coverage/coverage-summary.json)
            FUNCTIONS_COVERED=$(jq '.total.functions.covered' backend/coverage/coverage-summary.json)
            FUNCTIONS_PCT=$(jq '.total.functions.pct' backend/coverage/coverage-summary.json)

            BRANCHES_TOTAL=$(jq '.total.branches.total' backend/coverage/coverage-summary.json)
            BRANCHES_COVERED=$(jq '.total.branches.covered' backend/coverage/coverage-summary.json)
            BRANCHES_PCT=$(jq '.total.branches.pct' backend/coverage/coverage-summary.json)

            echo "coverage=$COVERAGE_PERCENT" >> $GITHUB_OUTPUT

            # Display coverage summary
            echo "üìä Coverage Summary:"
            echo "  Lines:      $LINES_HIT / $LINES_FOUND ($COVERAGE_PERCENT%)"
            echo "  Statements: $STATEMENTS_COVERED / $STATEMENTS_TOTAL ($STATEMENTS_PCT%)"
            echo "  Functions:  $FUNCTIONS_COVERED / $FUNCTIONS_TOTAL ($FUNCTIONS_PCT%)"
            echo "  Branches:   $BRANCHES_COVERED / $BRANCHES_TOTAL ($BRANCHES_PCT%)"
            echo "  Coverage report: Available in workflow artifacts"

            # Add to job summary for GitHub UI
            {
              echo "## üìä Test Coverage Report"
              echo ""
              echo "| Metric | Covered | Total | Percentage |"
              echo "|--------|---------|-------|------------|"
              echo "| **Lines** | $LINES_HIT | $LINES_FOUND | $COVERAGE_PERCENT% |"
              echo "| **Statements** | $STATEMENTS_COVERED | $STATEMENTS_TOTAL | $STATEMENTS_PCT% |"
              echo "| **Functions** | $FUNCTIONS_COVERED | $FUNCTIONS_TOTAL | $FUNCTIONS_PCT% |"
              echo "| **Branches** | $BRANCHES_COVERED | $BRANCHES_TOTAL | $BRANCHES_PCT% |"
              echo ""
              echo "| **Test Status** | $TEST_OUTCOME |"
              echo ""
              echo "### üì• Download Reports"
              echo "Coverage reports are available as workflow artifacts for 30 days."
              echo "- HTML Report: \`backend/coverage/lcov-report/index.html\`"
              echo "- LCOV Data: \`backend/coverage/lcov.info\`"
              echo "- JSON Summary: \`backend/coverage/coverage-summary.json\`"
            } >> $GITHUB_STEP_SUMMARY
          else
            echo "coverage=0" >> $GITHUB_OUTPUT
            echo "‚ö†Ô∏è No coverage data available"
          fi

      - name: "üí¨ Comment unit test results on PR"
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const fs = require('fs');
            const testPassed = '${{ steps.test-results.outputs.passed }}' === 'true';
            const statusEmoji = testPassed ? '‚úÖ' : '‚ùå';
            const statusText = testPassed ? 'Passed' : 'Failed';

            let coverageSection = '';
            const coveragePath = 'backend/coverage/coverage-summary.json';
            if (fs.existsSync(coveragePath)) {
              const coverageData = JSON.parse(fs.readFileSync(coveragePath, 'utf8'));
              const total = coverageData.total;
              coverageSection = [
                '',
                '### üìä Coverage',
                '| Metric | Covered | Total | Percentage |',
                '|--------|---------|-------|------------|',
                `| Lines | ${total.lines.covered} | ${total.lines.total} | ${total.lines.pct}% |`,
                `| Functions | ${total.functions.covered} | ${total.functions.total} | ${total.functions.pct}% |`,
                `| Branches | ${total.branches.covered} | ${total.branches.total} | ${total.branches.pct}% |`,
              ].join('\n');
            }

            const commentLines = [
              '## üß™ CI Test Results',
              '',
              '| Stage | Status |',
              '|-------|--------|',
              `| Unit Tests | ${statusEmoji} ${statusText} |`,
              '| Integration Tests | ‚è≥ Pending |',
              '| E2E Tests | ‚è≥ Pending |',
              coverageSection,
              '',
              testPassed
                ? '> Sequential pipeline: Integration tests starting next...'
                : '> ‚ùå Unit tests failed - pipeline stopped.',
              '',
              `> Commit: \`${context.sha.substring(0, 7)}\` | [View Artifacts](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`,
            ];

            const comment = commentLines.join('\n');

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            // Use consistent marker to track same comment across test stages
            const COMMENT_MARKER = '<!-- CI-TEST-RESULTS-COMMENT -->';
            const fullComment = COMMENT_MARKER + '\n' + comment;

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes(COMMENT_MARKER)
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: fullComment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: fullComment
              });
            }

      - name: "üßπ Cleanup isolated test environment"
        if: always()
        run: |
          echo "üßπ Cleaning up isolated unit test services..."

          # Stop and remove containers by explicit names (doesn't require compose file)
          docker stop loyalty_postgres_unit loyalty_redis_unit 2>/dev/null || true
          docker rm -f loyalty_postgres_unit loyalty_redis_unit 2>/dev/null || true

          # Remove volumes (check both with and without project prefix)
          docker volume rm loyalty_postgres_unit_data loyalty_redis_unit_data 2>/dev/null || true
          docker volume rm loyalty-unit-tests_loyalty_postgres_unit_data loyalty-unit-tests_loyalty_redis_unit_data 2>/dev/null || true

          # Clean up compose file
          rm -f docker-compose.unit-test.yml || true

          echo "‚úÖ Isolated unit test environment cleaned up"

  # Job 1C: Integration Tests (Sequential - after Unit Tests)
  integration-tests:
    name: "üîó Integration Tests"
    runs-on: self-hosted
    needs: [unit-tests]  # Sequential: runs after unit tests pass
    timeout-minutes: 5
    # Prevent multiple integration test jobs from running concurrently (queue instead)
    concurrency:
      group: integration-tests
      cancel-in-progress: false
    outputs:
      tests-passed: ${{ steps.test-results.outputs.passed }}

    steps:
      - name: "üì• Copy shared workspace from local storage"
        run: |
          SHARED_WORKSPACE="/tmp/runner-cache/workspace/${{ github.run_id }}"
          echo "üì¶ Copying workspace from shared location: $SHARED_WORKSPACE"

          if [ ! -d "$SHARED_WORKSPACE" ]; then
            echo "‚ùå CRITICAL: Shared workspace not found at $SHARED_WORKSPACE"
            echo "This job depends on the prepare-workspace job completing successfully"
            exit 1
          fi

          echo "üìã Copying workspace files..."
          rsync -av "$SHARED_WORKSPACE/" .

          echo "‚úÖ Workspace copied from shared location"

      - name: "‚ö° Setup Node.js"
        uses: actions/setup-node@v6
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: "üì¶ Restore backend dependencies from local cache"
        id: backend-cache-restore
        run: |
          if [ -d "/tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/node_modules" ]; then
            echo "‚úÖ Restoring backend node_modules from local cache..."
            cp -r /tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/node_modules ./backend/
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå CRITICAL: Backend cache missing - should have been prepared by prepare-workspace job"
            exit 1
          fi

      - name: "üì¶ Restore frontend dependencies from local cache"
        id: frontend-cache-restore
        run: |
          if [ -d "/tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/node_modules" ]; then
            echo "‚úÖ Restoring frontend node_modules from local cache..."
            cp -r /tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/node_modules ./frontend/
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå CRITICAL: Frontend cache missing - should have been prepared by prepare-workspace job"
            exit 1
          fi

      - name: "üê≥ Start isolated test services"
        run: |
          # Use unique project name and ports to avoid conflicts with Unit and E2E tests
          export COMPOSE_PROJECT_NAME="loyalty-integration-tests"
          export POSTGRES_PORT="5437"
          export REDIS_PORT="6382"

          # Start test services with isolated configuration
          echo "üöÄ Starting isolated integration test services on custom ports..."

          # Create temporary compose file for integration tests with isolated ports
          cat > docker-compose.integration-test.yml << 'EOF'
          version: '3.8'
          services:
            postgres:
              image: postgres:15-alpine
              container_name: loyalty_postgres_integration
              environment:
                POSTGRES_USER: loyalty
                POSTGRES_PASSWORD: loyalty_password
                POSTGRES_DB: loyalty_db
              ports:
                - "5437:5432"
              volumes:
                - loyalty_postgres_integration_data:/var/lib/postgresql/data
              healthcheck:
                test: ["CMD-SHELL", "pg_isready -U loyalty -d loyalty_db"]
                interval: 5s
                timeout: 5s
                retries: 5

            redis:
              image: redis:7-alpine
              container_name: loyalty_redis_integration
              ports:
                - "6382:6379"
              volumes:
                - loyalty_redis_integration_data:/data
              healthcheck:
                test: ["CMD", "redis-cli", "ping"]
                interval: 5s
                timeout: 3s
                retries: 5

          volumes:
            loyalty_postgres_integration_data:
            loyalty_redis_integration_data:
          EOF

          # Start isolated test services
          docker compose -f docker-compose.integration-test.yml up -d

          # Wait for services to be ready with health checks
          echo "‚è≥ Waiting for isolated integration test services to be ready..."
          timeout 60 bash -c 'until docker compose -f docker-compose.integration-test.yml exec -T postgres pg_isready -U loyalty -d loyalty_db; do echo "Waiting for isolated postgres..."; sleep 2; done'
          timeout 30 bash -c 'until docker compose -f docker-compose.integration-test.yml exec -T redis redis-cli ping | grep -q PONG; do echo "Waiting for isolated redis..."; sleep 2; done'

          # Configure isolated test database
          echo "üóÑÔ∏è Setting up isolated integration test database..."
          docker exec loyalty_postgres_integration psql -U loyalty -d postgres -c "DROP DATABASE IF EXISTS loyalty_integration_db;" || true
          docker exec loyalty_postgres_integration psql -U loyalty -d postgres -c "DROP USER IF EXISTS loyalty_integration;" || true
          docker exec loyalty_postgres_integration psql -U loyalty -d postgres -c "CREATE USER loyalty_integration WITH PASSWORD 'integration_password';" || true
          docker exec loyalty_postgres_integration psql -U loyalty -d postgres -c "CREATE DATABASE loyalty_integration_db OWNER loyalty_integration;" || true
          docker exec loyalty_postgres_integration psql -U loyalty -d postgres -c "GRANT ALL PRIVILEGES ON DATABASE loyalty_integration_db TO loyalty_integration;" || true

          # Enable UUID extension in isolated test database
          docker exec loyalty_postgres_integration psql -U loyalty_integration -d loyalty_integration_db -c "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";" || true

          echo "‚úÖ Isolated integration test services are ready on ports 5437 (postgres) and 6382 (redis)"

      - name: "üîß Setup isolated integration test environment"
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://loyalty_integration:integration_password@localhost:5437/loyalty_integration_db
          REDIS_URL: redis://localhost:6382
          NODE_ENV: test
          JWT_SECRET: test-jwt-secret-for-testing-only-must-be-64-chars-minimum-xxxxxx
          JWT_REFRESH_SECRET: test-refresh-secret-for-testing-only-must-be-64-chars-minimum-xx
          SESSION_SECRET: test-session-secret-for-testing-only-min-32-chars
        run: |
          # Generate Prisma client for tests
          npm run db:generate

          # Validate Prisma client was generated successfully
          if [ ! -d "src/generated/prisma" ]; then
            echo "‚ùå Prisma client generation failed - directory not found"
            exit 1
          fi

          if [ ! -f "src/generated/prisma/index.js" ]; then
            echo "‚ùå Prisma client generation failed - index.js not found"
            exit 1
          fi

          echo "‚úÖ Prisma client validated successfully"

          # Run database migrations for integration test DB
          npm run db:migrate:deploy

      - name: "üîó Run integration tests (isolated)"
        id: run-integration-tests
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://loyalty_integration:integration_password@localhost:5437/loyalty_integration_db
          REDIS_URL: redis://localhost:6382
          NODE_ENV: test
          JWT_SECRET: test-jwt-secret-for-testing-only-must-be-64-chars-minimum-xxxxxx
          JWT_REFRESH_SECRET: test-refresh-secret-for-testing-only-must-be-64-chars-minimum-xx
          SESSION_SECRET: test-session-secret-for-testing-only-min-32-chars
        run: npm run test:integration

      - name: "üìä Upload integration test Allure results"
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: allure-results-integration-${{ github.run_id }}
          path: backend/allure-results/
          retention-days: 7
          if-no-files-found: warn

      - name: "üß™ Run database schema tests (isolated)"
        id: run-db-tests
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://loyalty_integration:integration_password@localhost:5437/loyalty_integration_db
          NODE_ENV: test
        run: npm run test:db -- --passWithNoTests

      - name: "üîê OAuth validation tests"
        run: |
          echo "üîê Running OAuth validation tests..."
          # Check if services are running
          if curl -s http://localhost:4001/api/health >/dev/null 2>&1; then
            echo "‚úÖ Backend service is running, performing OAuth validation"
            if [ -f "./scripts/validate-oauth-health.sh" ]; then
              ./scripts/validate-oauth-health.sh || echo "‚ö†Ô∏è OAuth validation failed (non-blocking in CI)"
            else
              echo "‚ö†Ô∏è OAuth validation script not found - skipping"
            fi
          else
            echo "‚ö†Ô∏è Backend not running - skipping OAuth validation"
          fi

      - name: "üóÑÔ∏è Database migration validation (isolated)"
        env:
          DATABASE_URL: postgresql://loyalty_integration:integration_password@localhost:5437/loyalty_integration_db
          NODE_ENV: test
        run: |
          echo "üóÑÔ∏è Running database migration validation on isolated environment..."
          if [ -f "./scripts/validate-db-migration.sh" ]; then
            ./scripts/validate-db-migration.sh || echo "‚ö†Ô∏è DB migration validation failed (non-blocking in CI)"
          else
            echo "‚ö†Ô∏è Database migration validation script not found - skipping"
          fi

      - name: "üìà Test results summary (reporting only)"
        id: test-results
        if: always()
        run: |
          # Check if all critical integration tests passed
          INTEGRATION_RESULT="${{ steps.run-integration-tests.outcome }}"
          DB_TEST_RESULT="${{ steps.run-db-tests.outcome }}"

          if [ "$INTEGRATION_RESULT" == "success" ] && [ "$DB_TEST_RESULT" == "success" ]; then
            echo "‚úÖ All integration tests passed"
            echo "passed=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå Integration tests failed"
            echo "Integration tests: $INTEGRATION_RESULT"
            echo "DB schema tests: $DB_TEST_RESULT"
            echo "passed=false" >> $GITHUB_OUTPUT
            echo "üö® BLOCKING: Integration tests must pass for deployment"
            exit 1
          fi

      - name: "üí¨ Comment integration results on PR"
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const integrationResult = '${{ steps.run-integration-tests.outcome }}';
            const dbTestResult = '${{ steps.run-db-tests.outcome }}';
            const allPassed = integrationResult === 'success' && dbTestResult === 'success';

            const statusEmoji = allPassed ? '‚úÖ' : '‚ùå';
            const statusText = allPassed ? 'Passed' : 'Failed';
            const integrationEmoji = integrationResult === 'success' ? '‚úÖ' : '‚ùå';
            const dbEmoji = dbTestResult === 'success' ? '‚úÖ' : '‚ùå';

            const comment = [
              '## üß™ CI Test Results',
              '',
              '| Stage | Status |',
              '|-------|--------|',
              '| Unit Tests | ‚úÖ Passed |',
              `| Integration Tests | ${statusEmoji} ${statusText} |`,
              '| E2E Tests | ‚è≥ Pending |',
              '',
              '### üîó Integration Details',
              '| Test Suite | Status |',
              '|:-----------|:-------|',
              `| API Integration | ${integrationEmoji} ${integrationResult === 'success' ? 'Passed' : 'Failed'} |`,
              `| DB Schema Tests | ${dbEmoji} ${dbTestResult === 'success' ? 'Passed' : 'Failed'} |`,
              '',
              allPassed ? '> Sequential pipeline: E2E tests starting next...' : '> ‚ùå Integration tests failed - pipeline stopped.',
              '',
              `> Commit: \`${context.sha.substring(0, 7)}\` | [View Artifacts](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`,
            ].join('\n');

            const COMMENT_MARKER = '<!-- CI-TEST-RESULTS-COMMENT -->';
            const fullComment = COMMENT_MARKER + '\n' + comment;

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes(COMMENT_MARKER)
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: fullComment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: fullComment
              });
            }

      - name: "üßπ Cleanup isolated integration test environment"
        if: always()
        run: |
          echo "üßπ Cleaning up isolated integration test services..."

          # Stop and remove containers by explicit names (doesn't require compose file)
          docker stop loyalty_postgres_integration loyalty_redis_integration 2>/dev/null || true
          docker rm -f loyalty_postgres_integration loyalty_redis_integration 2>/dev/null || true

          # Remove volumes (check both with and without project prefix)
          docker volume rm loyalty_postgres_integration_data loyalty_redis_integration_data 2>/dev/null || true
          docker volume rm loyalty-integration-tests_loyalty_postgres_integration_data loyalty-integration-tests_loyalty_redis_integration_data 2>/dev/null || true

          # Clean up compose file
          rm -f docker-compose.integration-test.yml || true

          echo "‚úÖ Isolated integration test environment cleaned up"

  # Job 1D: E2E Tests (Conditional - only on main branch or PR to main)
  e2e-tests:
    name: "üé≠ E2E Tests"
    runs-on: self-hosted
    env:
      COMPOSE_PROJECT_NAME: loyalty-e2e-tests

    # Prevent parallel E2E runs to avoid port conflicts with host network
    concurrency:
      group: e2e-tests
      cancel-in-progress: false

    needs: [integration-tests]  # Sequential: runs after integration tests pass
    timeout-minutes: 20  # Increased from 8: allows ~15min for 36 tests + 3-5min infrastructure setup
    if: (github.ref == 'refs/heads/main' || github.base_ref == 'main')
    outputs:
      e2e-passed: ${{ steps.e2e-results.outputs.passed }}

    steps:
      - name: "üì• Copy shared workspace from local storage"
        run: |
          SHARED_WORKSPACE="/tmp/runner-cache/workspace/${{ github.run_id }}"
          echo "üì¶ Copying workspace from shared location: $SHARED_WORKSPACE"

          if [ ! -d "$SHARED_WORKSPACE" ]; then
            echo "‚ùå CRITICAL: Shared workspace not found at $SHARED_WORKSPACE"
            echo "This job depends on the prepare-workspace job completing successfully"
            exit 1
          fi

          echo "üìã Copying workspace files..."
          rsync -av "$SHARED_WORKSPACE/" .

          echo "‚úÖ Workspace copied from shared location"

      - name: "‚ö° Setup Node.js"
        uses: actions/setup-node@v6
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: "üì¶ Restore root dependencies (Playwright) from local cache"
        id: root-cache-restore
        run: |
          if [ -d "/tmp/runner-cache/${{ env.CACHE_VERSION }}/root-node_modules/node_modules" ]; then
            echo "‚úÖ Restoring root node_modules from local cache..."
            cp -r /tmp/runner-cache/${{ env.CACHE_VERSION }}/root-node_modules/node_modules ./
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå CRITICAL: Root cache missing - should have been prepared by prepare-workspace job"
            exit 1
          fi

      - name: "üì¶ Restore backend dependencies from local cache"
        id: backend-cache-restore
        run: |
          if [ -d "/tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/node_modules" ]; then
            echo "‚úÖ Restoring backend node_modules from local cache..."
            cp -r /tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/node_modules ./backend/
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå CRITICAL: Backend cache missing - should have been prepared by prepare-workspace job"
            exit 1
          fi

      - name: "üì¶ Restore frontend dependencies from local cache"
        id: frontend-cache-restore
        run: |
          if [ -d "/tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/node_modules" ]; then
            echo "‚úÖ Restoring frontend node_modules from local cache..."
            cp -r /tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/node_modules ./frontend/
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå CRITICAL: Frontend cache missing - should have been prepared by prepare-workspace job"
            exit 1
          fi

      - name: "üßπ Cleanup E2E ports"
        run: |
          echo "üßπ Cleaning up E2E ports before starting tests..."
          # Kill any processes using E2E ports
          for port in 5436 6381 3201 4202; do
            pid=$(lsof -ti:$port 2>/dev/null || true)
            if [ ! -z "$pid" ]; then
              echo "‚ö†Ô∏è Killing process on port $port (PID: $pid)"
              kill -9 $pid 2>/dev/null || true
            fi
          done

          # Wait for ports to be released
          sleep 2
          echo "‚úÖ Port cleanup completed"

      - name: "üîß Setup isolated E2E environment"
        run: |
          # Use unique project name and ports to avoid conflicts with unit tests
          export COMPOSE_PROJECT_NAME="loyalty-e2e-tests"
          export POSTGRES_PORT="5436"
          export REDIS_PORT="6381"
          export BACKEND_PORT="4202"
          export FRONTEND_PORT="3201"

          echo "üöÄ Setting up isolated E2E environment on custom ports..."

          # Database services will be started with the complete compose file after build
      
      - name: "üèóÔ∏è Build and start isolated application for E2E"
        env:
          DATABASE_URL: postgresql://loyalty:loyalty_password@localhost:5436/loyalty_db
          REDIS_URL: redis://localhost:6381
          NODE_ENV: development
          JWT_SECRET: e2e-jwt-secret-for-testing-only-must-be-64-chars-minimum-xxxxxxx
          JWT_REFRESH_SECRET: e2e-refresh-secret-for-testing-only-must-be-64-chars-minimum-xxx
          SESSION_SECRET: e2e-session-secret-for-testing-only-min-32-chars
          FRONTEND_URL: http://localhost:3201
          BACKEND_URL: http://localhost:4202
          VITE_API_URL: http://localhost:4202/api
        run: |
          # Validate Prisma client from workspace (already generated)
          echo "üîç Validating Prisma client from shared workspace..."
          if [ ! -d "backend/src/generated/prisma" ]; then
            echo "‚ùå Prisma client not found - workspace preparation may have failed"
            exit 1
          fi

          echo "‚úÖ Prisma client validated successfully"

          # Critical: Remove E2E database volumes to ensure clean state
          echo "üßπ Removing E2E database volumes to prevent migration state corruption..."
          docker volume rm loyalty_postgres_e2e_data loyalty_redis_e2e_data 2>/dev/null || true
          docker volume rm loyalty-e2e-tests_loyalty_postgres_e2e_data loyalty-e2e-tests_loyalty_redis_e2e_data 2>/dev/null || true
          
          # Build backend and frontend
          echo "üèóÔ∏è Building application for isolated E2E testing..."
          (cd backend && npm run build) &
          (cd frontend && npm run build) &
          wait
          

      - name: "üîÑ E2E Environment Setup and Database Migration"
        working-directory: .
        env:
          DATABASE_URL: postgresql://loyalty:loyalty_password@localhost:5436/loyalty_db
          NODE_ENV: development
          # OAuth Configuration for E2E tests (from GitHub Secrets)
          GOOGLE_CLIENT_ID: ${{ secrets.GOOGLE_CLIENT_ID || 'test-google-not-configured' }}
          GOOGLE_CLIENT_SECRET: ${{ secrets.GOOGLE_CLIENT_SECRET || 'test-google-secret-not-configured' }}
          LINE_CHANNEL_ID: ${{ secrets.LINE_CHANNEL_ID || 'test-line-not-configured' }}
          LINE_CHANNEL_SECRET: ${{ secrets.LINE_CHANNEL_SECRET || 'test-line-secret-not-configured' }}
        run: |
          echo "üöÄ Starting isolated application stack for E2E tests..."

          # Create the E2E Docker Compose file with host networking
          echo "üìù Creating E2E Docker Compose configuration with host network..."
          cat > docker-compose.e2e.ci.yml << EOF
          services:
            postgres:
              image: postgres:15-alpine
              container_name: loyalty_postgres_e2e
              network_mode: host
              environment:
                POSTGRES_USER: loyalty
                POSTGRES_PASSWORD: loyalty_password
                POSTGRES_DB: loyalty_db
                PGPORT: 5436
              tmpfs:
                - /dev/shm:size=256m
              volumes:
                - loyalty_postgres_e2e_data:/var/lib/postgresql/data
              healthcheck:
                test: ["CMD-SHELL", "pg_isready -U loyalty -d loyalty_db -p 5436"]
                interval: 5s
                timeout: 5s
                retries: 5
                start_period: 10s

            redis:
              image: redis:7-alpine
              container_name: loyalty_redis_e2e
              network_mode: host
              command: redis-server --port 6381
              volumes:
                - loyalty_redis_e2e_data:/data
              healthcheck:
                test: ["CMD", "redis-cli", "-p", "6381", "ping"]
                interval: 5s
                timeout: 3s
                retries: 5

            backend:
              build:
                context: ./backend
                dockerfile: Dockerfile
                target: development
              container_name: loyalty_backend_e2e
              network_mode: host
              environment:
                NODE_ENV: development
                PORT: 4202
                DATABASE_URL: postgresql://loyalty:loyalty_password@localhost:5436/loyalty_db
                REDIS_URL: redis://localhost:6381
                JWT_SECRET: e2e-jwt-secret-for-testing-only-must-be-64-chars-minimum-xxxxxxx
                JWT_REFRESH_SECRET: e2e-refresh-secret-for-testing-only-must-be-64-chars-minimum-xxx
                SESSION_SECRET: e2e-session-secret-for-testing-only-min-32-chars
                FRONTEND_URL: http://localhost:3201
                BACKEND_URL: http://localhost:4202
                CORS_ORIGINS: "http://localhost:3201"
                # OAuth Configuration (from GitHub Secrets)
                GOOGLE_CLIENT_ID: ${GOOGLE_CLIENT_ID}
                GOOGLE_CLIENT_SECRET: ${GOOGLE_CLIENT_SECRET}
                GOOGLE_CALLBACK_URL: http://localhost:4202/api/oauth/google/callback
                LINE_CHANNEL_ID: ${LINE_CHANNEL_ID}
                LINE_CHANNEL_SECRET: ${LINE_CHANNEL_SECRET}
                LINE_CALLBACK_URL: http://localhost:4202/api/oauth/line/callback
              depends_on:
                postgres:
                  condition: service_healthy
                redis:
                  condition: service_healthy
              healthcheck:
                test: ["CMD-SHELL", "curl -f http://localhost:4202/api/health || exit 1"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 30s

            frontend:
              build:
                context: ./frontend
                dockerfile: Dockerfile
                target: development
                args:
                  VITE_API_URL: http://localhost:4202/api
              container_name: loyalty_frontend_e2e
              network_mode: host
              environment:
                NODE_ENV: development
                PORT: 3201
              depends_on:
                backend:
                  condition: service_healthy
              healthcheck:
                test: ["CMD-SHELL", "curl -f http://localhost:3201 || exit 1"]
                interval: 10s
                timeout: 5s
                retries: 3

          volumes:
            loyalty_postgres_e2e_data:
              driver: local
            loyalty_redis_e2e_data:
              driver: local
          EOF
          
          echo "‚úÖ E2E Docker Compose configuration created"
          
          # Use project-specific compose teardown to avoid stopping production containers
          echo "üßπ Cleaning up any existing E2E containers and ports..."
          docker compose -p loyalty-e2e-tests -f docker-compose.e2e.ci.yml down -v --remove-orphans 2>/dev/null || true
          
          # Check for processes using E2E ports and kill them
          echo "üîç Checking for processes using E2E ports..."
          for port in 3201 4202 5436 6381; do
            pid=$(lsof -ti:$port 2>/dev/null || true)
            if [ ! -z "$pid" ]; then
              echo "‚ö†Ô∏è Found process $pid using port $port, terminating..."
              kill -9 $pid 2>/dev/null || true
              sleep 1
            fi
          done
          
          # Verify ports are free
          echo "‚úÖ Port cleanup completed"
          netstat -tlnp | grep -E ":320[1-9]|:420[2-9]|:543[6-9]|:638[1-9]" || echo "No conflicting ports found"
          
          # Start containers and capture any startup failures
          if docker compose -f docker-compose.e2e.ci.yml up -d --build; then
            echo "‚úÖ Docker containers started successfully"
          else
            echo "‚ùå Docker containers failed to start, capturing logs..."
            echo "üìÑ Backend container logs:"
            docker logs loyalty_backend_e2e 2>&1 || echo "No backend logs available"
            echo "üìÑ Frontend container logs:"
            docker logs loyalty_frontend_e2e 2>&1 || echo "No frontend logs available"
            echo "üìÑ PostgreSQL container logs:"
            docker logs loyalty_postgres_e2e 2>&1 || echo "No postgres logs available"
            echo "üìÑ Redis container logs:"
            docker logs loyalty_redis_e2e 2>&1 || echo "No redis logs available"
            echo "üìä Container status:"
            docker compose -f docker-compose.e2e.ci.yml ps
            exit 1
          fi
          
          # Wait for application to be ready
          echo "‚è≥ Waiting for isolated application to be ready..."
          
          # Wait for backend with simplified retry logic 
          echo "üîç Waiting for backend to be ready..."
          BACKEND_READY=false
          for i in {1..60}; do
            echo "Attempt $i/60: Testing backend connection..."
            if curl -s http://localhost:4202/api/health >/dev/null 2>&1; then
              echo "‚úÖ Backend is responding!"
              BACKEND_READY=true
              break
            fi
            echo "Backend not ready yet, waiting 5 seconds..."
            sleep 5
          done
          
          if [ "$BACKEND_READY" = false ]; then
            echo "‚ùå Backend failed to become ready after 5 minutes (60 attempts)"
            echo "üîç Final connection test:"
            curl -v http://localhost:4202/api/health || echo "Final curl test failed"
            echo "üìÑ Backend container logs:"
            docker logs loyalty_backend_e2e 2>&1 || echo "No backend logs available"
            echo "üìä Container status:"
            docker compose -f docker-compose.e2e.ci.yml ps
            echo "üåê Port connectivity test:"
            netstat -tlnp | grep :4202 || echo "Port 4202 not found in netstat"
            exit 1
          fi
          
          # Wait for frontend with simplified retry logic
          echo "üîç Waiting for frontend to be ready..."
          FRONTEND_READY=false
          for i in {1..40}; do
            echo "Attempt $i/40: Testing frontend connection..."
            if curl -s http://localhost:3201 >/dev/null 2>&1; then
              echo "‚úÖ Frontend is responding!"
              FRONTEND_READY=true
              break
            fi
            echo "Frontend not ready yet, waiting 3 seconds..."
            sleep 3
          done
          
          if [ "$FRONTEND_READY" = false ]; then
            echo "‚ùå Frontend failed to become ready after 2 minutes (40 attempts)"
            echo "üîç Final frontend connection test:"
            curl -v http://localhost:3201 || echo "Final frontend curl test failed"
            echo "üìÑ Frontend container logs:"
            docker logs loyalty_frontend_e2e 2>&1 || echo "No frontend logs available"
            echo "üìä Container status:"
            docker compose -f docker-compose.e2e.ci.yml ps
            echo "üåê Frontend port connectivity test:"
            netstat -tlnp | grep :3201 || echo "Port 3201 not found in netstat"
            exit 1
          fi
          
          echo "‚úÖ Isolated application stack is ready for E2E testing"
          echo "üåê Backend: http://localhost:4202/api/health"
          echo "üåê Frontend: http://localhost:3201"

          # Network diagnostics: Check what interfaces backend is listening on
          echo ""
          echo "üîç Network Diagnostics:"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"

          echo "üì° Port 4202 listening status (host network):"
          ss -tlnp | grep :4202 || netstat -tlnp | grep :4202 || echo "‚ö†Ô∏è Port 4202 not found in host network"

          echo ""
          echo "üê≥ Backend container network info:"
          docker exec loyalty_backend_e2e sh -c 'echo "Inside container - listening ports:" && netstat -tlnp 2>/dev/null || ss -tlnp' | grep -E "4202|State" || echo "‚ö†Ô∏è Could not get container network info"

          echo ""
          echo "üåê Testing connectivity from different contexts:"
          echo "  Host curl to localhost:4202:"
          curl -s -o /dev/null -w "    Status: %{http_code}, Time: %{time_total}s\n" http://localhost:4202/api/health || echo "    ‚ùå Failed"

          echo "  Host curl to 127.0.0.1:4202:"
          curl -s -o /dev/null -w "    Status: %{http_code}, Time: %{time_total}s\n" http://127.0.0.1:4202/api/health || echo "    ‚ùå Failed"

          echo "  Host curl to 0.0.0.0:4202:"
          curl -s -o /dev/null -w "    Status: %{http_code}, Time: %{time_total}s\n" http://0.0.0.0:4202/api/health || echo "    ‚ùå Failed"

          echo ""
          echo "üîß Backend container inspection:"
          echo "  Container IP (if bridged):"
          docker inspect loyalty_backend_e2e --format='    {{range .NetworkSettings.Networks}}{{.IPAddress}}{{end}}' || echo "    N/A (host network mode)"

          echo "  Network mode:"
          docker inspect loyalty_backend_e2e --format='    {{.HostConfig.NetworkMode}}'

          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo ""

      - name: "üé≠ Run E2E tests with Playwright (isolated)"
        id: run-e2e-tests
        env:
          NODE_ENV: test
          DATABASE_URL: postgresql://loyalty:loyalty_password@localhost:5436/loyalty_db
          REDIS_URL: redis://localhost:6381
          BACKEND_URL: http://localhost:4202
          FRONTEND_URL: http://localhost:3201
          SKIP_PLAYWRIGHT_DOCKER_SETUP: "true"
          # GitHub Secrets for CI/CD environment validation test
          JWT_SECRET: ${{ secrets.JWT_SECRET }}
          JWT_REFRESH_SECRET: ${{ secrets.JWT_REFRESH_SECRET }}
          SESSION_SECRET: ${{ secrets.SESSION_SECRET }}
          GOOGLE_CLIENT_ID: ${{ secrets.GOOGLE_CLIENT_ID }}
          GOOGLE_CLIENT_SECRET: ${{ secrets.GOOGLE_CLIENT_SECRET }}
          LINE_CHANNEL_ID: ${{ secrets.LINE_CHANNEL_ID }}
          LINE_CHANNEL_SECRET: ${{ secrets.LINE_CHANNEL_SECRET }}
        run: |
          echo "üé≠ Running E2E tests with Playwright on isolated environment..."
          echo "üåê Testing against Backend: http://localhost:4202"
          echo "üåê Testing against Frontend: http://localhost:3201"

          # Verify Playwright is available
          if [ ! -d "node_modules/@playwright" ]; then
            echo "‚ùå @playwright/test not found in node_modules"
            echo "üìÇ Root node_modules contents:"
            ls -la node_modules/ 2>&1 | head -20 || echo "node_modules directory not found"
            exit 1
          fi

          echo "‚úÖ @playwright/test found in node_modules"

          # Install Playwright browsers if needed
          echo "üì• Installing Playwright browsers..."
          npx playwright install --with-deps chromium

          # Verify Playwright installation
          echo "üîç Verifying Playwright installation..."
          npx playwright --version

          # Playwright connectivity diagnostic
          echo ""
          echo "üß™ Playwright Connectivity Diagnostic:"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          node -e 'const http = require("http"); const urls = ["http://localhost:4202/api/health", "http://127.0.0.1:4202/api/health", "http://0.0.0.0:4202/api/health"]; console.log("Testing connectivity from Node.js (same context as Playwright):"); let completed = 0; urls.forEach(url => { const urlObj = new URL(url); const options = { hostname: urlObj.hostname, port: urlObj.port, path: urlObj.pathname, method: "GET", timeout: 5000 }; const req = http.request(options, (res) => { console.log(`  ‚úÖ ${url}: HTTP ${res.statusCode}`); if (++completed === urls.length) process.exit(0); }); req.on("error", (e) => { console.log(`  ‚ùå ${url}: ${e.code} - ${e.message}`); if (++completed === urls.length) process.exit(0); }); req.on("timeout", () => { console.log(`  ‚è±Ô∏è ${url}: Timeout`); req.destroy(); if (++completed === urls.length) process.exit(0); }); req.end(); }); setTimeout(() => process.exit(0), 7000);' || echo "‚ö†Ô∏è Node.js connectivity test failed"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo ""

          # CRITICAL: Verify containers are still running RIGHT before test execution
          echo "üîç Pre-Test Container Status Check:"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          docker compose -f docker-compose.e2e.ci.yml ps
          echo ""
          echo "üìã Backend logs (last 30 lines):"
          docker compose -f docker-compose.e2e.ci.yml logs --tail=30 backend
          echo ""
          echo "üè• Final health check before tests:"
          curl -f http://localhost:4202/api/health || echo "‚ùå Backend health check FAILED before test execution!"
          echo "‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ"
          echo ""

          # Abort early if the isolated stack has disappeared
          RUNNING_SERVICES=$(docker compose -f docker-compose.e2e.ci.yml ps --services --filter "status=running" | tr '\n' ' ' | xargs)
          if [ -z "$RUNNING_SERVICES" ] || ! echo "$RUNNING_SERVICES" | grep -q "backend"; then
            echo "‚ùå Isolated E2E stack is not running. Capturing diagnostics before aborting..."
            echo "üìä docker compose ps --all"
            docker compose -f docker-compose.e2e.ci.yml ps -a || true
            echo "üì¶ docker ps -a (host)"
            docker ps -a || true
            echo "üìÑ Backend logs:"
            docker logs loyalty_backend_e2e 2>&1 || echo "No backend logs available"
            echo "üìÑ Frontend logs:"
            docker logs loyalty_frontend_e2e 2>&1 || echo "No frontend logs available"
            echo "üìÑ PostgreSQL logs:"
            docker logs loyalty_postgres_e2e 2>&1 || echo "No postgres logs available"
            echo "üìÑ Redis logs:"
            docker logs loyalty_redis_e2e 2>&1 || echo "No redis logs available"
            exit 1
          fi

          # Run all E2E tests against isolated environment (BLOCKING - must pass to succeed)
          echo "üé≠ Running E2E tests - failures will BLOCK the pipeline..."
          npx playwright test

      - name: "üóÑÔ∏è Database migration rollback safety"
        run: |
          echo "üóÑÔ∏è Checking migration rollback safety..."
          if [ -f "./scripts/migration-rollback-safety.sh" ]; then
            ./scripts/migration-rollback-safety.sh check || echo "‚ö†Ô∏è Migration rollback check failed (non-blocking in CI)"
          else
            echo "‚ö†Ô∏è Migration rollback safety script not found - skipping"
          fi
      
      - name: "üìä E2E results summary"
        id: e2e-results
        if: success()
        run: |
          echo "‚úÖ E2E tests completed successfully"
          echo "passed=true" >> $GITHUB_OUTPUT

      - name: "üìä E2E results summary (failure)"
        id: e2e-results-failure
        if: failure()
        run: |
          echo "‚ùå E2E tests failed"
          echo "passed=false" >> $GITHUB_OUTPUT
          exit 1

      - name: "üìä Upload E2E test Allure results"
        if: always()
        uses: actions/upload-artifact@v5
        with:
          name: allure-results-e2e-${{ github.run_id }}
          path: allure-results/
          retention-days: 7
          if-no-files-found: warn

      - name: "üí¨ Comment E2E results on PR"
        if: always() && github.event_name == 'pull_request'
        uses: actions/github-script@v7
        continue-on-error: true
        with:
          script: |
            const e2eResult = '${{ steps.run-e2e-tests.outcome }}';
            const e2ePassed = e2eResult === 'success';

            const statusEmoji = e2ePassed ? '‚úÖ' : '‚ùå';
            const statusText = e2ePassed ? 'Passed' : 'Failed';

            const comment = [
              '## üß™ CI Test Results',
              '',
              '| Stage | Status |',
              '|-------|--------|',
              '| Unit Tests | ‚úÖ Passed |',
              '| Integration Tests | ‚úÖ Passed |',
              `| E2E Tests | ${statusEmoji} ${statusText} |`,
              '',
              '### üé≠ E2E Details',
              '| Test Suite | Status |',
              '|:-----------|:-------|',
              `| Playwright E2E | ${statusEmoji} ${statusText} |`,
              '',
              e2ePassed
                ? '> ‚úÖ All tests passed! Ready for build and deployment.'
                : '> ‚ùå E2E tests failed - review test results.',
              '',
              `> Commit: \`${context.sha.substring(0, 7)}\` | [View Artifacts](https://github.com/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId})`,
              '',
              'üìä [View Allure Report](https://github.com/${{ github.repository }}/actions/runs/${{ github.run_id }})',
            ].join('\n');

            const COMMENT_MARKER = '<!-- CI-TEST-RESULTS-COMMENT -->';
            const fullComment = COMMENT_MARKER + '\n' + comment;

            const { data: comments } = await github.rest.issues.listComments({
              owner: context.repo.owner,
              repo: context.repo.repo,
              issue_number: context.issue.number,
            });

            const botComment = comments.find(c =>
              c.user.type === 'Bot' && c.body.includes(COMMENT_MARKER)
            );

            if (botComment) {
              await github.rest.issues.updateComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                comment_id: botComment.id,
                body: fullComment
              });
            } else {
              await github.rest.issues.createComment({
                owner: context.repo.owner,
                repo: context.repo.repo,
                issue_number: context.issue.number,
                body: fullComment
              });
            }

      - name: "üßπ Cleanup isolated E2E environment"
        if: always()
        run: |
          echo "üßπ Cleaning up isolated E2E environment..."
          docker compose -f docker-compose.e2e.ci.yml down -v --remove-orphans || true

          # SAFETY: Only remove test-specific volumes, NEVER docker system prune
          # docker system prune can destroy production containers/volumes!
          echo "üßπ Removing E2E volumes to prevent future migration state issues..."
          docker volume rm loyalty_postgres_e2e_data loyalty_redis_e2e_data 2>/dev/null || true
          docker volume rm loyalty-e2e-tests_loyalty_postgres_e2e_data loyalty-e2e-tests_loyalty_redis_e2e_data 2>/dev/null || true

          rm -f docker-compose.e2e.ci.yml || true
          echo "‚úÖ Isolated E2E environment cleaned up"

  # =============================================================================
  # PHASE 2: BUILD & DEPLOYMENT PREPARATION (2-3 minutes, only on main)
  # =============================================================================
  
  # Job 2A: Build validation (only for main branch, after all tests)
  build-validation:
    name: "üèóÔ∏è Build Validation & Docker Images"
    runs-on: self-hosted
    needs: [security-analysis, unit-tests, integration-tests, e2e-tests]
    if: github.ref == 'refs/heads/main' && always() && needs.security-analysis.outputs.security-passed == 'true' && needs.unit-tests.outputs.tests-passed == 'true' && needs.integration-tests.outputs.tests-passed == 'true' && needs.e2e-tests.outputs.e2e-passed == 'true'
    timeout-minutes: 15
    outputs:
      build-passed: ${{ steps.build-results.outputs.passed }}
      backend-image: ${{ steps.docker-build.outputs.backend_image }}
      frontend-image: ${{ steps.docker-build.outputs.frontend_image }}
      images-ready: ${{ steps.docker-build.outputs.images_ready }}
    
    steps:
      - name: "üßπ Workspace preparation"
        run: |
          # Clean workspace with passwordless sudo (now configured)
          echo "Cleaning workspace at ${{ github.workspace }}"
          
          cd ${{ github.workspace }}
          
          # Force clean the problematic directories with sudo
          echo "Removing build artifacts with elevated permissions..."
          sudo rm -rf frontend/dist backend/dist 2>/dev/null || true
          sudo rm -rf frontend/build backend/build 2>/dev/null || true
          sudo rm -rf frontend/.next backend/.next 2>/dev/null || true
          sudo rm -rf node_modules frontend/node_modules backend/node_modules 2>/dev/null || true
          
          # Clean git repository state
          if [ -d ".git" ]; then
            echo "Cleaning git repository state..."
            sudo git clean -xffd 2>/dev/null || true
            git reset --hard 2>/dev/null || true
          fi
          
          # Fix ownership of remaining files to current user
          sudo chown -R $USER:$USER . 2>/dev/null || true
          
          echo "Workspace after cleanup:"
          ls -la
      
      - name: "üì• Checkout code"
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          clean: false  # Don't try to clean, we already did that
      
      - name: "‚ö° Setup Node.js"
        uses: actions/setup-node@v6
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: "üì¶ Setup local cache directories"
        run: |
          mkdir -p /tmp/runner-cache/${{ env.CACHE_VERSION }}/npm
          mkdir -p /tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules
          mkdir -p /tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules
          echo "‚úÖ Cache directories prepared"

      - name: "üì¶ Restore backend node_modules from local cache"
        id: backend-cache
        run: |
          if [ -d "/tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/node_modules" ]; then
            echo "‚úÖ Restoring backend node_modules from local cache..."
            cp -r /tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/node_modules ./backend/
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå No backend cache found"
            echo "cache-hit=false" >> $GITHUB_OUTPUT
          fi

      - name: "üì¶ Install backend dependencies"
        if: steps.backend-cache.outputs.cache-hit != 'true'
        working-directory: ./backend
        run: |
          echo "üì¶ Installing backend dependencies with local npm cache..."
          npm ci --cache /tmp/runner-cache/${{ env.CACHE_VERSION }}/npm --prefer-offline --include=dev
          echo "üíæ Saving backend node_modules to local cache..."
          rm -rf /tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/node_modules
          cp -r node_modules /tmp/runner-cache/${{ env.CACHE_VERSION }}/backend-node_modules/

      - name: "üì¶ Restore frontend node_modules from local cache"
        id: frontend-cache
        run: |
          if [ -d "/tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/node_modules" ]; then
            echo "‚úÖ Restoring frontend node_modules from local cache..."
            cp -r /tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/node_modules ./frontend/
            echo "cache-hit=true" >> $GITHUB_OUTPUT
          else
            echo "‚ùå No frontend cache found"
            echo "cache-hit=false" >> $GITHUB_OUTPUT
          fi

      - name: "üì¶ Install frontend dependencies"
        if: steps.frontend-cache.outputs.cache-hit != 'true'
        working-directory: ./frontend
        run: |
          echo "üì¶ Installing frontend dependencies with local npm cache..."
          npm ci --cache /tmp/runner-cache/${{ env.CACHE_VERSION }}/npm --prefer-offline
          echo "üíæ Saving frontend node_modules to local cache..."
          rm -rf /tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/node_modules
          cp -r node_modules /tmp/runner-cache/${{ env.CACHE_VERSION }}/frontend-node_modules/
      
      - name: "üîß Generate Prisma client (build phase)"
        run: |
          echo "üîß Generating Prisma client for TypeScript compilation..."
          cd backend && npm run db:generate
          echo "‚úÖ Prisma client generated successfully"
      
      - name: "üèóÔ∏è Build applications (parallel)"
        run: |
          # Backend build
          cd backend && npm run build:prod &
          BACKEND_PID=$!
          
          # Frontend build  
          cd frontend && npm run build &
          FRONTEND_PID=$!
          
          # Wait for both builds
          wait $BACKEND_PID
          BACKEND_EXIT=$?
          wait $FRONTEND_PID
          FRONTEND_EXIT=$?
          
          # Check results
          if [ $BACKEND_EXIT -ne 0 ] || [ $FRONTEND_EXIT -ne 0 ]; then
            echo "Build failed"
            exit 1
          fi
      
      - name: "‚úÖ Build validation (artifact verification)"
        run: |
          # Verify build outputs
          if [ ! -d "backend/dist" ] || [ ! -d "frontend/dist" ]; then
            echo "Build artifacts missing"
            exit 1
          fi
          
          # Check key files exist
          if [ ! -f "backend/dist/index.js" ]; then
            echo "Backend build incomplete"
            exit 1
          fi
          
          if [ ! -f "frontend/dist/index.html" ]; then
            echo "Frontend build incomplete"
            exit 1
          fi
      
      - name: "üê≥ Build & Tag Docker Images (Production-Ready)"
        id: docker-build
        env:
          # Provide dummy environment variables for build-time validation
          # Real secrets will be used in production-deployment job
          NODE_ENV: production
          LOG_LEVEL: info
          CORS_ORIGINS: "https://loyalty.saichon.com"
          JWT_SECRET: "dummy-build-validation-secret-must-be-64-chars-minimum-xxxxxx"
          JWT_REFRESH_SECRET: "dummy-build-validation-refresh-secret-must-be-64-chars-minimum-xx"
          SESSION_SECRET: "dummy-build-validation-session-secret-min-32-chars"
          DATABASE_URL: "postgresql://dummy:dummy@dummy:5432/dummy"
          REDIS_URL: "redis://redis:6379"
          FRONTEND_URL: "https://loyalty.saichon.com"
          BACKEND_URL: "https://loyalty.saichon.com/api"
          VITE_API_URL: "https://loyalty.saichon.com/api"
          GOOGLE_CLIENT_ID: "dummy-google-client-id"
          GOOGLE_CLIENT_SECRET: "dummy-google-client-secret"
          GOOGLE_CALLBACK_URL: "https://loyalty.saichon.com/api/oauth/google/callback"
          FACEBOOK_APP_ID: "dummy-facebook-app-id"
          FACEBOOK_APP_SECRET: "dummy-facebook-app-secret"
          FACEBOOK_CALLBACK_URL: "https://loyalty.saichon.com/api/oauth/facebook/callback"
          LINE_CHANNEL_ID: "dummy-line-channel-id"
          LINE_CHANNEL_SECRET: "dummy-line-channel-secret"
          LINE_CALLBACK_URL: "https://loyalty.saichon.com/api/oauth/line/callback"
          TRANSLATION_FEATURE_ENABLED: "false"
          LOYALTY_USERNAME: "dummy-loyalty-user"
          LOYALTY_PASSWORD: "dummy-loyalty-password"
        run: |
          echo "üê≥ Building production Docker images with BuildKit optimization..."

          # Enable BuildKit for faster builds
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          export BUILDKIT_INLINE_CACHE=1

          # Validate Docker Compose configuration
          echo "üîç Validating Docker Compose configuration..."
          docker compose -f docker-compose.yml -f docker-compose.prod.yml config > /dev/null
          echo "‚úÖ Configuration validated"

          # Build images once with production target
          echo "üèóÔ∏è Building Docker images (production runner stage)..."
          docker compose -f docker-compose.yml -f docker-compose.prod.yml build \
            --build-arg BUILDKIT_INLINE_CACHE=1 \
            --parallel

          # Tag images with commit SHA for deployment tracking
          echo "üè∑Ô∏è Tagging images with commit SHA..."
          COMMIT_SHA="${{ github.sha }}"
          SHORT_SHA="${COMMIT_SHA:0:7}"

          # Get image names from docker compose
          BACKEND_IMAGE=$(docker compose -f docker-compose.yml -f docker-compose.prod.yml images -q backend | head -1)
          FRONTEND_IMAGE=$(docker compose -f docker-compose.yml -f docker-compose.prod.yml images -q frontend | head -1)

          # If images were built, tag them
          if [ -n "$BACKEND_IMAGE" ]; then
            docker tag $BACKEND_IMAGE loyalty-app-backend:$COMMIT_SHA
            docker tag $BACKEND_IMAGE loyalty-app-backend:$SHORT_SHA
            docker tag $BACKEND_IMAGE loyalty-app-backend:latest
            echo "‚úÖ Backend image tagged: $COMMIT_SHA, $SHORT_SHA, latest"
          fi

          if [ -n "$FRONTEND_IMAGE" ]; then
            docker tag $FRONTEND_IMAGE loyalty-app-frontend:$COMMIT_SHA
            docker tag $FRONTEND_IMAGE loyalty-app-frontend:$SHORT_SHA
            docker tag $FRONTEND_IMAGE loyalty-app-frontend:latest
            echo "‚úÖ Frontend image tagged: $COMMIT_SHA, $SHORT_SHA, latest"
          fi

          # Verify images exist
          echo "üîç Verifying tagged images..."
          docker images | grep -E "loyalty-app-(backend|frontend)" | grep -E "$SHORT_SHA|latest"

          # Quick smoke test
          echo "üß™ Running smoke tests..."
          if docker run --rm loyalty-app-backend:$SHORT_SHA node --version >/dev/null 2>&1; then
            echo "‚úÖ Backend image smoke test passed"
          else
            echo "‚ö†Ô∏è Backend smoke test failed (non-critical)"
          fi

          # Output image tags for deployment job
          echo "images_ready=true" >> $GITHUB_OUTPUT
          echo "backend_image=loyalty-app-backend:$COMMIT_SHA" >> $GITHUB_OUTPUT
          echo "frontend_image=loyalty-app-frontend:$COMMIT_SHA" >> $GITHUB_OUTPUT

          echo "‚úÖ Docker images built, tagged, and ready for deployment"
          echo "üì¶ Backend: loyalty-app-backend:$COMMIT_SHA"
          echo "üì¶ Frontend: loyalty-app-frontend:$COMMIT_SHA"
      
      - name: "üìä Build results (output summary)"
        id: build-results
        run: |
          echo "Build validation completed successfully"
          echo "passed=true" >> $GITHUB_OUTPUT

  # =============================================================================
  # PHASE 3A: DEVELOPMENT DEPLOYMENT (only on develop branch, auto-deploy)
  # No approval needed - deploys to port 5001 for testing
  # =============================================================================

  development-deployment:
    name: "üß™ Development Deployment"
    runs-on: self-hosted
    environment: development
    needs: [security-analysis, unit-tests, integration-tests]
    if: |
      github.ref == 'refs/heads/develop' &&
      needs.security-analysis.outputs.security-passed == 'true' &&
      needs.unit-tests.outputs.tests-passed == 'true' &&
      needs.integration-tests.outputs.tests-passed == 'true'
    timeout-minutes: 10
    concurrency:
      group: dev-deployment
      cancel-in-progress: true
    env:
      DEPLOY_PATH: /home/nut/loyalty-app-develop

    steps:
      - name: "üìä Pre-deployment validation"
        run: |
          echo "üîç Development deployment checks..."
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo "Security: ${{ needs.security-analysis.outputs.security-passed }}"
          echo "Unit Tests: ${{ needs.unit-tests.outputs.tests-passed }}"
          echo "Integration Tests: ${{ needs.integration-tests.outputs.tests-passed }}"
          echo "‚úÖ All checks passed - proceeding with development deployment"

      - name: "üì• Code deployment"
        run: |
          echo "üì• Deploying code to development environment..."

          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-develop}"

          if [ -d "$DEPLOY_DIR/.git" ]; then
            echo "üì• Updating existing repository..."
            cd "$DEPLOY_DIR"
            git fetch origin develop
            # Create or switch to develop branch using FETCH_HEAD
            git checkout -B develop FETCH_HEAD
          else
            echo "üì¶ Fresh clone..."
            mkdir -p "$(dirname "$DEPLOY_DIR")"
            git clone --depth 1 --single-branch -b develop \
              https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git \
              "$DEPLOY_DIR"
          fi

          echo "‚úÖ Code deployment completed"

      - name: "üîß Environment configuration"
        env:
          DEV_JWT_SECRET: ${{ secrets.DEV_JWT_SECRET }}
          DEV_JWT_REFRESH_SECRET: ${{ secrets.DEV_JWT_REFRESH_SECRET }}
          DEV_SESSION_SECRET: ${{ secrets.DEV_SESSION_SECRET }}
          DEV_GOOGLE_CLIENT_ID: ${{ secrets.DEV_GOOGLE_CLIENT_ID }}
          DEV_GOOGLE_CLIENT_SECRET: ${{ secrets.DEV_GOOGLE_CLIENT_SECRET }}
          DEV_LINE_CHANNEL_ID: ${{ secrets.DEV_LINE_CHANNEL_ID }}
          DEV_LINE_CHANNEL_SECRET: ${{ secrets.DEV_LINE_CHANNEL_SECRET }}
        run: |
          echo "üîß Configuring development environment..."

          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-develop}"
          cd "$DEPLOY_DIR"

          # Create .env.development file for development with GitHub Secrets
          cat > .env.development << EOF
          # Development Environment - Auto-generated by GitHub Actions
          NODE_ENV=development

          # URLs (Development) - use deployed dev domain for CORS consistency
          FRONTEND_URL=https://loyalty-dev.saichon.com
          BACKEND_URL=https://loyalty-dev.saichon.com/api
          VITE_API_URL=/api
          VITE_TRANSLATION_ENABLED=false

          # Internal ports
          BACKEND_PORT=4000
          FRONTEND_PORT=3000

          # Database & Redis
          DATABASE_URL=postgresql://loyalty_dev:loyalty_dev_pass@postgres:5432/loyalty_dev_db
          REDIS_URL=redis://redis:6379

          # Security
          JWT_SECRET=${DEV_JWT_SECRET}
          JWT_REFRESH_SECRET=${DEV_JWT_REFRESH_SECRET}
          SESSION_SECRET=${DEV_SESSION_SECRET}

          # OAuth
          GOOGLE_CLIENT_ID=${DEV_GOOGLE_CLIENT_ID}
          GOOGLE_CLIENT_SECRET=${DEV_GOOGLE_CLIENT_SECRET}
          GOOGLE_CALLBACK_URL=http://localhost:5001/api/oauth/google/callback
          LINE_CHANNEL_ID=${DEV_LINE_CHANNEL_ID}
          LINE_CHANNEL_SECRET=${DEV_LINE_CHANNEL_SECRET}
          LINE_CALLBACK_URL=http://localhost:5001/api/oauth/line/callback

          # Azure Translation
          TRANSLATION_FEATURE_ENABLED=false

          # Development settings
          LOG_LEVEL=debug
          CORS_ORIGINS=http://localhost:3000,http://localhost:5001,http://127.0.0.1:3000,http://127.0.0.1:5001,https://loyalty-dev.saichon.com

          # Admin credentials (development only)
          LOYALTY_USERNAME=admin@dev.local
          LOYALTY_PASSWORD=admin123
          EOF

          # docker compose always reads .env; symlink to reuse dev config
          ln -sf .env.development .env

          echo "‚úÖ Development environment configured"

      - name: "üîç Pre-deployment port check (dev)"
        run: |
          echo "üîç Checking if required development ports are available..."

          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-develop}"
          cd "$DEPLOY_DIR"

          # Development ports (from docker-compose.dev.yml)
          REQUIRED_PORTS=(5435 6380 5001)
          PORTS_IN_USE=()

          for port in "${REQUIRED_PORTS[@]}"; do
            if lsof -i ":$port" -sTCP:LISTEN -t >/dev/null 2>&1; then
              echo "‚ö†Ô∏è Port $port is in use:"
              lsof -i ":$port" -sTCP:LISTEN || true
              PORTS_IN_USE+=($port)
            else
              echo "‚úÖ Port $port is available"
            fi
          done

          if [ ${#PORTS_IN_USE[@]} -gt 0 ]; then
            echo ""
            echo "‚ö†Ô∏è Ports in use: ${PORTS_IN_USE[*]}"
            echo "üõë Stopping existing development containers..."
            docker compose -f docker-compose.yml -f docker-compose.dev.yml down --timeout 30 || true

            # Re-check ports after shutdown
            echo "üîç Re-checking ports after shutdown..."
            sleep 5
            STILL_IN_USE=()
            for port in "${PORTS_IN_USE[@]}"; do
              if lsof -i ":$port" -sTCP:LISTEN -t >/dev/null 2>&1; then
                STILL_IN_USE+=($port)
              fi
            done

            if [ ${#STILL_IN_USE[@]} -gt 0 ]; then
              echo "‚ùå WARNING: Ports still in use after shutdown: ${STILL_IN_USE[*]}"
              echo "Continuing with deployment - may fail if ports cannot be freed"
            else
              echo "‚úÖ All ports now available after shutdown"
            fi
          fi

          echo "‚úÖ Port check complete"

      - name: "üê≥ Deploy development services"
        run: |
          echo "üê≥ Deploying development services on port 5001..."

          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-develop}"
          cd "$DEPLOY_DIR"

          # Build and start with dev configuration
          echo "üèóÔ∏è Building and starting development services..."
          docker compose -f docker-compose.yml -f docker-compose.dev.yml up -d --build

          echo "‚è≥ Waiting for services to start..."
          sleep 15

          echo "‚úÖ Development services deployed"

      - name: "üóÑÔ∏è Database migration"
        run: |
          echo "üóÑÔ∏è Running database migrations on development..."

          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-develop}"
          cd "$DEPLOY_DIR"

          # Wait for database to be ready
          echo "‚è≥ Waiting for database..."
          for i in {1..30}; do
            if docker compose -f docker-compose.yml -f docker-compose.dev.yml exec -T postgres pg_isready -U loyalty_dev -d loyalty_dev_db >/dev/null 2>&1; then
              echo "‚úÖ Database is ready"
              break
            fi
            echo "Waiting for database... ($i/30)"
            sleep 2
          done

          # Check migration status before running
          echo "üîç Checking current migration status..."
          docker compose -f docker-compose.yml -f docker-compose.dev.yml exec -T backend npx prisma migrate status || echo "‚ö†Ô∏è Could not check migration status"

          # Run migrations with retry
          echo "üîÑ Running Prisma migrations..."
          for attempt in 1 2 3; do
            echo "üîÑ Migration attempt $attempt/3..."

            if docker compose -f docker-compose.yml -f docker-compose.dev.yml exec -T backend npm run db:migrate:deploy; then
              echo "‚úÖ Database migrations completed successfully"
              break
            else
              echo "‚ùå Migration attempt $attempt failed"

              if [ $attempt -eq 3 ]; then
                echo "üîß Attempting baseline resolution for 0_init..."
                docker compose -f docker-compose.yml -f docker-compose.dev.yml exec -T backend npx prisma migrate resolve --applied 0_init 2>/dev/null || true

                echo "üîÑ Final migration attempt after baseline..."
                if docker compose -f docker-compose.yml -f docker-compose.dev.yml exec -T backend npm run db:migrate:deploy; then
                  echo "‚úÖ Database migrations completed after baseline resolution"
                else
                  echo "‚ùå All migration attempts failed"
                  docker compose -f docker-compose.yml -f docker-compose.dev.yml logs --tail=20 backend
                  exit 1
                fi
              else
                sleep 3
              fi
            fi
          done

          # Verify final migration status
          echo "üîç Post-migration status:"
          docker compose -f docker-compose.yml -f docker-compose.dev.yml exec -T backend npx prisma migrate status || true

          echo "‚úÖ Database migration phase completed"

      - name: "üè• Health check"
        run: |
          echo "üè• Checking development service health..."

          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-develop}"

          # Check backend health
          for i in {1..30}; do
            if curl -f -s http://localhost:5001/api/health >/dev/null 2>&1; then
              echo "‚úÖ Backend health check passed"
              break
            fi
            echo "Waiting for backend... ($i/30)"
            sleep 2
          done

          # Check if frontend is responding
          if curl -f -s http://localhost:5001 >/dev/null 2>&1; then
            echo "‚úÖ Frontend health check passed"
          else
            echo "‚ö†Ô∏è Frontend may still be starting..."
          fi

          # Show running containers (run from deploy dir to find .env.development)
          echo ""
          echo "üìä Running development containers:"
          cd "$DEPLOY_DIR"
          docker compose -f docker-compose.yml -f docker-compose.dev.yml ps

          echo ""
          echo "‚úÖ Development deployment completed successfully!"
          echo "üåê Development URL: http://localhost:5001"

      - name: "üßπ Cleanup development env files"
        if: always()
        run: |
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-develop}"
          cd "$DEPLOY_DIR"
          rm -f .env .env.development || true

  # =============================================================================
  # PHASE 3B: PRODUCTION DEPLOYMENT (only on main, after all tests pass)
  # Pipeline Order: Pre-validation ‚Üí Backup ‚Üí Code Deploy ‚Üí Dependencies ‚Üí
  # Environment ‚Üí Service Deploy ‚Üí Migration ‚Üí Health Check ‚Üí Summary
  # =============================================================================

  production-shutdown-approval:
    name: "üõë Confirm Production Deployment"
    runs-on: ubuntu-latest
    environment:
      name: production-shutdown
    needs: [build-validation]
    if: github.ref == 'refs/heads/main' && needs.build-validation.outputs.build-passed == 'true'
    steps:
      - name: "üìã Await manual approval"
        run: |
          echo "‚úÖ Manual approval received for production deployment."

  production-deployment:
    name: "üöÄ Production Deployment"
    runs-on: self-hosted
    environment: production
    needs: [build-validation, production-shutdown-approval]
    if: |
      github.ref == 'refs/heads/main' &&
      needs.build-validation.outputs.build-passed == 'true' &&
      needs.production-shutdown-approval.result == 'success'
    timeout-minutes: 15
    concurrency:
      group: prod-deployment
      cancel-in-progress: true
    env:
      DEPLOY_PATH: /home/nut/loyalty-app-production

    steps:
      - name: "üìä Pre-deployment validation (quality gates)"
        run: |
          echo "üîç Pre-deployment checks..."
          echo "Build Validation: ${{ needs.build-validation.outputs.build-passed }}"
          echo "Backend Image: ${{ needs.build-validation.outputs.backend-image }}"
          echo "Frontend Image: ${{ needs.build-validation.outputs.frontend-image }}"
          echo "Images Ready: ${{ needs.build-validation.outputs.images-ready }}"

          # Verify build validation passed (which already validates all tests)
          if [ "${{ needs.build-validation.outputs.build-passed }}" != "true" ]; then
            echo "‚ùå Build validation failed - deployment blocked"
            echo "üö® Build validation ensures all tests (security, unit, integration, e2e) passed"
            exit 1
          fi

          echo "‚úÖ All pre-deployment checks passed"
          echo "üîê Build validation confirmed - all quality gates met for deployment"
      
      - name: "üíæ Smart database backup (pre-shutdown)"
        id: backup
        run: |
          echo "üíæ Creating smart database backup..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          
          # Only backup if deployment directory exists
          if [ ! -d "$DEPLOY_DIR" ]; then
            echo "‚ö†Ô∏è First deployment - skipping backup"
            echo "status=skipped" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          cd "$DEPLOY_DIR"
          mkdir -p backups
          
          # Quick backup with timeout
          timestamp=$(date +%Y%m%d_%H%M%S)
          backup_file="backups/pre_deploy_${timestamp}.sql"
          
          if timeout 60s docker exec loyalty_postgres pg_dump -U loyalty -d loyalty_db > "$backup_file" 2>/dev/null; then
            if [ -s "$backup_file" ]; then
              echo "‚úÖ Backup created: $backup_file ($(du -h "$backup_file" | cut -f1))"
              echo "status=success" >> $GITHUB_OUTPUT
            else
              echo "‚ö†Ô∏è Backup failed - empty file"
              rm -f "$backup_file"
              echo "status=failed" >> $GITHUB_OUTPUT
            fi
          else
            echo "‚ö†Ô∏è Backup timed out or failed"
            rm -f "$backup_file" 2>/dev/null || true
            echo "status=failed" >> $GITHUB_OUTPUT
          fi
      
      - name: "üì• Optimized code deployment"
        run: |
          echo "üì• Deploying code with optimizations..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          
          # Smart deployment: update existing repo or clone new
          if [ -d "$DEPLOY_DIR/.git" ]; then
            echo "üì• Updating existing repository..."
            cd "$DEPLOY_DIR"
            git clean -fd 2>/dev/null || true
            git reset --hard HEAD 2>/dev/null || true
            git remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git
            git fetch --depth 1 origin ${{ github.sha }}
            git checkout ${{ github.sha }}
          else
            echo "üì¶ Fresh deployment..."
            mkdir -p "$(dirname "$DEPLOY_DIR")"
            rm -rf "$DEPLOY_DIR" 2>/dev/null || true
            git clone --depth 1 --single-branch \
              https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git \
              "$DEPLOY_DIR"
            cd "$DEPLOY_DIR"
            git checkout ${{ github.sha }}
          fi
          
          echo "‚úÖ Code deployment completed"
      
      - name: "‚ö° Lightning-fast dependency setup"
        run: |
          echo "‚ö° Setting up dependencies with maximum speed..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          cd "$DEPLOY_DIR"
          
          # Configure npm for maximum performance
          export NPM_CONFIG_CACHE=/home/nut/.npm-cache
          export NPM_CONFIG_PREFER_OFFLINE=true
          export NPM_CONFIG_AUDIT=false
          export NPM_CONFIG_FUND=false
          mkdir -p /home/nut/.npm-cache
          
          # Parallel dependency installation with smart caching
          install_deps() {
            local service=$1
            local dir=$2
            echo "Installing $service dependencies..."
            cd "$DEPLOY_DIR/$dir"
            
            # Use npm ci for reproducible installs
            if npm ci --prefer-offline --no-audit --ignore-scripts 2>/dev/null; then
              echo "‚úÖ $service dependencies installed"
            else
              echo "üîÑ Fallback install for $service..."
              npm install --prefer-offline --no-audit
            fi
          }
          
          # Install in parallel
          install_deps "backend" "backend" &
          install_deps "frontend" "frontend" &
          wait
          
          echo "‚úÖ Dependencies installed in parallel"
      
      - name: "üîß Environment configuration"
        run: |
          echo "üîß Configuring production environment..."
          
          # Validate critical secrets
          missing_secrets=()
          if [ -z "${{ secrets.JWT_SECRET }}" ]; then missing_secrets+=("JWT_SECRET"); fi
          if [ -z "${{ secrets.DATABASE_URL }}" ]; then missing_secrets+=("DATABASE_URL"); fi
          
          if [ ${#missing_secrets[@]} -ne 0 ]; then
            echo "‚ùå Missing secrets: ${missing_secrets[*]}"
            exit 1
          fi
          
          # Export environment variables
          echo "NODE_ENV=production" >> $GITHUB_ENV
          echo "LOG_LEVEL=info" >> $GITHUB_ENV
          echo "CORS_ORIGINS=${{ secrets.CORS_ORIGINS }}" >> $GITHUB_ENV
          
          # Core secrets
          echo "JWT_SECRET=${{ secrets.JWT_SECRET }}" >> $GITHUB_ENV
          echo "JWT_REFRESH_SECRET=${{ secrets.JWT_REFRESH_SECRET }}" >> $GITHUB_ENV
          echo "SESSION_SECRET=${{ secrets.SESSION_SECRET }}" >> $GITHUB_ENV
          echo "DATABASE_URL=${{ secrets.DATABASE_URL }}" >> $GITHUB_ENV
          echo "REDIS_URL=${{ secrets.REDIS_URL || 'redis://redis:6379' }}" >> $GITHUB_ENV
          echo "FRONTEND_URL=${{ secrets.FRONTEND_URL }}" >> $GITHUB_ENV
          echo "BACKEND_URL=${{ secrets.BACKEND_URL }}" >> $GITHUB_ENV
          echo "VITE_API_URL=${{ secrets.VITE_API_URL }}" >> $GITHUB_ENV
          
          # OAuth Configuration
          echo "GOOGLE_CLIENT_ID=${{ secrets.GOOGLE_CLIENT_ID }}" >> $GITHUB_ENV
          echo "GOOGLE_CLIENT_SECRET=${{ secrets.GOOGLE_CLIENT_SECRET }}" >> $GITHUB_ENV
          echo "GOOGLE_CALLBACK_URL=${{ secrets.GOOGLE_CALLBACK_URL || vars.GOOGLE_CALLBACK_URL }}" >> $GITHUB_ENV
          echo "FACEBOOK_APP_ID=${{ secrets.FACEBOOK_APP_ID }}" >> $GITHUB_ENV
          echo "FACEBOOK_APP_SECRET=${{ secrets.FACEBOOK_APP_SECRET }}" >> $GITHUB_ENV
          echo "FACEBOOK_CALLBACK_URL=${{ secrets.FACEBOOK_CALLBACK_URL }}" >> $GITHUB_ENV
          echo "LINE_CHANNEL_ID=${{ secrets.LINE_CHANNEL_ID }}" >> $GITHUB_ENV
          echo "LINE_CHANNEL_SECRET=${{ secrets.LINE_CHANNEL_SECRET }}" >> $GITHUB_ENV
          echo "LINE_CALLBACK_URL=${{ secrets.LINE_CALLBACK_URL || vars.LINE_CALLBACK_URL }}" >> $GITHUB_ENV
          
          # Azure Translation Service
          echo "TRANSLATION_FEATURE_ENABLED=${{ secrets.TRANSLATION_FEATURE_ENABLED || 'false' }}" >> $GITHUB_ENV
          
          # Loyalty System Configuration
          echo "LOYALTY_USERNAME=${{ secrets.LOYALTY_USERNAME }}" >> $GITHUB_ENV
          echo "LOYALTY_PASSWORD=${{ secrets.LOYALTY_PASSWORD }}" >> $GITHUB_ENV
          
          echo "‚úÖ Environment configured"
          
          # Debug environment variable lengths for validation
          echo "üîç Environment Variable Debug:"
          echo "JWT_SECRET length: ${#JWT_SECRET}"
          echo "JWT_REFRESH_SECRET length: ${#JWT_REFRESH_SECRET}"
          echo "SESSION_SECRET length: ${#SESSION_SECRET}"
          echo "REDIS_URL configured: $([ -n "$REDIS_URL" ] && echo 'yes' || echo 'no')"
          echo "DATABASE_URL configured: $([ -n "$DATABASE_URL" ] && echo 'yes' || echo 'no')"
      
      - name: "üìù Create .env file (container configuration)"
        run: |
          echo "üìù Creating .env file for Docker Compose..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          cd "$DEPLOY_DIR"
          
          # Create .env file with all environment variables
          cat > .env << EOF
          # Node environment
          NODE_ENV=${NODE_ENV}
          LOG_LEVEL=${LOG_LEVEL}
          CORS_ORIGINS=${CORS_ORIGINS}
          
          # Core secrets
          JWT_SECRET=${JWT_SECRET}
          JWT_REFRESH_SECRET=${JWT_REFRESH_SECRET}
          SESSION_SECRET=${SESSION_SECRET}
          DATABASE_URL=${DATABASE_URL}
          REDIS_URL=${REDIS_URL:-redis://redis:6379}
          FRONTEND_URL=${FRONTEND_URL}
          BACKEND_URL=${BACKEND_URL}
          VITE_API_URL=${VITE_API_URL}
          VITE_TRANSLATION_ENABLED=${TRANSLATION_FEATURE_ENABLED}
          TRANSLATION_FEATURE_ENABLED=${TRANSLATION_FEATURE_ENABLED}
          
          # OAuth Configuration
          GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID}
          GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET}
          GOOGLE_CALLBACK_URL=${GOOGLE_CALLBACK_URL}
          FACEBOOK_APP_ID=${FACEBOOK_APP_ID}
          FACEBOOK_APP_SECRET=${FACEBOOK_APP_SECRET}
          FACEBOOK_CALLBACK_URL=${FACEBOOK_CALLBACK_URL}
          LINE_CHANNEL_ID=${LINE_CHANNEL_ID}
          LINE_CHANNEL_SECRET=${LINE_CHANNEL_SECRET}
          LINE_CALLBACK_URL=${LINE_CALLBACK_URL}
          
          # Loyalty System Configuration
          LOYALTY_USERNAME=${LOYALTY_USERNAME}
          LOYALTY_PASSWORD=${LOYALTY_PASSWORD}
          EOF
          
          echo "‚úÖ Environment file created successfully"
          echo "üìä Environment variables configured: $(wc -l < .env) lines"
          
          # Debug .env file contents (without exposing secrets)
          echo "üîç .env file validation:"
          echo "NODE_ENV: $(grep '^NODE_ENV=' .env | cut -d'=' -f2)"
          echo "REDIS_URL configured: $(grep -q '^REDIS_URL=' .env && echo 'yes' || echo 'no')"
          echo "DATABASE_URL configured: $(grep -q '^DATABASE_URL=' .env && echo 'yes' || echo 'no')"
          echo "JWT_SECRET configured: $(grep -q '^JWT_SECRET=' .env && echo 'yes' || echo 'no')"
          
          # Check for empty values
          empty_vars=$(grep '=$' .env | cut -d'=' -f1 | head -5)
          if [ -n "$empty_vars" ]; then
            echo "‚ö†Ô∏è Empty environment variables found: $empty_vars"
          fi
      
      - name: "üîç Pre-deployment port check"
        run: |
          echo "üîç Checking if required ports are available..."

          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          cd "$DEPLOY_DIR"

          # Production ports (from docker-compose.yml + docker-compose.prod.yml)
          REQUIRED_PORTS=(5434 6379 4001)
          PORTS_IN_USE=()

          for port in "${REQUIRED_PORTS[@]}"; do
            if lsof -i ":$port" -sTCP:LISTEN -t >/dev/null 2>&1; then
              echo "‚ö†Ô∏è Port $port is in use:"
              lsof -i ":$port" -sTCP:LISTEN || true
              PORTS_IN_USE+=($port)
            else
              echo "‚úÖ Port $port is available"
            fi
          done

          if [ ${#PORTS_IN_USE[@]} -gt 0 ]; then
            echo ""
            echo "‚ùå ERROR: Ports in use: ${PORTS_IN_USE[*]}"
            echo ""
            echo "üõë Attempting graceful shutdown of existing services..."
            if docker compose -f docker-compose.yml -f docker-compose.prod.yml ps -q | head -1 | grep -q .; then
              docker compose -f docker-compose.yml -f docker-compose.prod.yml down --timeout 30 || true

              # Re-check ports after shutdown
              echo "üîç Re-checking ports after shutdown..."
              sleep 5
              STILL_IN_USE=()
              for port in "${PORTS_IN_USE[@]}"; do
                if lsof -i ":$port" -sTCP:LISTEN -t >/dev/null 2>&1; then
                  STILL_IN_USE+=($port)
                fi
              done

              if [ ${#STILL_IN_USE[@]} -gt 0 ]; then
                echo "‚ùå FATAL: Ports still in use after shutdown: ${STILL_IN_USE[*]}"
                echo "Please manually stop services using these ports before deploying."
                exit 1
              else
                echo "‚úÖ All ports now available after shutdown"
              fi
            else
              echo "‚ùå FATAL: Ports in use but no compose services found to stop"
              echo "Please manually investigate and free these ports:"
              for port in "${PORTS_IN_USE[@]}"; do
                echo "  Port $port:"
                lsof -i ":$port" -sTCP:LISTEN || true
              done
              exit 1
            fi
          fi

          echo "‚úÖ All required ports are available for deployment"

      - name: "üöÄ Optimized service deployment"
        run: |
          echo "üöÄ Deploying services with zero-downtime strategy..."

          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          cd "$DEPLOY_DIR"

          # Enable BuildKit for faster builds
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          
          # Build and start services
          echo "üèóÔ∏è Building and starting services..."
          docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d --build
          
          echo "‚úÖ Services deployed"
          
          # Give services time to initialize before health checks
          echo "‚è≥ Allowing services to initialize (30 seconds)..."
          sleep 30
          
          # Quick startup verification
          echo "üîç Quick startup verification:"
          docker compose ps
          echo "üìù Backend startup logs (last 10 lines):"
          docker compose logs --tail=10 backend 2>/dev/null || echo "No backend logs yet"
      
      - name: "üóÉÔ∏è Database migration (container context)"
        run: |
          echo "üóÉÔ∏è Running database migrations with enhanced logging..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          cd "$DEPLOY_DIR"
          
          # Pre-migration container status
          echo "üì¶ Pre-migration container status:"
          docker compose ps
          
          # Wait for database to be ready with enhanced logging
          echo "üíæ Waiting for database to be ready..."
          if timeout 60s bash -c 'until docker compose exec -T postgres pg_isready -U loyalty -d loyalty_db; do echo "Database not ready, waiting..."; sleep 2; done'; then
            echo "‚úÖ Database is ready"
          else
            echo "‚ùå Database readiness timeout"
            echo "üìÑ Database logs:"
            docker compose logs --tail=20 postgres
            exit 1
          fi
          
          # Wait for backend container to be fully ready
          echo "üì¶ Waiting for backend container to be ready..."
          if timeout 60s bash -c 'until docker compose exec -T backend echo "ready" >/dev/null 2>&1; do echo "Backend not ready, waiting..."; sleep 2; done'; then
            echo "‚úÖ Backend container is ready"
          else
            echo "‚ùå Backend container timeout"
            echo "üìÑ Backend logs:"
            docker compose logs --tail=20 backend
            exit 1
          fi
          
          # Give containers additional time to stabilize
          echo "‚è≥ Allowing containers to stabilize (10 seconds)..."
          sleep 10
          
          # Show database connection info before migration
          echo "üîó Database connection check:"
          docker compose exec -T backend node -e "console.log('DATABASE_URL:', process.env.DATABASE_URL ? 'configured' : 'missing')" || echo "Cannot check DB config"
          
          # Run migration from within backend container (correct network context)
          echo "üóÉÔ∏è Running database migration from container..."
          for attempt in 1 2 3; do
            echo "\nüîÑ Migration attempt $attempt/3..."
            
            # First check migration status
            echo "üîç Checking current migration status:"
            docker compose exec -T backend npm run db:migrate:status || echo "Cannot check migration status"
            
            if docker compose exec -T backend npm run db:migrate:deploy; then
              echo "‚úÖ Database migrations completed successfully"
              
              # Verify migration success
              echo "üîç Post-migration verification:"
              docker compose exec -T backend npm run db:migrate:status || echo "Cannot verify final status"
              break
            else
              echo "‚ùå Migration attempt $attempt failed"
              
              # Show backend logs for debugging
              echo "üìÑ Backend logs during migration failure:"
              docker compose logs --tail=10 backend
              
              if [ $attempt -eq 3 ]; then
                echo "‚ùå Migration failed after 3 attempts"
                
                # Enhanced baseline scenario handling
                echo "üîç Checking for baseline scenario..."
                migration_status=$(docker compose exec -T backend npm run db:migrate:status 2>&1 || echo "status_check_failed")
                echo "Migration status output: $migration_status"
                
                if echo "$migration_status" | grep -q "migrations have not yet been applied"; then
                  echo "üîß Attempting baseline resolution..."
                  docker compose exec -T backend npx prisma migrate resolve --applied 0_init 2>/dev/null || echo "Baseline resolution failed"
                  
                  echo "üîÑ Retrying migration after baseline..."
                  if docker compose exec -T backend npm run db:migrate:deploy; then
                    echo "‚úÖ Database migrations completed after baseline resolution"
                    break
                  else
                    echo "‚ùå Migration still failed after baseline"
                  fi
                fi
                
                # Final failure diagnostics
                echo "üî¥ MIGRATION FAILURE DIAGNOSTICS:"
                echo "=== Container Status ==="
                docker compose ps
                echo "\n=== Database Connection Test ==="
                docker compose exec -T postgres pg_isready -U loyalty -d loyalty_db || echo "Database connection failed"
                echo "\n=== Backend Environment ==="
                docker compose exec -T backend env | grep -E "DATABASE_URL|NODE_ENV" || echo "Cannot check environment"
                echo "\n=== Backend Logs (last 30 lines) ==="
                docker compose logs --tail=30 backend
                
                echo "‚ùå All migration attempts failed"
                exit 1
              else
                echo "‚è≥ Retrying in 5 seconds..."
                sleep 5
              fi
            fi
          done
          
          echo "‚úÖ Database migration phase completed"
      
      - name: "üîê Post-deployment OAuth validation"
        run: |
          echo "üîê Validating OAuth configuration in production environment..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          cd "$DEPLOY_DIR"
          
          # Wait for application to be ready
          echo "‚è≥ Waiting for application to be ready..."
          sleep 10
          
          # Run OAuth health validation in container context
          echo "üîç Running OAuth validation..."
          if docker compose exec -T backend test -f "/app/scripts/validate-oauth-health.sh"; then
            # Run OAuth validation from within container (production network context)
            if docker compose exec -T backend bash -c "cd /app && ./scripts/validate-oauth-health.sh"; then
              echo "‚úÖ OAuth validation passed in production"
            else
              echo "‚ö†Ô∏è OAuth validation failed in production - check configuration"
              echo "üìÑ Recent backend logs:"
              docker compose logs --tail=20 backend
            fi
          else
            echo "‚ö†Ô∏è OAuth validation script not found in container - skipping"
          fi
      
      - name: "üóÑÔ∏è Post-deployment database validation"
        run: |
          echo "üóÑÔ∏è Validating database migration status in production..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          cd "$DEPLOY_DIR"
          
          # Verify database migration status
          echo "üîç Checking final migration status..."
          if docker compose exec -T backend npm run db:migrate:status; then
            echo "‚úÖ Database migration status verified"
          else
            echo "‚ö†Ô∏è Cannot verify database migration status"
          fi
          
          # Run database migration rollback safety check
          if docker compose exec -T backend test -f "/app/scripts/migration-rollback-safety.sh"; then
            echo "üîç Running rollback safety check..."
            if docker compose exec -T backend bash -c "cd /app && ./scripts/migration-rollback-safety.sh check"; then
              echo "‚úÖ Migration rollback safety verified"
            else
              echo "‚ö†Ô∏è Migration rollback safety check failed"
            fi
          else
            echo "‚ö†Ô∏è Migration rollback safety script not found - skipping"
          fi
      
      - name: "üè• Health checks & validation (service readiness)"
        run: |
          echo "üè• Running comprehensive health checks with enhanced logging..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          cd "$DEPLOY_DIR"
          
          # Enhanced logging function with container inspection
          enhanced_health_check() {
            local service=$1
            local url=$2
            local max_attempts=15
            local attempt=0
            
            echo "üîç Starting health check for $service at $url"
            
            # First, check container status
            echo "üì¶ Checking container status..."
            docker compose ps
            
            # Show container logs for startup diagnostics
            echo "üìú Recent startup logs from containers:"
            echo "=== Backend Logs (last 50 lines) ==="
            docker compose logs --tail=50 backend 2>/dev/null || echo "No backend logs available"
            
            echo "\n=== Frontend Logs (last 20 lines) ==="
            docker compose logs --tail=20 frontend 2>/dev/null || echo "No frontend logs available"
            
            echo "\n=== Database Logs (last 10 lines) ==="
            docker compose logs --tail=10 postgres 2>/dev/null || echo "No database logs available"
            
            # Check if containers are actually running
            if ! docker compose ps | grep -q "Up"; then
              echo "‚ùå Critical: No containers are running!"
              echo "üî¥ Container status details:"
              docker compose ps -a
              return 1
            fi
            
            # Enhanced health checking with detailed diagnostics
            while [ $attempt -lt $max_attempts ]; do
              echo "üîÑ Health Check Attempt $((attempt + 1))/$max_attempts for $service"
              
              # Test with verbose curl output for diagnostics
              if curl -f -s --max-time 3 "$url" > /dev/null 2>&1; then
                echo "‚úÖ $service is healthy and responding"
                
                # Additional validation - get response details
                response=$(curl -s --max-time 3 "$url" 2>/dev/null || echo "No response body")
                echo "üìÑ Response preview: ${response:0:200}..."
                return 0
              else
                # Detailed failure diagnostics
                http_code=$(curl -s -o /dev/null -w "%{http_code}" --max-time 3 "$url" 2>/dev/null || echo "000")
                echo "‚ö†Ô∏è $service not ready (HTTP: $http_code)"
                
                # Every 3rd attempt, show detailed diagnostics
                if [ $((attempt % 3)) -eq 0 ] && [ $attempt -gt 0 ]; then
                  echo "üîç Diagnostic info (attempt $((attempt + 1))):"
                  echo "  - Port check:" $(nc -z localhost 4001 && echo "Port 4001 open" || echo "Port 4001 closed")
                  echo "  - Backend container:" $(docker compose exec -T backend echo "responsive" 2>/dev/null || echo "not responding")
                  echo "  - Recent backend errors:"
                  docker compose logs --tail=5 backend 2>/dev/null | grep -i "error\|fail\|exception" | tail -3 || echo "    No recent errors found"
                fi
              fi
              
              attempt=$((attempt + 1))
              [ $attempt -lt $max_attempts ] && sleep 2
            done
            
            echo "‚ùå $service health check failed after $max_attempts attempts"
            
            # Final diagnostic dump on failure
            echo "üî¥ FAILURE DIAGNOSTICS:"
            echo "=== Container Status ==="
            docker compose ps -a
            
            echo "\n=== Network Status ==="
            docker compose exec -T backend cat /etc/hosts 2>/dev/null | head -5 || echo "Cannot access container network info"
            
            echo "\n=== Application Ports ==="
            docker compose exec -T backend netstat -tlnp 2>/dev/null | grep -E ":3000|:4001" || echo "Cannot check application ports"
            
            echo "\n=== Final Backend Logs ==="
            docker compose logs --tail=30 backend
            
            echo "\n=== System Resources ==="
            echo "Memory:" $(free -h | head -2 | tail -1)
            echo "Disk:" $(df -h / | tail -1)
            
            return 1
          }
          
          # Run enhanced health checks
          enhanced_health_check "Application" "http://localhost:4001/api/health"
          
          # If backend is healthy, check frontend
          echo "\nüåê Checking frontend..."
          if curl -f -s --max-time 3 "http://localhost:4001/" > /dev/null 2>&1; then
            echo "‚úÖ Frontend is responding"
          else
            echo "‚ö†Ô∏è Frontend not responding (non-critical for deployment)"
            echo "üìÑ Frontend logs:"
            docker compose logs --tail=10 frontend
          fi
          
          echo "‚úÖ Health checks completed successfully"

  # =============================================================================
  # PHASE 4: POST-DEPLOYMENT MONITORING & CLEANUP
  # =============================================================================
  
  post-deployment:
    name: "üìä Post-Deployment"
    runs-on: self-hosted
    needs: production-deployment
    if: always() && (needs.production-deployment.result == 'success' || needs.production-deployment.result == 'failure')
    timeout-minutes: 3
    continue-on-error: true  # Don't block pipeline completion on post-deployment issues
    
    steps:
      - name: "üîç Pre-deployment health check"
        timeout-minutes: 1
        run: |
          echo "üîç Checking runner status..."
          echo "Runner: $(hostname)"
          echo "Load: $(uptime)"
          echo "Memory: $(free -h 2>/dev/null || vm_stat | head -10)"
          echo "Disk: $(df -h / | tail -1)"
          echo "‚úÖ Runner health check passed"

      - name: "‚ö†Ô∏è Automatic Rollback on Failure"
        if: failure()
        timeout-minutes: 3
        run: |
          echo "‚ö†Ô∏è Deployment failed - initiating automatic rollback..."

          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          cd "$DEPLOY_DIR"

          # Find previous working images (excluding current SHA)
          CURRENT_SHA="${{ github.sha }}"
          echo "üîç Looking for previous working images (excluding $CURRENT_SHA)..."

          PREV_BACKEND=$(docker images loyalty-app-backend --format "{{.Tag}}" | grep -v "latest" | grep -v "$CURRENT_SHA" | head -1)
          PREV_FRONTEND=$(docker images loyalty-app-frontend --format "{{.Tag}}" | grep -v "latest" | grep -v "$CURRENT_SHA" | head -1)

          if [ -n "$PREV_BACKEND" ] && [ -n "$PREV_FRONTEND" ]; then
            echo "üì¶ Rolling back to previous images:"
            echo "  Backend: loyalty-app-backend:$PREV_BACKEND"
            echo "  Frontend: loyalty-app-frontend:$PREV_FRONTEND"

            # Export previous image tags
            export BACKEND_IMAGE="loyalty-app-backend:$PREV_BACKEND"
            export FRONTEND_IMAGE="loyalty-app-frontend:$PREV_FRONTEND"

            # Perform rollback with same hot-swap strategy
            echo "üîÑ Rolling back backend..."
            docker compose -f docker-compose.yml -f docker-compose.prod.yml \
              up -d --no-build --no-deps backend

            sleep 10

            echo "üîÑ Rolling back frontend..."
            docker compose -f docker-compose.yml -f docker-compose.prod.yml \
              up -d --no-build --no-deps frontend

            sleep 10

            # Verify rollback
            echo "üè• Verifying rollback health..."
            if timeout 30s bash -c 'until curl -f -s http://localhost:4001/api/health >/dev/null 2>&1; do sleep 2; done'; then
              echo "‚úÖ Rollback successful - service health check passed"
              echo "üìä Rollback summary:"
              echo "  Rolled back from: $CURRENT_SHA"
              echo "  Rolled back to: $PREV_BACKEND"
              docker compose ps
            else
              echo "‚ùå Rollback health check failed"
              echo "üìÑ Backend logs after rollback:"
              docker compose logs --tail=20 backend
            fi
          else
            echo "‚ö†Ô∏è No previous images found for rollback"
            echo "Available backend images:"
            docker images loyalty-app-backend
            echo "Available frontend images:"
            docker images loyalty-app-frontend
          fi

          echo "üî¥ Deployment failed and rollback attempted"
          echo "Please review the logs above for details"

      - name: "üìä Deployment summary (reporting only)"
        timeout-minutes: 1
        run: |
          echo "==============================================="
          echo "üìä DEPLOYMENT SUMMARY"
          echo "==============================================="
          echo "Repository: ${{ github.repository }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo "Actor: ${{ github.actor }}"
          echo "Timestamp: $(date -Iseconds)"
          echo ""
          echo "üéØ Job Results:"
          echo "Security Analysis: ${{ needs.security-analysis.result }}"
          echo "Unit Tests: ${{ needs.unit-tests.result }}"
          echo "Integration Tests: ${{ needs.integration-tests.result }}"
          echo "E2E Tests: ${{ needs.e2e-tests.result || 'skipped' }}"
          echo "Build Validation: ${{ needs.build-validation.result }}"
          echo "Production Deployment: ${{ needs.production-deployment.result }}"
          echo ""
          echo "üìà Metrics:"
          echo "Test Coverage: ${{ needs.unit-tests.outputs.coverage-percent }}%"
          echo ""
          echo "üåê Service Endpoints:"
          echo "Application: https://loyalty.saichon.com"
          echo "Health Check: https://loyalty.saichon.com/api/health"
          echo "==============================================="
      
      - name: "üåê Service health validation"
        timeout-minutes: 1
        run: |
          echo "üåê Testing deployed services..."
          
          # Test health endpoint with timeout and retry
          for i in {1..3}; do
            if curl -s --max-time 10 "https://loyalty.saichon.com/api/health" | grep -q "ok\|healthy\|success" 2>/dev/null; then
              echo "‚úÖ Service health check passed (attempt $i)"
              break
            else
              echo "‚ö†Ô∏è Health check failed (attempt $i/3)"
              [ $i -eq 3 ] && echo "üö® All health checks failed - service may be unhealthy"
              sleep 5
            fi
          done
      
      - name: "üßπ Smart cleanup (non-blocking)"
        timeout-minutes: 1
        continue-on-error: true
        run: |
          echo "üßπ Running smart cleanup with strict timeouts..."
          
          # Set strict timeout for all operations
          set +e  # Don't exit on command failures in cleanup
          
          # Check disk usage with timeout
          DISK_USAGE=$(timeout 10s df / | tail -1 | awk '{print $5}' | sed 's/%//' 2>/dev/null || echo "unknown")
          echo "üíæ Disk usage: ${DISK_USAGE}%"
          
          # Clean up old containers with timeout
          echo "üê≥ Cleaning old containers..."
          timeout 20s docker container prune -f 2>/dev/null || echo "‚ö†Ô∏è Container cleanup timed out"
          
          # Clean up old images with timeout (keep last 3 versions)
          echo "üñºÔ∏è Cleaning old images..."
          timeout 20s docker image prune -f --filter "until=72h" 2>/dev/null || echo "‚ö†Ô∏è Image cleanup timed out"
          
          # Clean up build cache with timeout
          echo "üßπ Cleaning build cache..."
          timeout 10s docker builder prune -f --filter "until=24h" 2>/dev/null || echo "‚ö†Ô∏è Build cache cleanup timed out"
          
          echo "‚úÖ Cleanup completed (may have warnings, non-blocking)"

      - name: "üìã Post-deployment summary"
        timeout-minutes: 1
        continue-on-error: true
        run: |
          echo "üéâ Deployment Summary:"
          echo "‚îú‚îÄ‚îÄ ‚úÖ Application deployed successfully"
          echo "‚îú‚îÄ‚îÄ üåê Frontend: https://loyalty.saichon.com"
          echo "‚îú‚îÄ‚îÄ üîó Backend: https://loyalty.saichon.com/api"
          echo "‚îú‚îÄ‚îÄ üìä Health check: $(curl -s -o /dev/null -w '%{http_code}' https://loyalty.saichon.com/api/health 2>/dev/null || echo 'timeout')"
          echo "‚îî‚îÄ‚îÄ üïí Completed at: $(date)"
          
          # Final completion message
          echo "üéä Post-deployment job completed successfully - pipeline will not get stuck"

      - name: "üßπ Cleanup production env file"
        if: always()
        run: |
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app-production}"
          cd "$DEPLOY_DIR"
          rm -f .env || true

  # =============================================================================
  # UNIFIED TEST REPORT GENERATION (Merges all test types)
  # =============================================================================

  generate-test-reports:
    name: "üìä Generate Unified Test Reports"
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, e2e-tests]
    if: always() && github.ref == 'refs/heads/main'

    # Prevent multiple concurrent report generations to avoid artifact conflicts
    concurrency:
      group: "test-reports-${{ github.repository }}"
      cancel-in-progress: true

    steps:
      - name: "üì• Checkout code"
        uses: actions/checkout@v4
        with:
          fetch-depth: 1

      - name: "‚ö° Setup Node.js"
        uses: actions/setup-node@v6
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: "üì• Download unit test results"
        uses: actions/download-artifact@v6
        with:
          name: allure-results-unit-${{ github.run_id }}
          path: allure-results-unit/
        continue-on-error: true

      - name: "üì• Download integration test results"
        uses: actions/download-artifact@v6
        with:
          name: allure-results-integration-${{ github.run_id }}
          path: allure-results-integration/
        continue-on-error: true

      - name: "üì• Download E2E test results"
        uses: actions/download-artifact@v6
        with:
          name: allure-results-e2e-${{ github.run_id }}
          path: allure-results-e2e/
        continue-on-error: true

      - name: "üîÄ Merge all test results"
        run: |
          echo "üîÄ Merging test results from all test types..."

          # Create combined results directory
          mkdir -p backend/allure-results-combined

          # Copy unit test results
          if [ -d "allure-results-unit" ] && [ "$(ls -A allure-results-unit 2>/dev/null)" ]; then
            echo "‚úÖ Copying unit test results..."
            cp -r allure-results-unit/* backend/allure-results-combined/ 2>/dev/null || true
            UNIT_COUNT=$(ls -1 allure-results-unit | wc -l)
            echo "   üìä Unit test files: $UNIT_COUNT"
          else
            echo "‚ö†Ô∏è No unit test results found"
          fi

          # Copy integration test results
          if [ -d "allure-results-integration" ] && [ "$(ls -A allure-results-integration 2>/dev/null)" ]; then
            echo "‚úÖ Copying integration test results..."
            cp -r allure-results-integration/* backend/allure-results-combined/ 2>/dev/null || true
            INTEGRATION_COUNT=$(ls -1 allure-results-integration | wc -l)
            echo "   üìä Integration test files: $INTEGRATION_COUNT"
          else
            echo "‚ö†Ô∏è No integration test results found"
          fi

          # Copy E2E test results
          if [ -d "allure-results-e2e" ] && [ "$(ls -A allure-results-e2e 2>/dev/null)" ]; then
            echo "‚úÖ Copying E2E test results..."
            cp -r allure-results-e2e/* backend/allure-results-combined/ 2>/dev/null || true
            E2E_COUNT=$(ls -1 allure-results-e2e | wc -l)
            echo "   üìä E2E test files: $E2E_COUNT"
          else
            echo "‚ö†Ô∏è No E2E test results found"
          fi

          # Count total merged results
          TOTAL_COUNT=$(ls -1 backend/allure-results-combined 2>/dev/null | wc -l)
          echo "üìä Total merged test result files: $TOTAL_COUNT"

          if [ "$TOTAL_COUNT" -eq 0 ]; then
            echo "‚ö†Ô∏è No test results to merge - creating placeholder"
            mkdir -p backend/allure-results-combined
            echo '{"name":"No tests found","status":"broken"}' > backend/allure-results-combined/placeholder.json
          fi

      - name: "‚òï Setup Java for Allure"
        uses: actions/setup-java@v4
        with:
          distribution: 'temurin'
          java-version: '17'

      - name: "üì¶ Install Allure CLI"
        run: |
          cd backend
          npm install --no-save allure-commandline@^2.34.1

      - name: "üìä Generate unified Allure report"
        run: |
          cd backend

          echo "üìä Generating unified Allure report from merged results..."

          if [ -d "allure-results-combined" ] && [ "$(ls -A allure-results-combined)" ]; then
            echo "‚úÖ Generating Allure report..."
            npx allure generate allure-results-combined --clean -o allure-report
            echo "‚úÖ Unified Allure report generated successfully"
            echo "üìä Report includes: Unit Tests + Integration Tests + E2E Tests (Playwright)"
          else
            echo "‚ö†Ô∏è No test results found, creating placeholder report"
            mkdir -p allure-report
            cat > allure-report/index.html <<'EOF'
          <!DOCTYPE html>
          <html>
          <head><title>No Test Results</title></head>
          <body><h1>No test results available for this run</h1></body>
          </html>
          EOF
          fi

      - name: "üîó Create latest redirect with branch indicator"
        run: |
          mkdir -p test-reports-latest

          # Determine branch name and badge color
          BRANCH_NAME="${{ github.ref_name }}"
          if [ "$BRANCH_NAME" = "main" ]; then
            BADGE_COLOR="#28a745"
            BADGE_TEXT="üöÄ PRODUCTION"
            ENV_NAME="Production"
          else
            BADGE_COLOR="#17a2b8"
            BADGE_TEXT="üß™ DEVELOPMENT"
            ENV_NAME="Development"
          fi

          cat > test-reports-latest/index.html <<REDIRECT_EOF
          <!DOCTYPE html>
          <html lang="en">
          <head>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1.0">
              <meta http-equiv="refresh" content="0; url=../${{ github.run_number }}/">
              <title>[$ENV_NAME] Test Report #${{ github.run_number }}</title>
              <style>
                  body {
                      font-family: -apple-system, BlinkMacSystemFont, 'Segoe UI', Roboto, Oxygen, Ubuntu, Cantarell, sans-serif;
                      display: flex;
                      justify-content: center;
                      align-items: center;
                      height: 100vh;
                      margin: 0;
                      background: linear-gradient(135deg, #667eea 0%, #764ba2 100%);
                      color: white;
                  }
                  .container {
                      text-align: center;
                      padding: 2rem;
                      background: rgba(255, 255, 255, 0.1);
                      border-radius: 10px;
                      backdrop-filter: blur(10px);
                  }
                  .badge {
                      display: inline-block;
                      padding: 0.5rem 1rem;
                      background: $BADGE_COLOR;
                      border-radius: 20px;
                      font-weight: bold;
                      margin-bottom: 1rem;
                  }
                  .spinner {
                      margin: 1rem auto;
                      width: 40px;
                      height: 40px;
                      border: 3px solid rgba(255, 255, 255, 0.3);
                      border-top-color: white;
                      border-radius: 50%;
                      animation: spin 0.8s linear infinite;
                  }
                  @keyframes spin {
                      to { transform: rotate(360deg); }
                  }
                  a { color: white; text-decoration: underline; }
                  .meta { font-size: 0.9rem; opacity: 0.8; margin-top: 1rem; }
              </style>
          </head>
          <body>
              <div class="container">
                  <div class="badge">$BADGE_TEXT</div>
                  <h2>Loading Test Report...</h2>
                  <div class="spinner"></div>
                  <p>Report #${{ github.run_number }} (Unit + Integration + E2E Tests)</p>
                  <p><a href="../${{ github.run_number }}/">Click here if not redirected</a></p>
                  <div class="meta">
                      Branch: $BRANCH_NAME | Commit: ${{ github.sha }}
                  </div>
              </div>
              <script>
                  setTimeout(() => window.location.href = '../${{ github.run_number }}/', 100);
              </script>
          </body>
          </html>
          REDIRECT_EOF

          echo "‚úÖ Created latest redirect for $ENV_NAME (run #${{ github.run_number }})"

      - name: "üì¶ Prepare GitHub Pages artifact structure"
        run: |
          echo "üì¶ Creating GitHub Pages artifact structure..."

          mkdir -p pages-artifact/test-reports

          # Copy Allure report
          if [ -d "backend/allure-report" ]; then
            echo "‚úÖ Copying unified Allure report to test-reports/${{ github.run_number }}/"
            cp -r backend/allure-report pages-artifact/test-reports/${{ github.run_number }}
          else
            echo "‚ö†Ô∏è Allure report not found, creating placeholder"
            mkdir -p pages-artifact/test-reports/${{ github.run_number }}
            echo "<h1>No test results available</h1>" > pages-artifact/test-reports/${{ github.run_number }}/index.html
          fi

          # Copy latest redirect
          if [ -d "test-reports-latest" ]; then
            echo "‚úÖ Copying latest redirect"
            mkdir -p pages-artifact/test-reports/latest
            cp -r test-reports-latest/* pages-artifact/test-reports/latest/
          fi

          # Create root index.html
          cat > pages-artifact/index.html <<'ROOT_INDEX_EOF'
          <!DOCTYPE html>
          <html lang="en">
          <head>
              <meta charset="UTF-8">
              <meta name="viewport" content="width=device-width, initial-scale=1.0">
              <meta http-equiv="refresh" content="0; url=./test-reports/latest/">
              <title>Loyalty App - Test Reports</title>
          </head>
          <body>
              <p>Redirecting to <a href="./test-reports/latest/">latest test report</a>...</p>
          </body>
          </html>
          ROOT_INDEX_EOF

          echo "‚úÖ GitHub Pages artifact structure prepared"
          du -sh pages-artifact/

      - name: "üì§ Upload GitHub Pages artifact"
        uses: actions/upload-pages-artifact@v4
        with:
          path: pages-artifact
          retention-days: 90
          # Note: artifact name is always 'github-pages' for this action

  # =============================================================================
  # GITHUB PAGES DEPLOYMENT (Deploys unified test reports)
  # Uses concurrency to prevent duplicate artifact conflicts
  # =============================================================================

  deploy-github-pages:
    name: "üìÑ Deploy to GitHub Pages"
    runs-on: ubuntu-latest
    needs: [generate-test-reports]
    if: always() && github.ref == 'refs/heads/main'

    permissions:
      pages: write
      id-token: write
      contents: read

    environment:
      name: github-pages
      url: ${{ steps.deployment.outputs.page_url }}

    # Strict concurrency - only one pages deployment at a time
    # This prevents the "Multiple artifacts named github-pages" error
    concurrency:
      group: "pages-deploy-${{ github.repository }}"
      cancel-in-progress: true

    steps:
      - name: "‚è≥ Wait for artifact to be ready"
        run: |
          echo "‚è≥ Waiting for artifact processing..."
          sleep 5
          echo "‚úÖ Proceeding with deployment"

      - name: "üöÄ Deploy to GitHub Pages"
        id: deployment
        uses: actions/deploy-pages@v4
        with:
          timeout: 600000
          error_count: 10
          reporting_interval: 5000
        # Uses default artifact_name: github-pages

      - name: "‚úÖ Deployment complete"
        run: |
          echo "üìÑ GitHub Pages deployed successfully!"
          echo "üîó URL: ${{ steps.deployment.outputs.page_url }}"
          echo "üìä Test Reports: ${{ steps.deployment.outputs.page_url }}test-reports/latest/"

# =============================================================================
# PIPELINE SUMMARY:
#
# üìä PERFORMANCE OPTIMIZATIONS:
# - Parallel job execution with isolated environments (prevents conflicts)
# - Port isolation strategy (Unit: 5438/6383, Integration: 5437/6382, E2E: 5436/6381/4202/3201, Dev: 5435/6380)
# - Container isolation with unique project names and containers
# - Intelligent caching (npm, Docker BuildKit)
# - Conditional jobs (E2E only on main/PR to main)
# - Smart dependency installation
# - Shallow git clones
# - Optimized Docker builds
#
# üîí SECURITY & QUALITY:
# - ESLint security rules
# - npm audit
# - Custom security validation
# - Test integrity validation (prevents test bypassing)
# - TypeScript type checking
# - Unit tests (parallel)
# - Integration tests (parallel)
# - E2E tests (conditional)
# - Database schema tests
# - OAuth health validation (pre & post deployment)
# - Database migration validation & rollback safety
# - OAuth E2E validation tests
#
# ‚ö° ESTIMATED TIMES:
# - Phase 0 (Workspace Prep): 30-60 seconds
# - Phase 1A (Security): 3-4 minutes (parallel)
# - Phase 1B (Unit Tests): 3-4 minutes (parallel)
# - Phase 1C (Integration Tests): 4-5 minutes (parallel)
# - Phase 1D (E2E Tests): 8-12 minutes (conditional, after workspace prep)
# - Phase 2 (Build): 2-3 minutes (only on main, after all tests)
# - Phase 3 (Deploy): 3-5 minutes (only on main, production)
# - Total: 11-15 minutes (improved from 13-18 minutes due to parallel tests)
#
# üéØ IMPROVEMENTS:
# - 100% environment isolation (prevents race conditions)
# - PARALLEL unit and integration tests (saves 2-3 minutes)
# - Comprehensive test coverage with parallel security analysis
# - Enhanced security validation
# - Better error handling and reporting
# - Smart caching for dependencies
# - Conditional execution to save resources
# - Reliable testing environments with port separation
# - Independent test job outputs for better visibility
# =============================================================================
