name: Optimized CI/CD Pipeline with Security & Testing

on:
  push:
    branches: [main, develop]
  pull_request:
    branches: [main]
  workflow_dispatch:

env:
  DEPLOY_PATH: /home/nut/loyalty-app
  NODE_VERSION: 24
  CACHE_VERSION: v1

jobs:
  # =============================================================================
  # PHASE 0: SHARED WORKSPACE PREPARATION (NEW - 30-60 seconds)
  # =============================================================================

  prepare-workspace:
    name: "ðŸš€ Workspace Preparation"
    runs-on: self-hosted
    timeout-minutes: 2
    outputs:
      workspace-ready: ${{ steps.setup-complete.outputs.ready }}
      backend-cache-hit: ${{ steps.backend-cache.outputs.cache-hit }}
      frontend-cache-hit: ${{ steps.frontend-cache.outputs.cache-hit }}

    steps:
      - name: "ðŸ“¥ Checkout code (shared for all jobs)"
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          clean: true

      - name: "âš¡ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: "ðŸš€ Setup local cache strategy"
        id: cache-setup
        run: |
          echo "ðŸš€ Using local cache strategy for self-hosted runner"

          LOCAL_CACHE_BASE="/home/nut/.cache/loyalty-app"
          NPM_CACHE="/home/nut/.npm-cache"

          mkdir -p "$LOCAL_CACHE_BASE"/{frontend,backend} "$NPM_CACHE"

          FRONTEND_KEY=$(shasum frontend/package-lock.json 2>/dev/null | cut -d' ' -f1 || echo "no-lock")
          BACKEND_KEY=$(shasum backend/package-lock.json 2>/dev/null | cut -d' ' -f1 || echo "no-lock")

          echo "frontend_cache_key=$FRONTEND_KEY" >> $GITHUB_OUTPUT
          echo "backend_cache_key=$BACKEND_KEY" >> $GITHUB_OUTPUT
          echo "cache_base=$LOCAL_CACHE_BASE" >> $GITHUB_OUTPUT
          echo "npm_cache=$NPM_CACHE" >> $GITHUB_OUTPUT

      - name: "ðŸ“¦ Install backend dependencies"
        id: backend-cache
        working-directory: ./backend
        run: |
          LOCAL_CACHE_BASE="${{ steps.cache-setup.outputs.cache_base }}"
          NPM_CACHE="${{ steps.cache-setup.outputs.npm_cache }}"

          npm config set cache "$NPM_CACHE"

          # Check cache
          CACHE_HIT=false
          if [ -f "$LOCAL_CACHE_BASE/backend/.cache-key" ]; then
            STORED_KEY=$(cat "$LOCAL_CACHE_BASE/backend/.cache-key")
            if [ "$STORED_KEY" = "${{ steps.cache-setup.outputs.backend_cache_key }}" ]; then
              CACHE_HIT=true
              echo "âš¡ Restoring backend from cache..."
              cp -r "$LOCAL_CACHE_BASE/backend/node_modules" .
              echo "cache-hit=true" >> $GITHUB_OUTPUT
            fi
          fi

          if [ "$CACHE_HIT" = false ]; then
            echo "ðŸ“¦ Installing backend dependencies..."
            npm ci --prefer-offline --include=dev

            echo "ðŸ’¾ Caching backend dependencies..."
            cp -r node_modules "$LOCAL_CACHE_BASE/backend/"
            echo "${{ steps.cache-setup.outputs.backend_cache_key }}" > "$LOCAL_CACHE_BASE/backend/.cache-key"
            echo "cache-hit=false" >> $GITHUB_OUTPUT
          fi

      - name: "ðŸ“¦ Install frontend dependencies"
        id: frontend-cache
        working-directory: ./frontend
        run: |
          LOCAL_CACHE_BASE="${{ steps.cache-setup.outputs.cache_base }}"
          NPM_CACHE="${{ steps.cache-setup.outputs.npm_cache }}"

          npm config set cache "$NPM_CACHE"

          # Check cache
          CACHE_HIT=false
          if [ -f "$LOCAL_CACHE_BASE/frontend/.cache-key" ]; then
            STORED_KEY=$(cat "$LOCAL_CACHE_BASE/frontend/.cache-key")
            if [ "$STORED_KEY" = "${{ steps.cache-setup.outputs.frontend_cache_key }}" ]; then
              CACHE_HIT=true
              echo "âš¡ Restoring frontend from cache..."
              cp -r "$LOCAL_CACHE_BASE/frontend/node_modules" .
              echo "cache-hit=true" >> $GITHUB_OUTPUT
            fi
          fi

          if [ "$CACHE_HIT" = false ]; then
            echo "ðŸ“¦ Installing frontend dependencies..."
            npm ci --prefer-offline

            echo "ðŸ’¾ Caching frontend dependencies..."
            cp -r node_modules "$LOCAL_CACHE_BASE/frontend/"
            echo "${{ steps.cache-setup.outputs.frontend_cache_key }}" > "$LOCAL_CACHE_BASE/frontend/.cache-key"
            echo "cache-hit=false" >> $GITHUB_OUTPUT
          fi

      - name: "ðŸ“¦ Install root dependencies (Playwright)"
        id: root-cache
        run: |
          LOCAL_CACHE_BASE="${{ steps.cache-setup.outputs.cache_base }}"
          NPM_CACHE="${{ steps.cache-setup.outputs.npm_cache }}"

          npm config set cache "$NPM_CACHE"

          # Generate cache key from root package-lock.json
          ROOT_KEY=$(shasum package-lock.json 2>/dev/null | cut -d' ' -f1 || echo "no-lock")

          # Check cache
          CACHE_HIT=false
          if [ -f "$LOCAL_CACHE_BASE/root/.cache-key" ]; then
            STORED_KEY=$(cat "$LOCAL_CACHE_BASE/root/.cache-key")
            if [ "$STORED_KEY" = "$ROOT_KEY" ]; then
              CACHE_HIT=true
              echo "âš¡ Restoring root dependencies from cache..."
              mkdir -p node_modules
              cp -r "$LOCAL_CACHE_BASE/root/node_modules/"* node_modules/ 2>/dev/null || true
              echo "cache-hit=true" >> $GITHUB_OUTPUT
            fi
          fi

          if [ "$CACHE_HIT" = false ]; then
            echo "ðŸ“¦ Installing root dependencies (Playwright)..."
            npm ci --prefer-offline

            echo "ðŸ’¾ Caching root dependencies..."
            mkdir -p "$LOCAL_CACHE_BASE/root"
            cp -r node_modules "$LOCAL_CACHE_BASE/root/"
            echo "$ROOT_KEY" > "$LOCAL_CACHE_BASE/root/.cache-key"
            echo "cache-hit=false" >> $GITHUB_OUTPUT
          fi

          echo "âœ… Root dependencies installed (includes @playwright/test)"

      - name: "ðŸ”§ Generate Prisma client (shared)"
        working-directory: ./backend
        run: |
          echo "ðŸ”§ Generating Prisma client for all jobs..."
          npm run db:generate

          if [ ! -d "src/generated/prisma" ]; then
            echo "âŒ Prisma client generation failed"
            exit 1
          fi

          echo "âœ… Prisma client generated successfully"

      - name: "ðŸ“Š Workspace preparation complete"
        id: setup-complete
        run: |
          echo "âœ… Workspace preparation completed"
          echo "Backend cache hit: ${{ steps.backend-cache.outputs.cache-hit }}"
          echo "Frontend cache hit: ${{ steps.frontend-cache.outputs.cache-hit }}"
          echo "ready=true" >> $GITHUB_OUTPUT

      - name: "ðŸ“¤ Upload workspace as artifact"
        uses: actions/upload-artifact@v4
        with:
          name: ci-workspace-${{ github.run_id }}
          path: |
            .
            !.git
            !node_modules
            !backend/node_modules
            !frontend/node_modules
          retention-days: 1
          compression-level: 0

  # =============================================================================
  # PHASE 1: PARALLEL VALIDATION & SECURITY (3-4 minutes)
  # =============================================================================

  # Job 1A: Code Quality & Security Analysis (Parallel)
  security-analysis:
    name: "ðŸ”’ Security & Code Quality"
    runs-on: self-hosted
    needs: prepare-workspace
    timeout-minutes: 3
    outputs:
      security-passed: ${{ steps.security-results.outputs.passed }}

    steps:
      - name: "ðŸ“¥ Download shared workspace"
        uses: actions/download-artifact@v4
        with:
          name: ci-workspace-${{ github.run_id }}
          path: .

      - name: "âš¡ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: "â™»ï¸ Restore dependencies from local cache"
        run: |
          LOCAL_CACHE_BASE="/home/nut/.cache/loyalty-app"

          echo "â™»ï¸ Restoring backend node_modules from local cache..."
          if [ -d "$LOCAL_CACHE_BASE/backend/node_modules" ]; then
            cp -r "$LOCAL_CACHE_BASE/backend/node_modules" backend/
            echo "âœ… Backend dependencies restored"
          else
            echo "âŒ Backend cache not found"
            exit 1
          fi

          echo "â™»ï¸ Restoring frontend node_modules from local cache..."
          if [ -d "$LOCAL_CACHE_BASE/frontend/node_modules" ]; then
            cp -r "$LOCAL_CACHE_BASE/frontend/node_modules" frontend/
            echo "âœ… Frontend dependencies restored"
          else
            echo "âŒ Frontend cache not found"
            exit 1
          fi

      - name: "ðŸ” TypeScript type checking (validation)"
        working-directory: ./backend
        run: npm run typecheck
      
      - name: "ðŸ”’ Security linting (ESLint + Security rules)"
        working-directory: ./backend
        run: npm run lint:security
      
      - name: "ðŸ›¡ï¸ Security audit (npm audit)"
        working-directory: ./backend
        run: npm run security:audit
      
      - name: "ðŸ”’ Run custom security validation"
        working-directory: ./backend
        run: node scripts/validate-security.js
      
      - name: "ðŸ›¡ï¸ Test integrity validation"
        run: |
          echo "ðŸ›¡ï¸ Running test integrity validation..."
          if [ -f "./scripts/validate-test-integrity.sh" ]; then
            ./scripts/validate-test-integrity.sh
          else
            echo "âš ï¸ Test integrity script not found - skipping"
          fi
      
      - name: "ðŸ“Š Security results summary (reporting only)"
        id: security-results
        run: |
          echo "Security analysis completed successfully"
          echo "passed=true" >> $GITHUB_OUTPUT

  # Job 1B: Unit & Integration Tests (Parallel)
  unit-integration-tests:
    name: "ðŸ§ª Unit & Integration Tests"
    runs-on: self-hosted
    needs: prepare-workspace
    timeout-minutes: 6
    outputs:
      tests-passed: ${{ steps.test-results.outputs.passed }}
      coverage-percent: ${{ steps.test-results.outputs.coverage }}

    # Use isolated test environment to prevent conflicts with E2E tests

    steps:
      - name: "ðŸ“¥ Download shared workspace"
        uses: actions/download-artifact@v4
        with:
          name: ci-workspace-${{ github.run_id }}
          path: .

      - name: "âš¡ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: "â™»ï¸ Restore dependencies from local cache"
        run: |
          LOCAL_CACHE_BASE="/home/nut/.cache/loyalty-app"

          echo "â™»ï¸ Restoring backend node_modules from local cache..."
          if [ -d "$LOCAL_CACHE_BASE/backend/node_modules" ]; then
            cp -r "$LOCAL_CACHE_BASE/backend/node_modules" backend/
            echo "âœ… Backend dependencies restored"
          else
            echo "âŒ Backend cache not found"
            exit 1
          fi

          echo "â™»ï¸ Restoring frontend node_modules from local cache..."
          if [ -d "$LOCAL_CACHE_BASE/frontend/node_modules" ]; then
            cp -r "$LOCAL_CACHE_BASE/frontend/node_modules" frontend/
            echo "âœ… Frontend dependencies restored"
          else
            echo "âŒ Frontend cache not found"
            exit 1
          fi

      - name: "ðŸ³ Start isolated test services"
        run: |
          # Use unique project name and ports to avoid conflicts with E2E tests
          export COMPOSE_PROJECT_NAME="loyalty-unit-tests"
          export POSTGRES_PORT="5435"
          export REDIS_PORT="6380"
          
          # Start test services with isolated configuration
          echo "ðŸš€ Starting isolated test services on custom ports..."
          
          # Create temporary compose file for unit tests with isolated ports
          cat > docker-compose.unit-test.yml << 'EOF'
          version: '3.8'
          services:
            postgres:
              image: postgres:15-alpine
              container_name: loyalty_postgres_unit
              environment:
                POSTGRES_USER: loyalty
                POSTGRES_PASSWORD: loyalty_password
                POSTGRES_DB: loyalty_db
              ports:
                - "5435:5432"
              volumes:
                - loyalty_postgres_unit_data:/var/lib/postgresql/data
              healthcheck:
                test: ["CMD-SHELL", "pg_isready -U loyalty -d loyalty_db"]
                interval: 5s
                timeout: 5s
                retries: 5
            
            redis:
              image: redis:7-alpine
              container_name: loyalty_redis_unit
              ports:
                - "6380:6379"
              volumes:
                - loyalty_redis_unit_data:/data
              healthcheck:
                test: ["CMD", "redis-cli", "ping"]
                interval: 5s
                timeout: 3s
                retries: 5
          
          volumes:
            loyalty_postgres_unit_data:
            loyalty_redis_unit_data:
          EOF
          
          # Start isolated test services
          docker compose -f docker-compose.unit-test.yml up -d
          
          # Wait for services to be ready with health checks
          echo "â³ Waiting for isolated test services to be ready..."
          timeout 60 bash -c 'until docker compose -f docker-compose.unit-test.yml exec -T postgres pg_isready -U loyalty -d loyalty_db; do echo "Waiting for isolated postgres..."; sleep 2; done'
          timeout 30 bash -c 'until docker compose -f docker-compose.unit-test.yml exec -T redis redis-cli ping | grep -q PONG; do echo "Waiting for isolated redis..."; sleep 2; done'
          
          # Configure isolated test database
          echo "ðŸ—„ï¸ Setting up isolated test database..."
          docker exec loyalty_postgres_unit psql -U loyalty -d postgres -c "DROP DATABASE IF EXISTS loyalty_test_db;" || true
          docker exec loyalty_postgres_unit psql -U loyalty -d postgres -c "DROP USER IF EXISTS loyalty_test;" || true  
          docker exec loyalty_postgres_unit psql -U loyalty -d postgres -c "CREATE USER loyalty_test WITH PASSWORD 'test_password';" || true
          docker exec loyalty_postgres_unit psql -U loyalty -d postgres -c "CREATE DATABASE loyalty_test_db OWNER loyalty_test;" || true
          docker exec loyalty_postgres_unit psql -U loyalty -d postgres -c "GRANT ALL PRIVILEGES ON DATABASE loyalty_test_db TO loyalty_test;" || true
          
          # Enable UUID extension in isolated test database
          docker exec loyalty_postgres_unit psql -U loyalty_test -d loyalty_test_db -c "CREATE EXTENSION IF NOT EXISTS \"uuid-ossp\";" || true
          
          echo "âœ… Isolated test services are ready on ports 5435 (postgres) and 6380 (redis)"
      
      - name: "ðŸ”§ Setup isolated test environment"
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://loyalty_test:test_password@localhost:5435/loyalty_test_db
          REDIS_URL: redis://localhost:6380
          NODE_ENV: test
          JWT_SECRET: test-jwt-secret-for-testing-only-min-32-chars
          JWT_REFRESH_SECRET: test-refresh-secret-for-testing-only-min-32-chars
          SESSION_SECRET: test-session-secret-for-testing-only-min-32-chars
        run: |
          # Generate Prisma client for tests
          npm run db:generate
          
          # Validate Prisma client was generated successfully
          if [ ! -d "src/generated/prisma" ]; then
            echo "âŒ Prisma client generation failed - directory not found"
            exit 1
          fi
          
          if [ ! -f "src/generated/prisma/index.js" ]; then
            echo "âŒ Prisma client generation failed - index.js not found"
            exit 1
          fi
          
          echo "âœ… Prisma client validated successfully"
          
          # Run database migrations for test DB
          npm run db:migrate:deploy
      
      - name: "ðŸ§ª Run unit tests (isolated)"
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://loyalty_test:test_password@localhost:5435/loyalty_test_db
          REDIS_URL: redis://localhost:6380
          NODE_ENV: test
          JWT_SECRET: test-jwt-secret-for-testing-only-min-32-chars
          JWT_REFRESH_SECRET: test-refresh-secret-for-testing-only-min-32-chars
          SESSION_SECRET: test-session-secret-for-testing-only-min-32-chars
        run: npm run test:unit -- --coverage --passWithNoTests
      
      - name: "ðŸ§ª Run integration tests (isolated)"
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://loyalty_test:test_password@localhost:5435/loyalty_test_db
          REDIS_URL: redis://localhost:6380
          NODE_ENV: test
          JWT_SECRET: test-jwt-secret-for-testing-only-min-32-chars
          JWT_REFRESH_SECRET: test-refresh-secret-for-testing-only-min-32-chars
          SESSION_SECRET: test-session-secret-for-testing-only-min-32-chars
        run: npm run test:integration -- --passWithNoTests
      
      - name: "ðŸ§ª Run database schema tests (isolated)"
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://loyalty_test:test_password@localhost:5435/loyalty_test_db
          NODE_ENV: test
        run: npm run test:db -- --passWithNoTests
      
      - name: "ðŸ“Š Upload coverage reports"
        uses: actions/upload-artifact@v4
        if: always()
        with:
          name: coverage-reports
          path: backend/coverage/
          retention-days: 7
      
      - name: "ðŸ” OAuth validation tests"
        run: |
          echo "ðŸ” Running OAuth validation tests..."
          # Check if services are running
          if curl -s http://localhost:4001/api/health >/dev/null 2>&1; then
            echo "âœ… Backend service is running, performing OAuth validation"
            if [ -f "./scripts/validate-oauth-health.sh" ]; then
              ./scripts/validate-oauth-health.sh || echo "âš ï¸ OAuth validation failed (non-blocking in CI)"
            else
              echo "âš ï¸ OAuth validation script not found - skipping"
            fi
          else
            echo "âš ï¸ Backend not running - skipping OAuth validation"
          fi
      
      - name: "ðŸ—„ï¸ Database migration validation (isolated)"
        env:
          DATABASE_URL: postgresql://loyalty_test:test_password@localhost:5435/loyalty_test_db
          NODE_ENV: test
        run: |
          echo "ðŸ—„ï¸ Running database migration validation on isolated environment..."
          if [ -f "./scripts/validate-db-migration.sh" ]; then
            ./scripts/validate-db-migration.sh || echo "âš ï¸ DB migration validation failed (non-blocking in CI)"
          else
            echo "âš ï¸ Database migration validation script not found - skipping"
          fi
      
      - name: "ðŸ“ˆ Test results summary (reporting only)"
        id: test-results
        run: |
          echo "Unit and integration tests completed"
          # Extract coverage percentage if available
          if [ -f backend/coverage/lcov.info ]; then
            COVERAGE=$(grep -o "LF:[0-9]*" backend/coverage/lcov.info | head -1 | cut -d: -f2 || echo "0")
            echo "coverage=$COVERAGE" >> $GITHUB_OUTPUT
          else
            echo "coverage=0" >> $GITHUB_OUTPUT
          fi
          echo "passed=true" >> $GITHUB_OUTPUT
      
      - name: "ðŸ§¹ Cleanup isolated test environment"
        if: always()
        run: |
          echo "ðŸ§¹ Cleaning up isolated test services..."
          docker compose -f docker-compose.unit-test.yml down -v --remove-orphans || true
          docker system prune -f --volumes || true
          rm -f docker-compose.unit-test.yml || true
          echo "âœ… Isolated test environment cleaned up"

  # Job 1C: E2E Tests (Conditional - only on main branch or PR to main, after unit tests)
  e2e-tests:
    name: "ðŸŽ­ E2E Tests"
    runs-on: self-hosted
    needs: prepare-workspace
    timeout-minutes: 8
    if: (github.ref == 'refs/heads/main' || github.base_ref == 'main')
    outputs:
      e2e-passed: ${{ steps.e2e-results.outputs.passed }}

    steps:
      - name: "ðŸ“¥ Download shared workspace"
        uses: actions/download-artifact@v4
        with:
          name: ci-workspace-${{ github.run_id }}
          path: .

      - name: "âš¡ Setup Node.js"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}

      - name: "â™»ï¸ Restore dependencies from local cache"
        run: |
          LOCAL_CACHE_BASE="/home/nut/.cache/loyalty-app"

          echo "â™»ï¸ Restoring root node_modules (Playwright) from local cache..."
          if [ -d "$LOCAL_CACHE_BASE/root/node_modules" ]; then
            mkdir -p node_modules
            cp -r "$LOCAL_CACHE_BASE/root/node_modules/"* node_modules/
            echo "âœ… Root dependencies restored (includes @playwright/test)"
          else
            echo "âŒ Root cache not found, installing..."
            npm ci --prefer-offline
            echo "âœ… Root dependencies installed"
          fi

          echo "â™»ï¸ Restoring backend node_modules from local cache..."
          if [ -d "$LOCAL_CACHE_BASE/backend/node_modules" ]; then
            cp -r "$LOCAL_CACHE_BASE/backend/node_modules" backend/
            echo "âœ… Backend dependencies restored"
          else
            echo "âŒ Backend cache not found"
            exit 1
          fi

          echo "â™»ï¸ Restoring frontend node_modules from local cache..."
          if [ -d "$LOCAL_CACHE_BASE/frontend/node_modules" ]; then
            cp -r "$LOCAL_CACHE_BASE/frontend/node_modules" frontend/
            echo "âœ… Frontend dependencies restored"
          else
            echo "âŒ Frontend cache not found"
            exit 1
          fi

      - name: "ðŸ”§ Setup isolated E2E environment"
        run: |
          # Use unique project name and ports to avoid conflicts with unit tests
          export COMPOSE_PROJECT_NAME="loyalty-e2e-tests"
          export POSTGRES_PORT="5436"
          export REDIS_PORT="6381"
          export BACKEND_PORT="4202"
          export FRONTEND_PORT="3201"
          
          echo "ðŸš€ Setting up isolated E2E environment on custom ports..."
          
          # Database services will be started with the complete compose file after build
      
      - name: "ðŸ—ï¸ Build and start isolated application for E2E"
        env:
          DATABASE_URL: postgresql://loyalty:loyalty_password@localhost:5436/loyalty_db
          REDIS_URL: redis://localhost:6381
          NODE_ENV: development
          JWT_SECRET: e2e-jwt-secret-for-testing-only-min-32-chars
          JWT_REFRESH_SECRET: e2e-refresh-secret-for-testing-only-min-32-chars
          SESSION_SECRET: e2e-session-secret-for-testing-only-min-32-chars
          FRONTEND_URL: http://localhost:3201
          BACKEND_URL: http://localhost:4202
          VITE_API_URL: http://localhost:4202/api
        run: |
          # Validate Prisma client from workspace (already generated)
          echo "ðŸ” Validating Prisma client from shared workspace..."
          if [ ! -d "backend/src/generated/prisma" ]; then
            echo "âŒ Prisma client not found - workspace preparation may have failed"
            exit 1
          fi

          echo "âœ… Prisma client validated successfully"

          # Clean up any existing E2E containers first (before creating new compose file)
          echo "ðŸ§¹ Cleaning up any existing E2E containers..."
          docker stop loyalty_postgres_e2e loyalty_redis_e2e loyalty_backend_e2e loyalty_frontend_e2e 2>/dev/null || true
          docker rm loyalty_postgres_e2e loyalty_redis_e2e loyalty_backend_e2e loyalty_frontend_e2e 2>/dev/null || true
          
          # Clean up any existing compose stack if file exists
          if [ -f "docker-compose.e2e-test.yml" ]; then
            echo "ðŸ§¹ Cleaning up existing E2E compose stack..."
            docker compose -f docker-compose.e2e-test.yml down -v --remove-orphans 2>/dev/null || true
            rm -f docker-compose.e2e-test.yml
          fi
          
          # Critical: Remove E2E database volumes to ensure clean state
          echo "ðŸ§¹ Removing E2E database volumes to prevent migration state corruption..."
          docker volume rm loyalty_postgres_e2e_data loyalty_redis_e2e_data 2>/dev/null || true
          docker volume rm loyalty-e2e-tests_loyalty_postgres_e2e_data loyalty-e2e-tests_loyalty_redis_e2e_data 2>/dev/null || true
          
          # Build backend and frontend
          echo "ðŸ—ï¸ Building application for isolated E2E testing..."
          npm run build &
          cd ../frontend && npm run build &
          wait
          
          # Return to project root to create Docker Compose file
          cd ..
          
          # Create complete E2E compose file (replace the initial file with complete version)
          cat > docker-compose.e2e-test.yml << 'EOF'
          services:
            postgres:
              image: postgres:15-alpine
              container_name: loyalty_postgres_e2e
              environment:
                POSTGRES_USER: loyalty
                POSTGRES_PASSWORD: loyalty_password
                POSTGRES_DB: loyalty_db
              ports:
                - "5436:5432"
              volumes:
                - loyalty_postgres_e2e_data:/var/lib/postgresql/data
              healthcheck:
                test: ["CMD-SHELL", "pg_isready -U loyalty -d loyalty_db"]
                interval: 5s
                timeout: 5s
                retries: 5
            
            redis:
              image: redis:7-alpine
              container_name: loyalty_redis_e2e
              ports:
                - "6381:6379"
              volumes:
                - loyalty_redis_e2e_data:/data
              healthcheck:
                test: ["CMD", "redis-cli", "ping"]
                interval: 5s
                timeout: 3s
                retries: 5
            
            backend:
              build:
                context: ./backend
                dockerfile: Dockerfile
              container_name: loyalty_backend_e2e
              environment:
                NODE_ENV: development
                DATABASE_URL: postgresql://loyalty:loyalty_password@postgres:5432/loyalty_db
                REDIS_URL: redis://redis:6379
                JWT_SECRET: e2e-jwt-secret-for-testing-only-min-32-chars
                JWT_REFRESH_SECRET: e2e-refresh-secret-for-testing-only-min-32-chars
                SESSION_SECRET: e2e-session-secret-for-testing-only-min-32-chars
                FRONTEND_URL: http://localhost:3201
                BACKEND_URL: http://localhost:4202
                CORS_ORIGINS: "http://localhost:3201"
              ports:
                - "4202:4000"
              depends_on:
                postgres:
                  condition: service_healthy
                redis:
                  condition: service_healthy
              healthcheck:
                test: ["CMD", "curl", "-f", "http://localhost:4000/api/health"]
                interval: 10s
                timeout: 5s
                retries: 5
                start_period: 30s
            
            frontend:
              build:
                context: ./frontend
                dockerfile: Dockerfile
                args:
                  VITE_API_URL: http://localhost:4202/api
              container_name: loyalty_frontend_e2e
              ports:
                - "3201:80"
              depends_on:
                backend:
                  condition: service_healthy
          
          volumes:
            loyalty_postgres_e2e_data:
            loyalty_redis_e2e_data:
          EOF
          
          # Validate the generated Docker Compose file
          echo "ðŸ” Validating E2E Docker Compose configuration..."
          if ! docker compose -f docker-compose.e2e-test.yml config >/dev/null 2>&1; then
            echo "âŒ E2E Docker Compose validation failed"
            echo "Generated compose file contents:"
            cat docker-compose.e2e-test.yml
            exit 1
          fi
          
          echo "âœ… E2E Docker Compose configuration validated"
          
          # Start database services first
          echo "ðŸ—„ï¸ Starting isolated database services..."
          docker compose -f docker-compose.e2e-test.yml up -d postgres redis
          
          # Wait for database services to be ready using healthcheck status
          echo "â³ Waiting for isolated database services to be ready..."

          # Wait for postgres healthcheck to pass
          POSTGRES_READY=false
          for i in {1..30}; do
            if docker compose -f docker-compose.e2e-test.yml ps postgres | grep -q "healthy"; then
              echo "âœ… Postgres is healthy"
              POSTGRES_READY=true
              break
            fi
            echo "Waiting for postgres healthcheck... ($i/30)"
            sleep 2
          done

          if [ "$POSTGRES_READY" = false ]; then
            echo "âŒ Postgres failed to become healthy"
            docker compose -f docker-compose.e2e-test.yml logs postgres
            exit 1
          fi

          # Wait for redis healthcheck to pass
          REDIS_READY=false
          for i in {1..30}; do
            if docker compose -f docker-compose.e2e-test.yml ps redis | grep -q "healthy"; then
              echo "âœ… Redis is healthy"
              REDIS_READY=true
              break
            fi
            echo "Waiting for redis healthcheck... ($i/30)"
            sleep 2
          done

          if [ "$REDIS_READY" = false ]; then
            echo "âŒ Redis failed to become healthy"
            docker compose -f docker-compose.e2e-test.yml logs redis
            exit 1
          fi

          echo "âœ… Isolated database services are ready"
          
          # Run database migrations before starting the backend
          echo "ðŸ”„ Running database migrations for E2E tests..."
      
      - name: "ðŸ”„ E2E Database Migration Setup"
        working-directory: ./backend
        env:
          DATABASE_URL: postgresql://loyalty:loyalty_password@localhost:5436/loyalty_db
          NODE_ENV: development
        run: |
          echo "ðŸ”„ Setting up E2E database migrations..."
          
          # Generate Prisma client if not already done
          if [ ! -d "src/generated/prisma" ]; then
            npm run db:generate
          fi
          
          # Critical fix: Reset any failed migration state to prevent P3009 error
          echo "ðŸ”„ Resetting any failed migration state for clean E2E environment..."
          npx prisma migrate reset --force --skip-seed 2>/dev/null || true
          
          # Run migrations with fresh state
          if npm run db:migrate:deploy; then
            echo "âœ… E2E database migrations completed successfully"
          else
            echo "âŒ E2E database migrations failed, attempting recovery..."
            echo "ðŸ“Š Migration status for debugging:"
            npx prisma migrate status || true
            
            # Advanced fix: Handle P3009 error by resolving failed migrations
            echo "ðŸ”§ Attempting to resolve failed migration state..."
            if npx prisma migrate resolve --applied 0_init 2>/dev/null; then
              echo "âœ… Failed migration marked as applied, retrying deploy..."
              if npm run db:migrate:deploy; then
                echo "âœ… E2E database migrations completed after resolve"
              else
                echo "âŒ E2E database migrations still failing after resolve"
                echo "ðŸ“„ Database logs:"
                docker compose -f ../docker-compose.e2e-test.yml logs postgres
                exit 1
              fi
            else
              echo "âŒ Failed to resolve migration state"
              echo "ðŸ“„ Database logs:"
              docker compose -f ../docker-compose.e2e-test.yml logs postgres
              exit 1
            fi
          fi
          
          # Start the complete isolated application stack
          echo "ðŸš€ Starting isolated application stack for E2E tests..."
          cd ..
          
          # Create the E2E Docker Compose file
          echo "ðŸ“ Creating E2E Docker Compose configuration..."
          cat > docker-compose.e2e-test.yml << 'EOF'
          services:
            postgres:
              image: postgres:15-alpine
              container_name: loyalty_postgres_e2e
              environment:
                POSTGRES_USER: loyalty
                POSTGRES_PASSWORD: loyalty_password
                POSTGRES_DB: loyalty_db
              ports:
                - "5436:5432"
              volumes:
                - loyalty_postgres_e2e_data:/var/lib/postgresql/data
              healthcheck:
                test: ["CMD-SHELL", "pg_isready -U loyalty -d loyalty_db"]
                interval: 5s
                timeout: 5s
                retries: 5
            
            redis:
              image: redis:7-alpine
              container_name: loyalty_redis_e2e
              ports:
                - "6381:6379"
              volumes:
                - loyalty_redis_e2e_data:/data
              healthcheck:
                test: ["CMD", "redis-cli", "ping"]
                interval: 5s
                timeout: 3s
                retries: 5
            
            backend:
              build:
                context: ./backend
                dockerfile: Dockerfile
              container_name: loyalty_backend_e2e
              environment:
                NODE_ENV: development
                DATABASE_URL: postgresql://loyalty:loyalty_password@postgres:5432/loyalty_db
                REDIS_URL: redis://redis:6379
                JWT_SECRET: e2e-jwt-secret-for-testing-only-min-32-chars
                JWT_REFRESH_SECRET: e2e-refresh-secret-for-testing-only-min-32-chars
                SESSION_SECRET: e2e-session-secret-for-testing-only-min-32-chars
                FRONTEND_URL: http://localhost:3201
                BACKEND_URL: http://localhost:4202
                CORS_ORIGINS: "http://localhost:3201"
              ports:
                - "4202:4000"
              depends_on:
                postgres:
                  condition: service_healthy
                redis:
                  condition: service_healthy
              healthcheck:
                test: ["CMD-SHELL", "curl -f http://localhost:4000/api/health || exit 1"]
                interval: 10s
                timeout: 5s
                retries: 5
            
            frontend:
              build:
                context: ./frontend
                dockerfile: Dockerfile
                args:
                  VITE_API_URL: http://localhost:4202/api
              container_name: loyalty_frontend_e2e
              environment:
                NODE_ENV: development
              ports:
                - "3201:3000"
              depends_on:
                backend:
                  condition: service_healthy
              healthcheck:
                test: ["CMD-SHELL", "curl -f http://localhost:3000 || exit 1"]
                interval: 10s
                timeout: 5s
                retries: 3
          
          volumes:
            loyalty_postgres_e2e_data:
              driver: local
            loyalty_redis_e2e_data:
              driver: local
          EOF
          
          echo "âœ… E2E Docker Compose configuration created"
          
          # Clean up any existing E2E containers and ports to prevent conflicts
          echo "ðŸ§¹ Cleaning up any existing E2E containers and ports..."
          
          # Stop and remove containers
          docker stop loyalty_postgres_e2e loyalty_redis_e2e loyalty_backend_e2e loyalty_frontend_e2e 2>/dev/null || true
          docker rm loyalty_postgres_e2e loyalty_redis_e2e loyalty_backend_e2e loyalty_frontend_e2e 2>/dev/null || true
          
          # Check for processes using E2E ports and kill them
          echo "ðŸ” Checking for processes using E2E ports..."
          for port in 3201 4202 5436 6381; do
            pid=$(lsof -ti:$port 2>/dev/null || true)
            if [ ! -z "$pid" ]; then
              echo "âš ï¸ Found process $pid using port $port, terminating..."
              kill -9 $pid 2>/dev/null || true
              sleep 1
            fi
          done
          
          # Verify ports are free
          echo "âœ… Port cleanup completed"
          netstat -tlnp | grep -E ":320[1-9]|:420[2-9]|:543[6-9]|:638[1-9]" || echo "No conflicting ports found"
          
          # Start containers and capture any startup failures
          if docker compose -f docker-compose.e2e-test.yml up -d --build; then
            echo "âœ… Docker containers started successfully"
          else
            echo "âŒ Docker containers failed to start, capturing logs..."
            echo "ðŸ“„ Backend container logs:"
            docker logs loyalty_backend_e2e 2>&1 || echo "No backend logs available"
            echo "ðŸ“„ Frontend container logs:"
            docker logs loyalty_frontend_e2e 2>&1 || echo "No frontend logs available"
            echo "ðŸ“„ PostgreSQL container logs:"
            docker logs loyalty_postgres_e2e 2>&1 || echo "No postgres logs available"
            echo "ðŸ“„ Redis container logs:"
            docker logs loyalty_redis_e2e 2>&1 || echo "No redis logs available"
            echo "ðŸ“Š Container status:"
            docker compose -f docker-compose.e2e-test.yml ps
            exit 1
          fi
          
          # Wait for application to be ready
          echo "â³ Waiting for isolated application to be ready..."
          
          # Wait for backend with simplified retry logic 
          echo "ðŸ” Waiting for backend to be ready..."
          BACKEND_READY=false
          for i in {1..60}; do
            echo "Attempt $i/60: Testing backend connection..."
            if curl -s http://localhost:4202/api/health >/dev/null 2>&1; then
              echo "âœ… Backend is responding!"
              BACKEND_READY=true
              break
            fi
            echo "Backend not ready yet, waiting 5 seconds..."
            sleep 5
          done
          
          if [ "$BACKEND_READY" = false ]; then
            echo "âŒ Backend failed to become ready after 5 minutes (60 attempts)"
            echo "ðŸ” Final connection test:"
            curl -v http://localhost:4202/api/health || echo "Final curl test failed"
            echo "ðŸ“„ Backend container logs:"
            docker logs loyalty_backend_e2e 2>&1 || echo "No backend logs available"
            echo "ðŸ“Š Container status:"
            docker compose -f docker-compose.e2e-test.yml ps
            echo "ðŸŒ Port connectivity test:"
            netstat -tlnp | grep :4202 || echo "Port 4202 not found in netstat"
            exit 1
          fi
          
          # Wait for frontend with simplified retry logic
          echo "ðŸ” Waiting for frontend to be ready..."
          FRONTEND_READY=false
          for i in {1..40}; do
            echo "Attempt $i/40: Testing frontend connection..."
            if curl -s http://localhost:3201 >/dev/null 2>&1; then
              echo "âœ… Frontend is responding!"
              FRONTEND_READY=true
              break
            fi
            echo "Frontend not ready yet, waiting 3 seconds..."
            sleep 3
          done
          
          if [ "$FRONTEND_READY" = false ]; then
            echo "âŒ Frontend failed to become ready after 2 minutes (40 attempts)"
            echo "ðŸ” Final frontend connection test:"
            curl -v http://localhost:3201 || echo "Final frontend curl test failed"
            echo "ðŸ“„ Frontend container logs:"
            docker logs loyalty_frontend_e2e 2>&1 || echo "No frontend logs available"
            echo "ðŸ“Š Container status:"
            docker compose -f docker-compose.e2e-test.yml ps
            echo "ðŸŒ Frontend port connectivity test:"
            netstat -tlnp | grep :3201 || echo "Port 3201 not found in netstat"
            exit 1
          fi
          
          echo "âœ… Isolated application stack is ready for E2E testing"
          echo "ðŸŒ Backend: http://localhost:4202/api/health"
          echo "ðŸŒ Frontend: http://localhost:3201"
      
      - name: "ðŸŽ­ Run E2E tests with Playwright (isolated)"
        env:
          NODE_ENV: test
          DATABASE_URL: postgresql://loyalty:loyalty_password@localhost:5436/loyalty_db
          REDIS_URL: redis://localhost:6381
          BACKEND_URL: http://localhost:4202
          FRONTEND_URL: http://localhost:3201
        run: |
          echo "ðŸŽ­ Running E2E tests with Playwright on isolated environment..."
          echo "ðŸŒ Testing against Backend: http://localhost:4202"
          echo "ðŸŒ Testing against Frontend: http://localhost:3201"
          
          # Install Playwright browsers if needed
          npx playwright install --with-deps chromium

          # Run all E2E tests against isolated environment (BLOCKING - must pass to succeed)
          echo "ðŸŽ­ Running E2E tests - failures will BLOCK the pipeline..."
          npx playwright test --reporter=line

      - name: "ðŸ” OAuth E2E validation tests (isolated)"
        env:
          NODE_ENV: test
          BACKEND_URL: http://localhost:4202
          FRONTEND_URL: http://localhost:3201
        run: |
          echo "ðŸ” Running OAuth-specific E2E tests on isolated environment..."
          echo "ðŸ” Testing OAuth against: http://localhost:4202"
          # Run OAuth-specific tests separately for focused validation (BLOCKING - must pass to succeed)
          echo "ðŸ” OAuth E2E test failures will BLOCK the pipeline..."
          npx playwright test tests/oauth-validation.spec.ts --reporter=line
      
      - name: "ðŸ—„ï¸ Database migration rollback safety"
        run: |
          echo "ðŸ—„ï¸ Checking migration rollback safety..."
          if [ -f "./scripts/migration-rollback-safety.sh" ]; then
            ./scripts/migration-rollback-safety.sh check || echo "âš ï¸ Migration rollback check failed (non-blocking in CI)"
          else
            echo "âš ï¸ Migration rollback safety script not found - skipping"
          fi
      
      - name: "ðŸ“Š E2E results summary"
        id: e2e-results
        if: success()
        run: |
          echo "âœ… E2E tests completed successfully"
          echo "passed=true" >> $GITHUB_OUTPUT

      - name: "ðŸ“Š E2E results summary (failure)"
        id: e2e-results-failure
        if: failure()
        run: |
          echo "âŒ E2E tests failed"
          echo "passed=false" >> $GITHUB_OUTPUT
          exit 1
      
      - name: "ðŸ§¹ Cleanup isolated E2E environment"
        if: always()
        run: |
          echo "ðŸ§¹ Cleaning up isolated E2E environment..."
          docker compose -f docker-compose.e2e-test.yml down -v --remove-orphans || true
          
          # Critical: Ensure E2E volumes are completely removed
          echo "ðŸ§¹ Removing E2E volumes to prevent future migration state issues..."
          docker volume rm loyalty_postgres_e2e_data loyalty_redis_e2e_data 2>/dev/null || true
          docker volume rm loyalty-e2e-tests_loyalty_postgres_e2e_data loyalty-e2e-tests_loyalty_redis_e2e_data 2>/dev/null || true
          
          docker system prune -f --volumes || true
          rm -f docker-compose.e2e-test.yml || true
          echo "âœ… Isolated E2E environment cleaned up"

  # =============================================================================
  # PHASE 2: BUILD & DEPLOYMENT PREPARATION (2-3 minutes, only on main)
  # =============================================================================
  
  # Job 2A: Build validation (only for main branch, after all tests)
  build-validation:
    name: "ðŸ—ï¸ Build Validation & Docker Images"
    runs-on: self-hosted
    needs: [security-analysis, unit-integration-tests, e2e-tests]
    if: github.ref == 'refs/heads/main' && always() && needs.security-analysis.outputs.security-passed == 'true' && needs.unit-integration-tests.outputs.tests-passed == 'true'
    timeout-minutes: 15
    outputs:
      build-passed: ${{ steps.build-results.outputs.passed }}
      backend-image: ${{ steps.docker-build.outputs.backend_image }}
      frontend-image: ${{ steps.docker-build.outputs.frontend_image }}
      images-ready: ${{ steps.docker-build.outputs.images_ready }}
    
    steps:
      - name: "ðŸ§¹ Workspace preparation"
        run: |
          # Clean workspace with passwordless sudo (now configured)
          echo "Cleaning workspace at ${{ github.workspace }}"
          
          cd ${{ github.workspace }}
          
          # Force clean the problematic directories with sudo
          echo "Removing build artifacts with elevated permissions..."
          sudo rm -rf frontend/dist backend/dist 2>/dev/null || true
          sudo rm -rf frontend/build backend/build 2>/dev/null || true
          sudo rm -rf frontend/.next backend/.next 2>/dev/null || true
          sudo rm -rf node_modules frontend/node_modules backend/node_modules 2>/dev/null || true
          
          # Clean git repository state
          if [ -d ".git" ]; then
            echo "Cleaning git repository state..."
            sudo git clean -xffd 2>/dev/null || true
            git reset --hard 2>/dev/null || true
          fi
          
          # Fix ownership of remaining files to current user
          sudo chown -R $USER:$USER . 2>/dev/null || true
          
          echo "Workspace after cleanup:"
          ls -la
      
      - name: "ðŸ“¥ Checkout code"
        uses: actions/checkout@v4
        with:
          fetch-depth: 1
          clean: false  # Don't try to clean, we already did that
      
      - name: "âš¡ Setup Node.js (no remote cache)"
        uses: actions/setup-node@v4
        with:
          node-version: ${{ env.NODE_VERSION }}
          # Removed cache: 'npm' to use local cache instead
      
      - name: "ðŸš€ Setup local cache strategy"
        id: local-cache
        run: |
          echo "ðŸš€ Using local cache strategy for self-hosted runner"
          
          # Define local cache directories (using nut user home)
          LOCAL_CACHE_BASE="/home/nut/.cache/loyalty-app"
          NPM_CACHE="/home/nut/.npm-cache"
          
          # Create cache directories if they don't exist
          mkdir -p "$LOCAL_CACHE_BASE"/{frontend,backend} "$NPM_CACHE"
          
          # Calculate cache keys based on package files
          FRONTEND_KEY=$(shasum frontend/package-lock.json 2>/dev/null | cut -d' ' -f1 || echo "no-lock")
          BACKEND_KEY=$(shasum backend/package-lock.json 2>/dev/null | cut -d' ' -f1 || echo "no-lock")
          
          echo "frontend_cache_key=$FRONTEND_KEY" >> $GITHUB_OUTPUT
          echo "backend_cache_key=$BACKEND_KEY" >> $GITHUB_OUTPUT
          echo "cache_base=$LOCAL_CACHE_BASE" >> $GITHUB_OUTPUT
          echo "npm_cache=$NPM_CACHE" >> $GITHUB_OUTPUT
          
          # Check if local cache exists and is valid
          CACHE_HIT=false
          if [ -f "$LOCAL_CACHE_BASE/frontend/.cache-key" ] && [ -f "$LOCAL_CACHE_BASE/backend/.cache-key" ]; then
            STORED_FRONTEND_KEY=$(cat "$LOCAL_CACHE_BASE/frontend/.cache-key")
            STORED_BACKEND_KEY=$(cat "$LOCAL_CACHE_BASE/backend/.cache-key")
            
            if [ "$STORED_FRONTEND_KEY" = "$FRONTEND_KEY" ] && [ "$STORED_BACKEND_KEY" = "$BACKEND_KEY" ]; then
              CACHE_HIT=true
              echo "âœ… Local cache valid and ready to use"
            else
              echo "â™»ï¸ Package files changed, cache needs refresh"
            fi
          else
            echo "ðŸ“¦ No local cache found, will create one"
          fi
          
          echo "cache_hit=$CACHE_HIT" >> $GITHUB_OUTPUT
      
      - name: "ðŸ“¦ Install dependencies (parallel)"
        run: |
          echo "ðŸ“¦ Installing dependencies with local cache..."
          
          # Variables from cache setup
          LOCAL_CACHE_BASE="${{ steps.local-cache.outputs.cache_base }}"
          NPM_CACHE="${{ steps.local-cache.outputs.npm_cache }}"
          CACHE_HIT="${{ steps.local-cache.outputs.cache_hit }}"
          
          # Set npm cache directory
          npm config set cache "$NPM_CACHE"
          
          if [ "$CACHE_HIT" = "true" ]; then
            echo "âš¡ Restoring from local cache..."
            # Restore node_modules from local cache in parallel
            if [ -d "$LOCAL_CACHE_BASE/backend/node_modules" ] && [ -d "$LOCAL_CACHE_BASE/frontend/node_modules" ]; then
              cp -r "$LOCAL_CACHE_BASE/backend/node_modules" backend/ &
              cp -r "$LOCAL_CACHE_BASE/frontend/node_modules" frontend/ &
              wait
              echo "âœ… Dependencies restored from local cache"
            else
              echo "ðŸ“¦ Cache miss - installing normally"
              cd backend && npm ci --prefer-offline &
              cd frontend && npm ci --prefer-offline &
              wait
            fi
          else
            echo "ðŸ“¦ Installing and caching dependencies..."
            cd backend && npm ci --prefer-offline &
            cd frontend && npm ci --prefer-offline &
            wait
            
            # Cache the installed node_modules
            echo "ðŸ’¾ Caching dependencies..."
            cp -r backend/node_modules "$LOCAL_CACHE_BASE/backend/" &
            cp -r frontend/node_modules "$LOCAL_CACHE_BASE/frontend/" &
            wait
            echo "${{ steps.local-cache.outputs.backend_cache_key }}" > "$LOCAL_CACHE_BASE/backend/.cache-key"
            echo "${{ steps.local-cache.outputs.frontend_cache_key }}" > "$LOCAL_CACHE_BASE/frontend/.cache-key"
            echo "âœ… Dependencies cached locally"
          fi
      
      - name: "ðŸ”§ Generate Prisma client (build phase)"
        run: |
          echo "ðŸ”§ Generating Prisma client for TypeScript compilation..."
          cd backend && npm run db:generate
          echo "âœ… Prisma client generated successfully"
      
      - name: "ðŸ—ï¸ Build applications (parallel)"
        run: |
          # Backend build
          cd backend && npm run build:prod &
          BACKEND_PID=$!
          
          # Frontend build  
          cd frontend && npm run build &
          FRONTEND_PID=$!
          
          # Wait for both builds
          wait $BACKEND_PID
          BACKEND_EXIT=$?
          wait $FRONTEND_PID
          FRONTEND_EXIT=$?
          
          # Check results
          if [ $BACKEND_EXIT -ne 0 ] || [ $FRONTEND_EXIT -ne 0 ]; then
            echo "Build failed"
            exit 1
          fi
      
      - name: "âœ… Build validation (artifact verification)"
        run: |
          # Verify build outputs
          if [ ! -d "backend/dist" ] || [ ! -d "frontend/dist" ]; then
            echo "Build artifacts missing"
            exit 1
          fi
          
          # Check key files exist
          if [ ! -f "backend/dist/index.js" ]; then
            echo "Backend build incomplete"
            exit 1
          fi
          
          if [ ! -f "frontend/dist/index.html" ]; then
            echo "Frontend build incomplete"
            exit 1
          fi
      
      - name: "ðŸ³ Build & Tag Docker Images (Production-Ready)"
        id: docker-build
        env:
          # Provide dummy environment variables for build-time validation
          # Real secrets will be used in production-deployment job
          NODE_ENV: production
          LOG_LEVEL: info
          CORS_ORIGINS: "https://loyalty.saichon.com"
          JWT_SECRET: "dummy-build-validation-secret-min-32-chars"
          JWT_REFRESH_SECRET: "dummy-build-validation-refresh-secret-min-32-chars"
          SESSION_SECRET: "dummy-build-validation-session-secret-min-32-chars"
          DATABASE_URL: "postgresql://dummy:dummy@dummy:5432/dummy"
          REDIS_URL: "redis://redis:6379"
          FRONTEND_URL: "https://loyalty.saichon.com"
          BACKEND_URL: "https://loyalty.saichon.com/api"
          VITE_API_URL: "https://loyalty.saichon.com/api"
          GOOGLE_CLIENT_ID: "dummy-google-client-id"
          GOOGLE_CLIENT_SECRET: "dummy-google-client-secret"
          GOOGLE_CALLBACK_URL: "https://loyalty.saichon.com/api/oauth/google/callback"
          FACEBOOK_APP_ID: "dummy-facebook-app-id"
          FACEBOOK_APP_SECRET: "dummy-facebook-app-secret"
          FACEBOOK_CALLBACK_URL: "https://loyalty.saichon.com/api/oauth/facebook/callback"
          LINE_CHANNEL_ID: "dummy-line-channel-id"
          LINE_CHANNEL_SECRET: "dummy-line-channel-secret"
          LINE_CALLBACK_URL: "https://loyalty.saichon.com/api/oauth/line/callback"
          AZURE_TRANSLATION_TEXT_URI: "https://dummy.cognitiveservices.azure.com"
          AZURE_TRANSLATION_KEY_1: "dummy-azure-key-1"
          AZURE_TRANSLATION_KEY_2: "dummy-azure-key-2"
          AZURE_TRANSLATION_REGION: "southeastasia"
          LOYALTY_USERNAME: "dummy-loyalty-user"
          LOYALTY_PASSWORD: "dummy-loyalty-password"
        run: |
          echo "ðŸ³ Building production Docker images with BuildKit optimization..."

          # Enable BuildKit for faster builds
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          export BUILDKIT_INLINE_CACHE=1

          # Validate Docker Compose configuration
          echo "ðŸ” Validating Docker Compose configuration..."
          docker compose -f docker-compose.yml -f docker-compose.prod.yml config > /dev/null
          echo "âœ… Configuration validated"

          # Build images once with production target
          echo "ðŸ—ï¸ Building Docker images (production runner stage)..."
          docker compose -f docker-compose.yml -f docker-compose.prod.yml build \
            --build-arg BUILDKIT_INLINE_CACHE=1 \
            --parallel

          # Tag images with commit SHA for deployment tracking
          echo "ðŸ·ï¸ Tagging images with commit SHA..."
          COMMIT_SHA="${{ github.sha }}"
          SHORT_SHA="${COMMIT_SHA:0:7}"

          # Get image names from docker compose
          BACKEND_IMAGE=$(docker compose -f docker-compose.yml -f docker-compose.prod.yml images -q backend | head -1)
          FRONTEND_IMAGE=$(docker compose -f docker-compose.yml -f docker-compose.prod.yml images -q frontend | head -1)

          # If images were built, tag them
          if [ -n "$BACKEND_IMAGE" ]; then
            docker tag $BACKEND_IMAGE loyalty-app-backend:$COMMIT_SHA
            docker tag $BACKEND_IMAGE loyalty-app-backend:$SHORT_SHA
            docker tag $BACKEND_IMAGE loyalty-app-backend:latest
            echo "âœ… Backend image tagged: $COMMIT_SHA, $SHORT_SHA, latest"
          fi

          if [ -n "$FRONTEND_IMAGE" ]; then
            docker tag $FRONTEND_IMAGE loyalty-app-frontend:$COMMIT_SHA
            docker tag $FRONTEND_IMAGE loyalty-app-frontend:$SHORT_SHA
            docker tag $FRONTEND_IMAGE loyalty-app-frontend:latest
            echo "âœ… Frontend image tagged: $COMMIT_SHA, $SHORT_SHA, latest"
          fi

          # Verify images exist
          echo "ðŸ” Verifying tagged images..."
          docker images | grep -E "loyalty-app-(backend|frontend)" | grep -E "$SHORT_SHA|latest"

          # Quick smoke test
          echo "ðŸ§ª Running smoke tests..."
          if docker run --rm loyalty-app-backend:$SHORT_SHA node --version >/dev/null 2>&1; then
            echo "âœ… Backend image smoke test passed"
          else
            echo "âš ï¸ Backend smoke test failed (non-critical)"
          fi

          # Output image tags for deployment job
          echo "images_ready=true" >> $GITHUB_OUTPUT
          echo "backend_image=loyalty-app-backend:$COMMIT_SHA" >> $GITHUB_OUTPUT
          echo "frontend_image=loyalty-app-frontend:$COMMIT_SHA" >> $GITHUB_OUTPUT

          echo "âœ… Docker images built, tagged, and ready for deployment"
          echo "ðŸ“¦ Backend: loyalty-app-backend:$COMMIT_SHA"
          echo "ðŸ“¦ Frontend: loyalty-app-frontend:$COMMIT_SHA"
      
      - name: "ðŸ“Š Build results (output summary)"
        id: build-results
        run: |
          echo "Build validation completed successfully"
          echo "passed=true" >> $GITHUB_OUTPUT

  # =============================================================================
  # PHASE 3: PRODUCTION DEPLOYMENT (only on main, after all tests pass)
  # Pipeline Order: Pre-validation â†’ Backup â†’ Code Deploy â†’ Dependencies â†’ 
  # Environment â†’ Service Deploy â†’ Migration â†’ Health Check â†’ Summary
  # =============================================================================
  
  production-deployment:
    name: "ðŸš€ Production Deployment"
    runs-on: self-hosted
    environment: production
    needs: [security-analysis, unit-integration-tests, build-validation]
    if: github.ref == 'refs/heads/main' && needs.security-analysis.outputs.security-passed == 'true' && needs.unit-integration-tests.outputs.tests-passed == 'true' && needs.build-validation.outputs.build-passed == 'true'
    timeout-minutes: 15
    
    steps:
      - name: "ðŸ“Š Pre-deployment validation (quality gates)"
        run: |
          echo "ðŸ” Pre-deployment checks..."
          echo "Security Analysis: ${{ needs.security-analysis.outputs.security-passed }}"
          echo "Unit/Integration Tests: ${{ needs.unit-integration-tests.outputs.tests-passed }}"
          echo "Build Validation: ${{ needs.build-validation.outputs.build-passed }}"
          echo "Test Coverage: ${{ needs.unit-integration-tests.outputs.coverage-percent }}%"
          
          # Verify all requirements are met
          if [ "${{ needs.security-analysis.outputs.security-passed }}" != "true" ]; then
            echo "âŒ Security validation failed"
            exit 1
          fi
          
          if [ "${{ needs.unit-integration-tests.outputs.tests-passed }}" != "true" ]; then
            echo "âŒ Tests failed"
            exit 1
          fi
          
          if [ "${{ needs.build-validation.outputs.build-passed }}" != "true" ]; then
            echo "âŒ Build validation failed"  
            exit 1
          fi
          
          echo "âœ… All pre-deployment checks passed"
      
      - name: "ðŸ’¾ Smart database backup (pre-shutdown)"
        id: backup
        run: |
          echo "ðŸ’¾ Creating smart database backup..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          
          # Only backup if deployment directory exists
          if [ ! -d "$DEPLOY_DIR" ]; then
            echo "âš ï¸ First deployment - skipping backup"
            echo "status=skipped" >> $GITHUB_OUTPUT
            exit 0
          fi
          
          cd "$DEPLOY_DIR"
          mkdir -p backups
          
          # Quick backup with timeout
          timestamp=$(date +%Y%m%d_%H%M%S)
          backup_file="backups/pre_deploy_${timestamp}.sql"
          
          if timeout 60s docker exec loyalty_postgres pg_dump -U loyalty -d loyalty_db > "$backup_file" 2>/dev/null; then
            if [ -s "$backup_file" ]; then
              echo "âœ… Backup created: $backup_file ($(du -h "$backup_file" | cut -f1))"
              echo "status=success" >> $GITHUB_OUTPUT
            else
              echo "âš ï¸ Backup failed - empty file"
              rm -f "$backup_file"
              echo "status=failed" >> $GITHUB_OUTPUT
            fi
          else
            echo "âš ï¸ Backup timed out or failed"
            rm -f "$backup_file" 2>/dev/null || true
            echo "status=failed" >> $GITHUB_OUTPUT
          fi
      
      - name: "ðŸ“¥ Optimized code deployment"
        run: |
          echo "ðŸ“¥ Deploying code with optimizations..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          
          # Smart deployment: update existing repo or clone new
          if [ -d "$DEPLOY_DIR/.git" ]; then
            echo "ðŸ“¥ Updating existing repository..."
            cd "$DEPLOY_DIR"
            git clean -fd 2>/dev/null || true
            git reset --hard HEAD 2>/dev/null || true
            git remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git
            git fetch --depth 1 origin ${{ github.sha }}
            git checkout ${{ github.sha }}
          else
            echo "ðŸ“¦ Fresh deployment..."
            mkdir -p "$(dirname "$DEPLOY_DIR")"
            rm -rf "$DEPLOY_DIR" 2>/dev/null || true
            git clone --depth 1 --single-branch \
              https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git \
              "$DEPLOY_DIR"
            cd "$DEPLOY_DIR"
            git checkout ${{ github.sha }}
          fi
          
          echo "âœ… Code deployment completed"
      
      - name: "âš¡ Lightning-fast dependency setup"
        run: |
          echo "âš¡ Setting up dependencies with maximum speed..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Configure npm for maximum performance
          export NPM_CONFIG_CACHE=/home/nut/.npm-cache
          export NPM_CONFIG_PREFER_OFFLINE=true
          export NPM_CONFIG_AUDIT=false
          export NPM_CONFIG_FUND=false
          mkdir -p /home/nut/.npm-cache
          
          # Parallel dependency installation with smart caching
          install_deps() {
            local service=$1
            local dir=$2
            echo "Installing $service dependencies..."
            cd "$DEPLOY_DIR/$dir"
            
            # Use npm ci for reproducible installs
            if npm ci --prefer-offline --no-audit --ignore-scripts 2>/dev/null; then
              echo "âœ… $service dependencies installed"
            else
              echo "ðŸ”„ Fallback install for $service..."
              npm install --prefer-offline --no-audit
            fi
          }
          
          # Install in parallel
          install_deps "backend" "backend" &
          install_deps "frontend" "frontend" &
          wait
          
          echo "âœ… Dependencies installed in parallel"
      
      - name: "ðŸ”§ Environment configuration"
        run: |
          echo "ðŸ”§ Configuring production environment..."
          
          # Validate critical secrets
          missing_secrets=()
          if [ -z "${{ secrets.JWT_SECRET }}" ]; then missing_secrets+=("JWT_SECRET"); fi
          if [ -z "${{ secrets.DATABASE_URL }}" ]; then missing_secrets+=("DATABASE_URL"); fi
          
          if [ ${#missing_secrets[@]} -ne 0 ]; then
            echo "âŒ Missing secrets: ${missing_secrets[*]}"
            exit 1
          fi
          
          # Export environment variables
          echo "NODE_ENV=production" >> $GITHUB_ENV
          echo "LOG_LEVEL=info" >> $GITHUB_ENV
          echo "CORS_ORIGINS=${{ secrets.CORS_ORIGINS }}" >> $GITHUB_ENV
          
          # Core secrets
          echo "JWT_SECRET=${{ secrets.JWT_SECRET }}" >> $GITHUB_ENV
          echo "JWT_REFRESH_SECRET=${{ secrets.JWT_REFRESH_SECRET }}" >> $GITHUB_ENV
          echo "SESSION_SECRET=${{ secrets.SESSION_SECRET }}" >> $GITHUB_ENV
          echo "DATABASE_URL=${{ secrets.DATABASE_URL }}" >> $GITHUB_ENV
          echo "REDIS_URL=${{ secrets.REDIS_URL || 'redis://redis:6379' }}" >> $GITHUB_ENV
          echo "FRONTEND_URL=${{ secrets.FRONTEND_URL }}" >> $GITHUB_ENV
          echo "BACKEND_URL=${{ secrets.BACKEND_URL }}" >> $GITHUB_ENV
          echo "VITE_API_URL=${{ secrets.VITE_API_URL }}" >> $GITHUB_ENV
          
          # OAuth Configuration
          echo "GOOGLE_CLIENT_ID=${{ secrets.GOOGLE_CLIENT_ID }}" >> $GITHUB_ENV
          echo "GOOGLE_CLIENT_SECRET=${{ secrets.GOOGLE_CLIENT_SECRET }}" >> $GITHUB_ENV
          echo "GOOGLE_CALLBACK_URL=${{ secrets.GOOGLE_CALLBACK_URL || vars.GOOGLE_CALLBACK_URL }}" >> $GITHUB_ENV
          echo "FACEBOOK_APP_ID=${{ secrets.FACEBOOK_APP_ID }}" >> $GITHUB_ENV
          echo "FACEBOOK_APP_SECRET=${{ secrets.FACEBOOK_APP_SECRET }}" >> $GITHUB_ENV
          echo "FACEBOOK_CALLBACK_URL=${{ secrets.FACEBOOK_CALLBACK_URL }}" >> $GITHUB_ENV
          echo "LINE_CHANNEL_ID=${{ secrets.LINE_CHANNEL_ID }}" >> $GITHUB_ENV
          echo "LINE_CHANNEL_SECRET=${{ secrets.LINE_CHANNEL_SECRET }}" >> $GITHUB_ENV
          echo "LINE_CALLBACK_URL=${{ secrets.LINE_CALLBACK_URL || vars.LINE_CALLBACK_URL }}" >> $GITHUB_ENV
          
          # Azure Translation Service
          echo "AZURE_TRANSLATION_TEXT_URI=${{ secrets.AZURE_TRANSLATION_TEXT_URI }}" >> $GITHUB_ENV
          echo "AZURE_TRANSLATION_KEY_1=${{ secrets.AZURE_TRANSLATION_KEY_1 }}" >> $GITHUB_ENV
          echo "AZURE_TRANSLATION_KEY_2=${{ secrets.AZURE_TRANSLATION_KEY_2 }}" >> $GITHUB_ENV
          echo "AZURE_TRANSLATION_REGION=${{ secrets.AZURE_TRANSLATION_REGION }}" >> $GITHUB_ENV
          
          # Loyalty System Configuration
          echo "LOYALTY_USERNAME=${{ secrets.LOYALTY_USERNAME }}" >> $GITHUB_ENV
          echo "LOYALTY_PASSWORD=${{ secrets.LOYALTY_PASSWORD }}" >> $GITHUB_ENV
          
          echo "âœ… Environment configured"
          
          # Debug environment variable lengths for validation
          echo "ðŸ” Environment Variable Debug:"
          echo "JWT_SECRET length: ${#JWT_SECRET}"
          echo "JWT_REFRESH_SECRET length: ${#JWT_REFRESH_SECRET}"
          echo "SESSION_SECRET length: ${#SESSION_SECRET}"
          echo "REDIS_URL configured: $([ -n "$REDIS_URL" ] && echo 'yes' || echo 'no')"
          echo "DATABASE_URL configured: $([ -n "$DATABASE_URL" ] && echo 'yes' || echo 'no')"
      
      - name: "ðŸ“ Create .env file (container configuration)"
        run: |
          echo "ðŸ“ Creating .env file for Docker Compose..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Create .env file with all environment variables
          cat > .env << EOF
          # Node environment
          NODE_ENV=${NODE_ENV}
          LOG_LEVEL=${LOG_LEVEL}
          CORS_ORIGINS=${CORS_ORIGINS}
          
          # Core secrets
          JWT_SECRET=${JWT_SECRET}
          JWT_REFRESH_SECRET=${JWT_REFRESH_SECRET}
          SESSION_SECRET=${SESSION_SECRET}
          DATABASE_URL=${DATABASE_URL}
          REDIS_URL=${REDIS_URL:-redis://redis:6379}
          FRONTEND_URL=${FRONTEND_URL}
          BACKEND_URL=${BACKEND_URL}
          VITE_API_URL=${VITE_API_URL}
          
          # OAuth Configuration
          GOOGLE_CLIENT_ID=${GOOGLE_CLIENT_ID}
          GOOGLE_CLIENT_SECRET=${GOOGLE_CLIENT_SECRET}
          GOOGLE_CALLBACK_URL=${GOOGLE_CALLBACK_URL}
          FACEBOOK_APP_ID=${FACEBOOK_APP_ID}
          FACEBOOK_APP_SECRET=${FACEBOOK_APP_SECRET}
          FACEBOOK_CALLBACK_URL=${FACEBOOK_CALLBACK_URL}
          LINE_CHANNEL_ID=${LINE_CHANNEL_ID}
          LINE_CHANNEL_SECRET=${LINE_CHANNEL_SECRET}
          LINE_CALLBACK_URL=${LINE_CALLBACK_URL}
          
          # Azure Translation Service
          AZURE_TRANSLATION_TEXT_URI=${AZURE_TRANSLATION_TEXT_URI}
          AZURE_TRANSLATION_KEY_1=${AZURE_TRANSLATION_KEY_1}
          AZURE_TRANSLATION_KEY_2=${AZURE_TRANSLATION_KEY_2}
          AZURE_TRANSLATION_REGION=${AZURE_TRANSLATION_REGION}
          
          # Loyalty System Configuration
          LOYALTY_USERNAME=${LOYALTY_USERNAME}
          LOYALTY_PASSWORD=${LOYALTY_PASSWORD}
          EOF
          
          echo "âœ… Environment file created successfully"
          echo "ðŸ“Š Environment variables configured: $(wc -l < .env) lines"
          
          # Debug .env file contents (without exposing secrets)
          echo "ðŸ” .env file validation:"
          echo "NODE_ENV: $(grep '^NODE_ENV=' .env | cut -d'=' -f2)"
          echo "REDIS_URL configured: $(grep -q '^REDIS_URL=' .env && echo 'yes' || echo 'no')"
          echo "DATABASE_URL configured: $(grep -q '^DATABASE_URL=' .env && echo 'yes' || echo 'no')"
          echo "JWT_SECRET configured: $(grep -q '^JWT_SECRET=' .env && echo 'yes' || echo 'no')"
          
          # Check for empty values
          empty_vars=$(grep '=$' .env | cut -d'=' -f1 | head -5)
          if [ -n "$empty_vars" ]; then
            echo "âš ï¸ Empty environment variables found: $empty_vars"
          fi
      
      - name: "ðŸš€ Optimized service deployment"
        run: |
          echo "ðŸš€ Deploying services with zero-downtime strategy..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Enable BuildKit for faster builds
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          
          # Graceful shutdown of existing services
          if docker compose ps -q | head -1 | grep -q .; then
            echo "ðŸ›‘ Graceful shutdown of existing services..."
            timeout 30s docker compose -f docker-compose.yml -f docker-compose.prod.yml down --timeout 10 || \
            docker compose -f docker-compose.yml -f docker-compose.prod.yml down --timeout 5
          fi
          
          # Build and start services
          echo "ðŸ—ï¸ Building and starting services..."
          docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d --build
          
          echo "âœ… Services deployed"
          
          # Give services time to initialize before health checks
          echo "â³ Allowing services to initialize (30 seconds)..."
          sleep 30
          
          # Quick startup verification
          echo "ðŸ” Quick startup verification:"
          docker compose ps
          echo "ðŸ“ Backend startup logs (last 10 lines):"
          docker compose logs --tail=10 backend 2>/dev/null || echo "No backend logs yet"
      
      - name: "ðŸ—ƒï¸ Database migration (container context)"
        run: |
          echo "ðŸ—ƒï¸ Running database migrations with enhanced logging..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Pre-migration container status
          echo "ðŸ“¦ Pre-migration container status:"
          docker compose ps
          
          # Wait for database to be ready with enhanced logging
          echo "ðŸ’¾ Waiting for database to be ready..."
          if timeout 60s bash -c 'until docker compose exec -T postgres pg_isready -U loyalty -d loyalty_db; do echo "Database not ready, waiting..."; sleep 2; done'; then
            echo "âœ… Database is ready"
          else
            echo "âŒ Database readiness timeout"
            echo "ðŸ“„ Database logs:"
            docker compose logs --tail=20 postgres
            exit 1
          fi
          
          # Wait for backend container to be fully ready
          echo "ðŸ“¦ Waiting for backend container to be ready..."
          if timeout 60s bash -c 'until docker compose exec -T backend echo "ready" >/dev/null 2>&1; do echo "Backend not ready, waiting..."; sleep 2; done'; then
            echo "âœ… Backend container is ready"
          else
            echo "âŒ Backend container timeout"
            echo "ðŸ“„ Backend logs:"
            docker compose logs --tail=20 backend
            exit 1
          fi
          
          # Give containers additional time to stabilize
          echo "â³ Allowing containers to stabilize (10 seconds)..."
          sleep 10
          
          # Show database connection info before migration
          echo "ðŸ”— Database connection check:"
          docker compose exec -T backend node -e "console.log('DATABASE_URL:', process.env.DATABASE_URL ? 'configured' : 'missing')" || echo "Cannot check DB config"
          
          # Run migration from within backend container (correct network context)
          echo "ðŸ—ƒï¸ Running database migration from container..."
          for attempt in 1 2 3; do
            echo "\nðŸ”„ Migration attempt $attempt/3..."
            
            # First check migration status
            echo "ðŸ” Checking current migration status:"
            docker compose exec -T backend npm run db:migrate:status || echo "Cannot check migration status"
            
            if docker compose exec -T backend npm run db:migrate:deploy; then
              echo "âœ… Database migrations completed successfully"
              
              # Verify migration success
              echo "ðŸ” Post-migration verification:"
              docker compose exec -T backend npm run db:migrate:status || echo "Cannot verify final status"
              break
            else
              echo "âŒ Migration attempt $attempt failed"
              
              # Show backend logs for debugging
              echo "ðŸ“„ Backend logs during migration failure:"
              docker compose logs --tail=10 backend
              
              if [ $attempt -eq 3 ]; then
                echo "âŒ Migration failed after 3 attempts"
                
                # Enhanced baseline scenario handling
                echo "ðŸ” Checking for baseline scenario..."
                migration_status=$(docker compose exec -T backend npm run db:migrate:status 2>&1 || echo "status_check_failed")
                echo "Migration status output: $migration_status"
                
                if echo "$migration_status" | grep -q "migrations have not yet been applied"; then
                  echo "ðŸ”§ Attempting baseline resolution..."
                  docker compose exec -T backend npx prisma migrate resolve --applied 0_init 2>/dev/null || echo "Baseline resolution failed"
                  
                  echo "ðŸ”„ Retrying migration after baseline..."
                  if docker compose exec -T backend npm run db:migrate:deploy; then
                    echo "âœ… Database migrations completed after baseline resolution"
                    break
                  else
                    echo "âŒ Migration still failed after baseline"
                  fi
                fi
                
                # Final failure diagnostics
                echo "ðŸ”´ MIGRATION FAILURE DIAGNOSTICS:"
                echo "=== Container Status ==="
                docker compose ps
                echo "\n=== Database Connection Test ==="
                docker compose exec -T postgres pg_isready -U loyalty -d loyalty_db || echo "Database connection failed"
                echo "\n=== Backend Environment ==="
                docker compose exec -T backend env | grep -E "DATABASE_URL|NODE_ENV" || echo "Cannot check environment"
                echo "\n=== Backend Logs (last 30 lines) ==="
                docker compose logs --tail=30 backend
                
                echo "âŒ All migration attempts failed"
                exit 1
              else
                echo "â³ Retrying in 5 seconds..."
                sleep 5
              fi
            fi
          done
          
          echo "âœ… Database migration phase completed"
      
      - name: "ðŸ” Post-deployment OAuth validation"
        run: |
          echo "ðŸ” Validating OAuth configuration in production environment..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Wait for application to be ready
          echo "â³ Waiting for application to be ready..."
          sleep 10
          
          # Run OAuth health validation in container context
          echo "ðŸ” Running OAuth validation..."
          if docker compose exec -T backend test -f "/app/scripts/validate-oauth-health.sh"; then
            # Run OAuth validation from within container (production network context)
            if docker compose exec -T backend bash -c "cd /app && ./scripts/validate-oauth-health.sh"; then
              echo "âœ… OAuth validation passed in production"
            else
              echo "âš ï¸ OAuth validation failed in production - check configuration"
              echo "ðŸ“„ Recent backend logs:"
              docker compose logs --tail=20 backend
            fi
          else
            echo "âš ï¸ OAuth validation script not found in container - skipping"
          fi
      
      - name: "ðŸ—„ï¸ Post-deployment database validation"
        run: |
          echo "ðŸ—„ï¸ Validating database migration status in production..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Verify database migration status
          echo "ðŸ” Checking final migration status..."
          if docker compose exec -T backend npm run db:migrate:status; then
            echo "âœ… Database migration status verified"
          else
            echo "âš ï¸ Cannot verify database migration status"
          fi
          
          # Run database migration rollback safety check
          if docker compose exec -T backend test -f "/app/scripts/migration-rollback-safety.sh"; then
            echo "ðŸ” Running rollback safety check..."
            if docker compose exec -T backend bash -c "cd /app && ./scripts/migration-rollback-safety.sh check"; then
              echo "âœ… Migration rollback safety verified"
            else
              echo "âš ï¸ Migration rollback safety check failed"
            fi
          else
            echo "âš ï¸ Migration rollback safety script not found - skipping"
          fi
      
      - name: "ðŸ¥ Health checks & validation (service readiness)"
        run: |
          echo "ðŸ¥ Running comprehensive health checks with enhanced logging..."
          
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Enhanced logging function with container inspection
          enhanced_health_check() {
            local service=$1
            local url=$2
            local max_attempts=15
            local attempt=0
            
            echo "ðŸ” Starting health check for $service at $url"
            
            # First, check container status
            echo "ðŸ“¦ Checking container status..."
            docker compose ps
            
            # Show container logs for startup diagnostics
            echo "ðŸ“œ Recent startup logs from containers:"
            echo "=== Backend Logs (last 50 lines) ==="
            docker compose logs --tail=50 backend 2>/dev/null || echo "No backend logs available"
            
            echo "\n=== Frontend Logs (last 20 lines) ==="
            docker compose logs --tail=20 frontend 2>/dev/null || echo "No frontend logs available"
            
            echo "\n=== Database Logs (last 10 lines) ==="
            docker compose logs --tail=10 postgres 2>/dev/null || echo "No database logs available"
            
            # Check if containers are actually running
            if ! docker compose ps | grep -q "Up"; then
              echo "âŒ Critical: No containers are running!"
              echo "ðŸ”´ Container status details:"
              docker compose ps -a
              return 1
            fi
            
            # Enhanced health checking with detailed diagnostics
            while [ $attempt -lt $max_attempts ]; do
              echo "ðŸ”„ Health Check Attempt $((attempt + 1))/$max_attempts for $service"
              
              # Test with verbose curl output for diagnostics
              if curl -f -s --max-time 3 "$url" > /dev/null 2>&1; then
                echo "âœ… $service is healthy and responding"
                
                # Additional validation - get response details
                response=$(curl -s --max-time 3 "$url" 2>/dev/null || echo "No response body")
                echo "ðŸ“„ Response preview: ${response:0:200}..."
                return 0
              else
                # Detailed failure diagnostics
                http_code=$(curl -s -o /dev/null -w "%{http_code}" --max-time 3 "$url" 2>/dev/null || echo "000")
                echo "âš ï¸ $service not ready (HTTP: $http_code)"
                
                # Every 3rd attempt, show detailed diagnostics
                if [ $((attempt % 3)) -eq 0 ] && [ $attempt -gt 0 ]; then
                  echo "ðŸ” Diagnostic info (attempt $((attempt + 1))):"
                  echo "  - Port check:" $(nc -z localhost 4001 && echo "Port 4001 open" || echo "Port 4001 closed")
                  echo "  - Backend container:" $(docker compose exec -T backend echo "responsive" 2>/dev/null || echo "not responding")
                  echo "  - Recent backend errors:"
                  docker compose logs --tail=5 backend 2>/dev/null | grep -i "error\|fail\|exception" | tail -3 || echo "    No recent errors found"
                fi
              fi
              
              attempt=$((attempt + 1))
              [ $attempt -lt $max_attempts ] && sleep 2
            done
            
            echo "âŒ $service health check failed after $max_attempts attempts"
            
            # Final diagnostic dump on failure
            echo "ðŸ”´ FAILURE DIAGNOSTICS:"
            echo "=== Container Status ==="
            docker compose ps -a
            
            echo "\n=== Network Status ==="
            docker compose exec -T backend cat /etc/hosts 2>/dev/null | head -5 || echo "Cannot access container network info"
            
            echo "\n=== Application Ports ==="
            docker compose exec -T backend netstat -tlnp 2>/dev/null | grep -E ":3000|:4001" || echo "Cannot check application ports"
            
            echo "\n=== Final Backend Logs ==="
            docker compose logs --tail=30 backend
            
            echo "\n=== System Resources ==="
            echo "Memory:" $(free -h | head -2 | tail -1)
            echo "Disk:" $(df -h / | tail -1)
            
            return 1
          }
          
          # Run enhanced health checks
          enhanced_health_check "Application" "http://localhost:4001/api/health"
          
          # If backend is healthy, check frontend
          echo "\nðŸŒ Checking frontend..."
          if curl -f -s --max-time 3 "http://localhost:4001/" > /dev/null 2>&1; then
            echo "âœ… Frontend is responding"
          else
            echo "âš ï¸ Frontend not responding (non-critical for deployment)"
            echo "ðŸ“„ Frontend logs:"
            docker compose logs --tail=10 frontend
          fi
          
          echo "âœ… Health checks completed successfully"

  # =============================================================================
  # PHASE 4: POST-DEPLOYMENT MONITORING & CLEANUP
  # =============================================================================
  
  post-deployment:
    name: "ðŸ“Š Post-Deployment"
    runs-on: self-hosted
    needs: [production-deployment]
    if: always() && (needs.production-deployment.result == 'success' || needs.production-deployment.result == 'failure')
    timeout-minutes: 3
    continue-on-error: true  # Don't block pipeline completion on post-deployment issues
    
    steps:
      - name: "ðŸ” Pre-deployment health check"
        timeout-minutes: 1
        run: |
          echo "ðŸ” Checking runner status..."
          echo "Runner: $(hostname)"
          echo "Load: $(uptime)"
          echo "Memory: $(free -h 2>/dev/null || vm_stat | head -10)"
          echo "Disk: $(df -h / | tail -1)"
          echo "âœ… Runner health check passed"

      - name: "âš ï¸ Automatic Rollback on Failure"
        if: failure()
        timeout-minutes: 3
        run: |
          echo "âš ï¸ Deployment failed - initiating automatic rollback..."

          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"

          # Find previous working images (excluding current SHA)
          CURRENT_SHA="${{ github.sha }}"
          echo "ðŸ” Looking for previous working images (excluding $CURRENT_SHA)..."

          PREV_BACKEND=$(docker images loyalty-app-backend --format "{{.Tag}}" | grep -v "latest" | grep -v "$CURRENT_SHA" | head -1)
          PREV_FRONTEND=$(docker images loyalty-app-frontend --format "{{.Tag}}" | grep -v "latest" | grep -v "$CURRENT_SHA" | head -1)

          if [ -n "$PREV_BACKEND" ] && [ -n "$PREV_FRONTEND" ]; then
            echo "ðŸ“¦ Rolling back to previous images:"
            echo "  Backend: loyalty-app-backend:$PREV_BACKEND"
            echo "  Frontend: loyalty-app-frontend:$PREV_FRONTEND"

            # Export previous image tags
            export BACKEND_IMAGE="loyalty-app-backend:$PREV_BACKEND"
            export FRONTEND_IMAGE="loyalty-app-frontend:$PREV_FRONTEND"

            # Perform rollback with same hot-swap strategy
            echo "ðŸ”„ Rolling back backend..."
            docker compose -f docker-compose.yml -f docker-compose.prod.yml \
              up -d --no-build --no-deps backend

            sleep 10

            echo "ðŸ”„ Rolling back frontend..."
            docker compose -f docker-compose.yml -f docker-compose.prod.yml \
              up -d --no-build --no-deps frontend

            sleep 10

            # Verify rollback
            echo "ðŸ¥ Verifying rollback health..."
            if timeout 30s bash -c 'until curl -f -s http://localhost:4001/api/health >/dev/null 2>&1; do sleep 2; done'; then
              echo "âœ… Rollback successful - service health check passed"
              echo "ðŸ“Š Rollback summary:"
              echo "  Rolled back from: $CURRENT_SHA"
              echo "  Rolled back to: $PREV_BACKEND"
              docker compose ps
            else
              echo "âŒ Rollback health check failed"
              echo "ðŸ“„ Backend logs after rollback:"
              docker compose logs --tail=20 backend
            fi
          else
            echo "âš ï¸ No previous images found for rollback"
            echo "Available backend images:"
            docker images loyalty-app-backend
            echo "Available frontend images:"
            docker images loyalty-app-frontend
          fi

          echo "ðŸ”´ Deployment failed and rollback attempted"
          echo "Please review the logs above for details"

      - name: "ðŸ“Š Deployment summary (reporting only)"
        timeout-minutes: 1
        run: |
          echo "==============================================="
          echo "ðŸ“Š DEPLOYMENT SUMMARY"
          echo "==============================================="
          echo "Repository: ${{ github.repository }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo "Actor: ${{ github.actor }}"
          echo "Timestamp: $(date -Iseconds)"
          echo ""
          echo "ðŸŽ¯ Job Results:"
          echo "Security Analysis: ${{ needs.security-analysis.result }}"
          echo "Unit/Integration Tests: ${{ needs.unit-integration-tests.result }}"
          echo "E2E Tests: ${{ needs.e2e-tests.result || 'skipped' }}"
          echo "Build Validation: ${{ needs.build-validation.result }}"
          echo "Production Deployment: ${{ needs.production-deployment.result }}"
          echo ""
          echo "ðŸ“ˆ Metrics:"
          echo "Test Coverage: ${{ needs.unit-integration-tests.outputs.coverage-percent }}%"
          echo ""
          echo "ðŸŒ Service Endpoints:"
          echo "Application: https://loyalty.saichon.com"
          echo "Health Check: https://loyalty.saichon.com/api/health"
          echo "==============================================="
      
      - name: "ðŸŒ Service health validation"
        timeout-minutes: 1
        run: |
          echo "ðŸŒ Testing deployed services..."
          
          # Test health endpoint with timeout and retry
          for i in {1..3}; do
            if curl -s --max-time 10 "https://loyalty.saichon.com/api/health" | grep -q "ok\|healthy\|success" 2>/dev/null; then
              echo "âœ… Service health check passed (attempt $i)"
              break
            else
              echo "âš ï¸ Health check failed (attempt $i/3)"
              [ $i -eq 3 ] && echo "ðŸš¨ All health checks failed - service may be unhealthy"
              sleep 5
            fi
          done
      
      - name: "ðŸ§¹ Smart cleanup (non-blocking)"
        timeout-minutes: 1
        continue-on-error: true
        run: |
          echo "ðŸ§¹ Running smart cleanup with strict timeouts..."
          
          # Set strict timeout for all operations
          set +e  # Don't exit on command failures in cleanup
          
          # Check disk usage with timeout
          DISK_USAGE=$(timeout 10s df / | tail -1 | awk '{print $5}' | sed 's/%//' 2>/dev/null || echo "unknown")
          echo "ðŸ’¾ Disk usage: ${DISK_USAGE}%"
          
          # Clean up old containers with timeout
          echo "ðŸ³ Cleaning old containers..."
          timeout 20s docker container prune -f 2>/dev/null || echo "âš ï¸ Container cleanup timed out"
          
          # Clean up old images with timeout (keep last 3 versions)
          echo "ðŸ–¼ï¸ Cleaning old images..."
          timeout 20s docker image prune -f --filter "until=72h" 2>/dev/null || echo "âš ï¸ Image cleanup timed out"
          
          # Clean up build cache with timeout
          echo "ðŸ§¹ Cleaning build cache..."
          timeout 10s docker builder prune -f --filter "until=24h" 2>/dev/null || echo "âš ï¸ Build cache cleanup timed out"
          
          echo "âœ… Cleanup completed (may have warnings, non-blocking)"

      - name: "ðŸ“‹ Post-deployment summary"
        timeout-minutes: 1
        continue-on-error: true
        run: |
          echo "ðŸŽ‰ Deployment Summary:"
          echo "â”œâ”€â”€ âœ… Application deployed successfully"
          echo "â”œâ”€â”€ ðŸŒ Frontend: https://loyalty.saichon.com"
          echo "â”œâ”€â”€ ðŸ”— Backend: https://loyalty.saichon.com/api"
          echo "â”œâ”€â”€ ðŸ“Š Health check: $(curl -s -o /dev/null -w '%{http_code}' https://loyalty.saichon.com/api/health 2>/dev/null || echo 'timeout')"
          echo "â””â”€â”€ ðŸ•’ Completed at: $(date)"
          
          # Final completion message
          echo "ðŸŽŠ Post-deployment job completed successfully - pipeline will not get stuck"

# =============================================================================
# PIPELINE SUMMARY:
# 
# ðŸ“Š PERFORMANCE OPTIMIZATIONS:
# - Sequential job execution with isolated environments (prevents conflicts)
# - Port isolation strategy (Unit: 5435/6380, E2E: 5436/6381/4002/3001)
# - Container isolation with unique project names and containers
# - Intelligent caching (npm, Docker BuildKit)
# - Conditional jobs (E2E only on main/PR to main)
# - Smart dependency installation
# - Shallow git clones
# - Optimized Docker builds
# 
# ðŸ”’ SECURITY & QUALITY:
# - ESLint security rules
# - npm audit
# - Custom security validation
# - Test integrity validation (prevents test bypassing)
# - TypeScript type checking
# - Unit tests
# - Integration tests
# - E2E tests (conditional)
# - Database schema tests
# - OAuth health validation (pre & post deployment)
# - Database migration validation & rollback safety
# - OAuth E2E validation tests
# 
# âš¡ ESTIMATED TIMES:
# - Phase 1A (Security): 3-4 minutes (parallel with Unit Tests)
# - Phase 1B (Unit/Integration): 5-8 minutes (isolated environment)
# - Phase 1C (E2E Tests): 8-12 minutes (after unit tests, isolated environment)
# - Phase 2 (Build): 2-3 minutes (only on main, after all tests)
# - Phase 3 (Deploy): 3-5 minutes (only on main, production)
# - Total: 13-18 minutes (sequential for reliability)
# 
# ðŸŽ¯ IMPROVEMENTS:
# - 100% environment isolation (prevents race conditions)
# - Comprehensive test coverage with parallel security analysis
# - Enhanced security validation
# - Better error handling and reporting
# - Smart caching for dependencies
# - Conditional execution to save resources
# - Reliable testing environments with port separation
# =============================================================================