name: Deploy to Production

on:
  push:
    branches: [main]
  workflow_dispatch:  # Allow manual trigger

env:
  DEPLOY_PATH: /home/nut/loyalty-app  # Path on your server

jobs:
  deploy:
    runs-on: self-hosted  # Will run on your server
    environment: production  # Use production environment for secrets
    
    steps:
      - name: Clean workspace and checkout code
        run: |
          echo "üîß Preparing deployment workspace..."
          
          # Get workspace path
          WORKSPACE_DIR="${GITHUB_WORKSPACE:-$PWD}"
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          
          # Validate deployment path (no tilde paths in production)
          if [[ "$DEPLOY_DIR" == ~* ]]; then
            echo "‚ùå Error: Tilde paths not allowed in production deployment"
            echo "Current DEPLOY_DIR: $DEPLOY_DIR"
            echo "Use absolute paths like /home/nut/loyalty-app instead"
            exit 1
          fi
          
          echo "Workspace: $WORKSPACE_DIR"
          echo "Deploy Path: $DEPLOY_DIR"
          echo "Repository: ${{ github.repository }}"
          echo "Commit: ${{ github.sha }}"
          
          # Stop containers first
          echo "Stopping existing containers..."
          if [ -d "$DEPLOY_DIR" ]; then
            cd "$DEPLOY_DIR"
            docker compose down --remove-orphans 2>/dev/null || true
            docker compose -f docker-compose.yml -f docker-compose.prod.yml down --remove-orphans 2>/dev/null || true
          fi
          
          # Smart deployment: pull if git repo exists, clone if not
          if [ -d "$DEPLOY_DIR/.git" ]; then
            echo "üì• Updating existing repository..."
            cd "$DEPLOY_DIR"
            
            # Clean any local changes and reset to main
            git clean -fd 2>/dev/null || true
            git reset --hard HEAD 2>/dev/null || true
            git checkout main 2>/dev/null || git checkout -b main 2>/dev/null || true
            
            # Update remote and pull latest changes
            git remote set-url origin https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git
            git fetch --depth 1 origin main
            git reset --hard origin/main
            
            # Checkout specific commit
            git fetch origin ${{ github.sha }} --depth 1
            git checkout ${{ github.sha }}
            
            echo "‚úÖ Repository updated successfully"
          else
            echo "üì¶ Cloning repository (first deployment)..."
            
            # Ensure parent directory exists
            mkdir -p "$(dirname "$DEPLOY_DIR")"
            
            # Remove any non-git content if directory exists but isn't a git repo
            if [ -d "$DEPLOY_DIR" ]; then
              echo "Cleaning non-git directory..."
              rm -rf "$DEPLOY_DIR"/* 2>/dev/null || true
              rm -rf "$DEPLOY_DIR"/.* 2>/dev/null || true
            fi
            
            # Clone fresh repository
            git clone --depth 1 --branch main --single-branch \
              https://x-access-token:${{ secrets.GITHUB_TOKEN }}@github.com/${{ github.repository }}.git \
              "$DEPLOY_DIR"
            
            cd "$DEPLOY_DIR"
            
            # Checkout specific commit
            git fetch origin ${{ github.sha }} --depth 1
            git checkout ${{ github.sha }}
            
            echo "‚úÖ Repository cloned successfully"
          fi
          
          # Verify deployment
          echo "Verifying deployment files..."
          if [ ! -f "docker-compose.yml" ] || [ ! -f "docker-compose.prod.yml" ]; then
            echo "‚ùå Critical deployment files missing"
            ls -la
            exit 1
          fi
          
          echo "‚úÖ Code checkout completed successfully"
          echo "Current directory: $(pwd)"
          echo "Files in deployment directory:"
          ls -la

      - name: Workspace validation and setup
        run: |
          echo "üîç Post-checkout validation..."
          
          # Navigate to deployment directory
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Verify critical files exist
          if [ ! -f "docker-compose.yml" ] || [ ! -f "docker-compose.prod.yml" ]; then
            echo "‚ùå Critical deployment files missing"
            exit 1
          fi
          
          # Configure sudo authentication for workspace setup
          echo "Configuring secure sudo access..."
          echo "${{ secrets.SUDO_PASSWORD }}" | sudo -S echo "Sudo access configured" || {
            echo "‚ùå Failed to authenticate with sudo"
            exit 1
          }
          
          # Create necessary directories with proper permissions
          mkdir -p backups logs 2>/dev/null || true
          echo "${{ secrets.SUDO_PASSWORD }}" | sudo -S chmod 755 backups logs 2>/dev/null || true
          
          # Ensure proper ownership and permissions
          echo "${{ secrets.SUDO_PASSWORD }}" | sudo -S chown -R $(whoami):$(whoami) . 2>/dev/null || true
          
          # Test write permissions
          if ! touch test-write-permission 2>/dev/null; then
            echo "‚ùå Cannot write to deployment directory"
            exit 1
          else
            rm -f test-write-permission
          fi
          
          echo "‚úÖ Deployment validation completed"
          echo "Working in: $(pwd)"
          
      - name: Validate and set production environment variables
        run: |
          echo "üîß Setting up production environment..."
          
          # Navigate to deployment directory
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Validate critical secrets exist
          missing_secrets=()
          
          if [ -z "${{ secrets.JWT_SECRET }}" ]; then missing_secrets+=("JWT_SECRET"); fi
          if [ -z "${{ secrets.GOOGLE_CLIENT_ID }}" ]; then missing_secrets+=("GOOGLE_CLIENT_ID"); fi
          if [ -z "${{ secrets.FRONTEND_URL }}" ]; then missing_secrets+=("FRONTEND_URL"); fi
          if [ -z "${{ secrets.BACKEND_URL }}" ]; then missing_secrets+=("BACKEND_URL"); fi
          if [ -z "${{ secrets.VITE_API_URL }}" ]; then missing_secrets+=("VITE_API_URL"); fi
          if [ -z "${{ secrets.SUDO_PASSWORD }}" ]; then missing_secrets+=("SUDO_PASSWORD"); fi
          
          if [ ${#missing_secrets[@]} -ne 0 ]; then
            echo "‚ùå Missing critical secrets: ${missing_secrets[*]}"
            echo "Please configure these secrets in GitHub repository settings > Environments > production"
            exit 1
          fi
          
          # Export all environment variables
          echo "NODE_ENV=production" >> $GITHUB_ENV
          echo "LOG_LEVEL=info" >> $GITHUB_ENV
          echo "CORS_ORIGINS=https://loyalty.saichon.com" >> $GITHUB_ENV
          
          # Security secrets
          echo "JWT_SECRET=${{ secrets.JWT_SECRET }}" >> $GITHUB_ENV
          echo "JWT_REFRESH_SECRET=${{ secrets.JWT_REFRESH_SECRET }}" >> $GITHUB_ENV
          echo "SESSION_SECRET=${{ secrets.SESSION_SECRET }}" >> $GITHUB_ENV
          
          # Database
          echo "DATABASE_URL=${{ secrets.DATABASE_URL }}" >> $GITHUB_ENV
          echo "REDIS_URL=${{ secrets.REDIS_URL }}" >> $GITHUB_ENV
          
          # URLs (fallback to variables if secrets don't exist yet)
          echo "FRONTEND_URL=${{ secrets.FRONTEND_URL || vars.FRONTEND_URL }}" >> $GITHUB_ENV
          echo "BACKEND_URL=${{ secrets.BACKEND_URL || vars.BACKEND_URL }}" >> $GITHUB_ENV
          echo "VITE_API_URL=${{ secrets.VITE_API_URL || vars.BACKEND_URL }}" >> $GITHUB_ENV
          
          # OAuth
          echo "GOOGLE_CLIENT_ID=${{ secrets.GOOGLE_CLIENT_ID }}" >> $GITHUB_ENV
          echo "GOOGLE_CLIENT_SECRET=${{ secrets.GOOGLE_CLIENT_SECRET }}" >> $GITHUB_ENV
          echo "GOOGLE_CALLBACK_URL=${{ secrets.GOOGLE_CALLBACK_URL || vars.GOOGLE_CALLBACK_URL }}" >> $GITHUB_ENV
          echo "FACEBOOK_APP_ID=${{ secrets.FACEBOOK_APP_ID }}" >> $GITHUB_ENV
          echo "FACEBOOK_APP_SECRET=${{ secrets.FACEBOOK_APP_SECRET }}" >> $GITHUB_ENV
          echo "FACEBOOK_CALLBACK_URL=${{ secrets.FACEBOOK_CALLBACK_URL }}" >> $GITHUB_ENV
          echo "LINE_CHANNEL_ID=${{ secrets.LINE_CHANNEL_ID }}" >> $GITHUB_ENV
          echo "LINE_CHANNEL_SECRET=${{ secrets.LINE_CHANNEL_SECRET }}" >> $GITHUB_ENV
          echo "LINE_CALLBACK_URL=${{ secrets.LINE_CALLBACK_URL || vars.LINE_CALLBACK_URL }}" >> $GITHUB_ENV
          
          # Azure Translation
          echo "AZURE_TRANSLATION_TEXT_URI=${{ secrets.AZURE_TRANSLATION_TEXT_URI }}" >> $GITHUB_ENV
          echo "AZURE_TRANSLATION_KEY_1=${{ secrets.AZURE_TRANSLATION_KEY_1 }}" >> $GITHUB_ENV
          echo "AZURE_TRANSLATION_KEY_2=${{ secrets.AZURE_TRANSLATION_KEY_2 }}" >> $GITHUB_ENV
          echo "AZURE_TRANSLATION_REGION=${{ secrets.AZURE_TRANSLATION_REGION }}" >> $GITHUB_ENV
          
          # Admin credentials
          echo "LOYALTY_USERNAME=${{ secrets.LOYALTY_USERNAME }}" >> $GITHUB_ENV
          echo "LOYALTY_PASSWORD=${{ secrets.LOYALTY_PASSWORD }}" >> $GITHUB_ENV
          
          echo "‚úÖ Environment variables configured"

      - name: Stop existing containers with graceful shutdown
        run: |
          echo "üõë Gracefully stopping existing containers..."
          
          # Navigate to deployment directory
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Check if containers are running
          if docker compose -f docker-compose.yml -f docker-compose.prod.yml ps -q | head -1 | grep -q .; then
            echo "Containers found, initiating shutdown..."
            
            # Graceful shutdown with timeout
            timeout 30s docker compose -f docker-compose.yml -f docker-compose.prod.yml down --timeout 10 || {
              echo "‚ö†Ô∏è Graceful shutdown timed out, forcing stop..."
              docker compose -f docker-compose.yml -f docker-compose.prod.yml down --timeout 5 || true
            }
          else
            echo "No containers running"
          fi
          
          # Clean up any orphaned containers
          docker compose down --remove-orphans 2>/dev/null || true
          
          sleep 3
          echo "‚úÖ Container shutdown completed"

      - name: Intelligent database backup with retry logic
        run: |
          echo "üíæ Creating intelligent database backup..."
          
          # Navigate to deployment directory
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Create backups directory with proper permissions
          mkdir -p backups
          chmod 755 backups
          
          # Function to check if database is ready for backup
          check_database_ready() {
            local max_attempts=10
            local attempt=0
            
            while [ $attempt -lt $max_attempts ]; do
              # Check if postgres container exists and is running
              if docker ps --format "{{.Names}}" | grep -q "loyalty_postgres"; then
                # Check if database is ready to accept connections
                if docker exec loyalty_postgres pg_isready -U loyalty -d loyalty_db >/dev/null 2>&1; then
                  echo "‚úÖ Database is ready for backup"
                  return 0
                fi
              fi
              
              attempt=$((attempt + 1))
              echo "Waiting for database... (attempt $attempt/$max_attempts)"
              sleep 3
            done
            
            echo "‚ùå Database not ready for backup after ${max_attempts} attempts"
            return 1
          }
          
          # Function to create backup with detailed error handling
          create_backup() {
            local timestamp=$(date +%Y%m%d_%H%M%S)
            local backup_file="backups/backup_${timestamp}.sql"
            local temp_file="backups/.backup_${timestamp}.tmp"
            
            echo "Creating backup: $backup_file"
            
            # Create backup with detailed error reporting
            if docker exec loyalty_postgres pg_dump -U loyalty -d loyalty_db > "$temp_file" 2>&1; then
              # Validate backup content
              if [ -s "$temp_file" ] && head -5 "$temp_file" | grep -q "PostgreSQL database dump"; then
                mv "$temp_file" "$backup_file"
                local size=$(du -h "$backup_file" | cut -f1)
                echo "‚úÖ Database backup created successfully ($size)"
                echo "Backup contains $(wc -l < "$backup_file") lines"
                return 0
              else
                echo "‚ùå Backup validation failed - file is empty or corrupt"
                cat "$temp_file" | head -10  # Show first 10 lines for debugging
                rm -f "$temp_file"
                return 1
              fi
            else
              echo "‚ùå pg_dump command failed:"
              cat "$temp_file" | head -10  # Show error output
              rm -f "$temp_file"
              return 1
            fi
          }
          
          # Main backup logic with retry
          backup_success=false
          max_backup_attempts=3
          backup_attempt=0
          
          while [ $backup_attempt -lt $max_backup_attempts ] && [ "$backup_success" = false ]; do
            backup_attempt=$((backup_attempt + 1))
            echo "Backup attempt $backup_attempt/$max_backup_attempts"
            
            if check_database_ready; then
              if create_backup; then
                backup_success=true
                break
              fi
            fi
            
            if [ $backup_attempt -lt $max_backup_attempts ]; then
              echo "Retrying backup in 5 seconds..."
              sleep 5
            fi
          done
          
          if [ "$backup_success" = false ]; then
            echo "‚ö†Ô∏è Primary backup attempts failed - trying alternative backup methods"
            
            # Try file-system level backup if database container has data volume
            echo "Attempting volume-based backup as fallback..."
            if docker volume ls | grep -q postgres_data; then
              timestamp=$(date +%Y%m%d_%H%M%S)
              volume_backup="backups/volume_backup_${timestamp}.tar.gz"
              
              if docker run --rm -v postgres_data:/data -v "$PWD/backups:/backup" alpine tar czf "/backup/volume_backup_${timestamp}.tar.gz" -C /data .; then
                echo "‚úÖ Volume backup created successfully: $volume_backup"
                backup_success=true
              else
                echo "‚ùå Volume backup also failed"
              fi
            fi
            
            if [ "$backup_success" = false ]; then
              echo "‚ö†Ô∏è All backup methods failed - proceeding without backup"
              echo "This is not critical for deployment but should be investigated"
              
              # Show detailed diagnostics
              echo "Container status:"
              docker ps --format "table {{.Names}}\t{{.Status}}\t{{.Ports}}"
              
              echo "Available volumes:"
              docker volume ls | grep -E "(postgres|loyalty)"
              
              # Try basic database connection test
              if docker ps | grep -q loyalty_postgres; then
                echo "Testing database connection..."
                if docker exec loyalty_postgres psql -U loyalty -d loyalty_db -c "SELECT version();" 2>/dev/null; then
                  echo "‚úÖ Database connection works but backup failed"
                else
                  echo "‚ùå Database connection failed"
                fi
              else
                echo "‚ùå PostgreSQL container not running"
              fi
            fi
          fi
          
          # Cleanup old backups (keep last 10 for better history)
          echo "Cleaning up old backups (keeping last 10)..."
          if ls backups/backup_*.sql >/dev/null 2>&1; then
            ls -t backups/backup_*.sql | tail -n +11 | xargs rm -f 2>/dev/null || true
            echo "Backup cleanup completed. Current backups:"
            ls -la backups/backup_*.sql 2>/dev/null | tail -5 || echo "No backups found"
          else
            echo "No existing backups found"
          fi

      - name: Cache Node.js dependencies
        uses: actions/cache@v4
        with:
          path: |
            ~/.npm
            /tmp/.npm-cache
          key: ${{ runner.os }}-node-${{ hashFiles('**/package-lock.json') }}
          restore-keys: |
            ${{ runner.os }}-node-

      - name: Setup Node.js dependencies in parallel
        run: |
          echo "üì¶ Setting up dependencies in parallel..."
          
          # Navigate to deployment directory
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Configure sudo authentication once
          echo "Configuring secure sudo access..."
          echo "${{ secrets.SUDO_PASSWORD }}" | sudo -S echo "Sudo access configured" || {
            echo "‚ùå Failed to authenticate with sudo"
            exit 1
          }
          
          # Set up npm cache directory
          export NPM_CONFIG_CACHE=/tmp/.npm-cache
          mkdir -p /tmp/.npm-cache
          
          # Function to setup dependencies for a service
          setup_dependencies() {
            local service=$1
            local dir=$2
            
            echo "Setting up $service dependencies..."
            cd "$DEPLOY_DIR/$dir"
            
            # Fix permissions
            echo "${{ secrets.SUDO_PASSWORD }}" | sudo -S chown -R $(whoami):$(whoami) . 2>/dev/null || true
            echo "${{ secrets.SUDO_PASSWORD }}" | sudo -S chmod -R 755 . 2>/dev/null || true
            
            # Clean existing node_modules
            if [ -d "node_modules" ]; then
              echo "Cleaning existing $service node_modules..."
              echo "${{ secrets.SUDO_PASSWORD }}" | sudo -S rm -rf node_modules 2>/dev/null || true
            fi
            
            # Install dependencies with cache
            if [ -f "package-lock.json" ]; then
              echo "Installing $service dependencies with npm ci..."
              if [ "$service" = "backend" ]; then
                npm ci --include=dev --cache /tmp/.npm-cache
              else
                npm ci --cache /tmp/.npm-cache
              fi
            else
              echo "Installing $service dependencies with npm install..."
              if [ "$service" = "backend" ]; then
                npm install --include=dev --cache /tmp/.npm-cache
              else
                npm install --cache /tmp/.npm-cache
              fi
            fi
            
            echo "‚úÖ $service dependencies installed successfully"
          }
          
          # Run frontend and backend dependency installation in parallel
          setup_dependencies "frontend" "frontend" &
          FRONTEND_PID=$!
          
          setup_dependencies "backend" "backend" &
          BACKEND_PID=$!
          
          # Wait for both to complete
          echo "Waiting for parallel dependency installation to complete..."
          wait $FRONTEND_PID
          FRONTEND_EXIT=$?
          wait $BACKEND_PID  
          BACKEND_EXIT=$?
          
          # Check if both succeeded
          if [ $FRONTEND_EXIT -ne 0 ]; then
            echo "‚ùå Frontend dependency installation failed"
            exit 1
          fi
          
          if [ $BACKEND_EXIT -ne 0 ]; then
            echo "‚ùå Backend dependency installation failed"
            exit 1
          fi
          
          # Verify Prisma installation
          cd "$DEPLOY_DIR/backend"
          echo "Verifying Prisma CLI installation..."
          npx prisma --version
          
          echo "‚úÖ All dependencies installed successfully in parallel"

      - name: Setup Docker BuildKit and cache
        run: |
          echo "üîß Setting up Docker BuildKit with caching..."
          
          # Enable BuildKit for better caching and performance
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          
          # Create buildx builder with cache support if not exists
          if ! docker buildx ls | grep -q "ci-builder"; then
            docker buildx create --name ci-builder --use
          else
            docker buildx use ci-builder
          fi
          
          echo "‚úÖ Docker BuildKit setup completed"

      - name: Build containers with optimized caching
        run: |
          echo "üî® Building containers with optimized caching..."
          
          # Navigate to deployment directory
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Enable BuildKit
          export DOCKER_BUILDKIT=1
          export COMPOSE_DOCKER_CLI_BUILD=1
          
          # Validate docker-compose files exist
          if [ ! -f "docker-compose.yml" ] || [ ! -f "docker-compose.prod.yml" ]; then
            echo "‚ùå Required docker-compose files not found"
            exit 1
          fi
          
          # Validate configuration
          echo "Validating Docker Compose configuration..."
          docker compose -f docker-compose.yml -f docker-compose.prod.yml config > /dev/null
          
          # Selective cleanup - only if needed
          DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
          if [ "$DISK_USAGE" -gt 80 ]; then
            echo "High disk usage detected, cleaning up..."
            docker system prune -f || true
          fi
          
          # Pull base images in parallel
          echo "Pulling base images..."
          docker pull node:18-alpine &
          docker pull postgres:15-alpine &
          docker pull redis:7-alpine &
          docker pull nginx:alpine &
          wait
          
          # Build with cache optimization
          echo "Building containers with cache optimization..."
          docker compose -f docker-compose.yml -f docker-compose.prod.yml build \
            --build-arg BUILDKIT_INLINE_CACHE=1 \
            --parallel || {
            echo "‚ùå Container build failed"
            docker compose -f docker-compose.yml -f docker-compose.prod.yml logs --tail=50
            exit 1
          }
          
          echo "‚úÖ Container build completed successfully"

      - name: Optimized database migrations
        run: |
          echo "üìä Running optimized database migrations..."
          
          # Navigate to deployment directory
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Start containers with production config immediately
          echo "Starting containers with production configuration..."
          docker compose -f docker-compose.yml -f docker-compose.prod.yml up -d
          
          # Wait for database to be ready with optimized timing
          echo "Waiting for database to be ready..."
          max_attempts=20  # Reduced from 30
          attempt=0
          
          while [ $attempt -lt $max_attempts ]; do
            if docker compose exec -T postgres pg_isready -U loyalty -d loyalty_db >/dev/null 2>&1; then
              echo "‚úÖ Database is ready"
              break
            fi
            attempt=$((attempt + 1))
            [ $((attempt % 5)) -eq 0 ] && echo "Waiting for database... (attempt $attempt/$max_attempts)"
            sleep 2  # Reduced from 3
          done
          
          if [ $attempt -ge $max_attempts ]; then
            echo "‚ùå Database failed to become ready"
            docker compose ps postgres
            exit 1
          fi
          
          # Shorter initialization wait
          echo "Allowing database initialization..."
          sleep 5  # Reduced from 10
          
          # Navigate to backend directory
          cd backend
          
          # Generate Prisma client (if not cached)
          echo "Ensuring Prisma client is generated..."
          npm run db:generate
          
          # Run migrations through container network (faster than exposing ports)
          echo "Deploying Prisma migrations through container network..."
          if ! docker compose exec -T backend npm run db:migrate:deploy 2>&1; then
            echo "Migration deployment failed, checking for P3005 baseline scenario..."
            
            # Check if the error is P3005 (database schema not empty)
            if docker compose exec -T backend npm run db:migrate:deploy 2>&1 | grep -q "P3005"; then
              echo "P3005 detected - performing baseline..."
              
              # Baseline through container
              docker compose exec -T backend npx prisma migrate resolve --applied 0_init
              echo "‚úÖ Database baseline completed"
              
              # Retry migration
              docker compose exec -T backend npm run db:migrate:deploy
            else
              echo "‚ùå Migration failed with non-P3005 error"
              exit 1
            fi
          else
            echo "‚úÖ Migration deployment completed successfully"
          fi
          
          # Test database connection through container
          echo "Testing database connection..."
          docker compose exec -T backend npx prisma db pull --print > /dev/null || {
            echo "‚ö†Ô∏è Database connection test failed, but migrations completed"
          }
          
          # Quick container health check
          echo "Verifying container health..."
          sleep 5  # Reduced from 15
          
          echo "‚úÖ Database migrations completed successfully"

      - name: Optimized parallel health checks
        run: |
          echo "üè• Running optimized health checks in parallel..."
          
          # Navigate to deployment directory
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Function to check service health with faster timeouts
          check_service_health() {
            local service=$1
            local url=$2
            local max_attempts=15  # Reduced from 30
            local attempt=0
            
            echo "Checking $service health at $url"
            
            while [ $attempt -lt $max_attempts ]; do
              if curl -f -s --max-time 3 --connect-timeout 2 "$url" > /dev/null 2>&1; then
                echo "‚úÖ $service is healthy!"
                return 0
              fi
              
              attempt=$((attempt + 1))
              [ $((attempt % 5)) -eq 0 ] && echo "Waiting for $service... (attempt $attempt/$max_attempts)"
              sleep 2  # Reduced from 3
            done
            
            echo "‚ùå $service health check failed!"
            return 1
          }
          
          # Run health checks in parallel
          echo "Starting parallel health checks..."
          
          check_service_health "Application" "http://localhost:4001/api/health" &
          APP_PID=$!
          
          check_service_health "Frontend" "http://localhost:4001/" &
          FRONTEND_PID=$!
          
          # Wait for both health checks to complete
          wait $APP_PID
          APP_EXIT=$?
          wait $FRONTEND_PID
          FRONTEND_EXIT=$?
          
          # Collect failed services
          failed_services=()
          if [ $APP_EXIT -ne 0 ]; then
            failed_services+=("Application")
          fi
          if [ $FRONTEND_EXIT -ne 0 ]; then
            failed_services+=("Frontend")
          fi
          
          # If health checks failed, show diagnostic information
          if [ ${#failed_services[@]} -ne 0 ]; then
            echo "‚ùå Health checks failed for: ${failed_services[*]}"
            echo "\nüìä Diagnostic Information:"
            
            echo "\nüê≥ Container Status:"
            docker compose -f docker-compose.yml -f docker-compose.prod.yml ps
            
            echo "\nüìã Backend Logs (last 20 lines):"  # Reduced from 30
            docker compose -f docker-compose.yml -f docker-compose.prod.yml logs --tail=20 backend || true
            
            echo "\nüìã Nginx Logs (last 15 lines):"  # Reduced from 20
            docker compose -f docker-compose.yml -f docker-compose.prod.yml logs --tail=15 nginx || true
            
            echo "\nüîç Port Status:"
            ss -tlnp | grep :4001 || netstat -tlnp | grep :4001 || echo "Port 4001 not listening"
            
            exit 1
          fi
          
          echo "‚úÖ All health checks passed successfully!"

      - name: Smart conditional cleanup
        run: |
          echo "üßπ Smart conditional cleanup..."
          
          # Navigate to deployment directory
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          # Check current disk usage
          DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
          echo "Current disk usage: ${DISK_USAGE}%"
          
          # Only run cleanup if disk usage is above threshold
          if [ "$DISK_USAGE" -gt 75 ]; then
            echo "Disk usage above 75%, running cleanup..."
            
            # Clean up unused Docker resources
            echo "Cleaning Docker images..."
            docker image prune -f || true
            
            echo "Cleaning unused volumes (keeping data volumes)..."
            docker volume prune -f || true
            
            echo "Cleaning build cache..."
            docker builder prune -f || true
            
            echo "Cleaning networks..."
            docker network prune -f || true
            
            # Show disk usage after cleanup
            NEW_DISK_USAGE=$(df / | tail -1 | awk '{print $5}' | sed 's/%//')
            echo "Disk usage after cleanup: ${NEW_DISK_USAGE}%"
            echo "Space freed: $((DISK_USAGE - NEW_DISK_USAGE))%"
          else
            echo "Disk usage below 75%, skipping cleanup to save time"
            
            # Just clean old/unused containers for safety
            docker container prune -f || true
          fi
          
          # Show final disk usage
          echo "\nüíæ Final disk usage:"
          df -h . | head -2
          
          echo "‚úÖ Smart cleanup completed"
          
      - name: Deployment summary and monitoring setup
        if: always()
        run: |
          echo "=============================="
          echo "üìã Deployment Status Report"
          echo "=============================="
          echo "Repository: ${{ github.repository }}"
          echo "Branch: ${{ github.ref_name }}"
          echo "Commit: ${{ github.sha }}"
          echo "Commit Message: ${{ github.event.head_commit.message }}"
          echo "Actor: ${{ github.actor }}"
          echo "Workflow: ${{ github.workflow }}"
          echo "Run ID: ${{ github.run_id }}"
          echo "Timestamp: $(date -Iseconds)"
          echo "=============================="
          
          # Navigate to deployment directory
          DEPLOY_DIR="${DEPLOY_PATH:-/home/nut/loyalty-app}"
          cd "$DEPLOY_DIR"
          
          echo "\nüê≥ Container Status:"
          if docker compose -f docker-compose.yml -f docker-compose.prod.yml ps 2>/dev/null; then
            echo "\nüìä Resource Usage:"
            docker stats --no-stream --format "table {{.Container}}\t{{.CPUPerc}}\t{{.MemUsage}}" || true
          else
            echo "‚ùå Could not retrieve container status"
          fi
          
          echo "\nüåê Service Endpoints:"
          echo "Main Application: https://loyalty.saichon.com"
          echo "Health Check: https://loyalty.saichon.com/api/health"
          echo "Admin Panel: https://loyalty.saichon.com/admin"
          
          # Test final connectivity
          echo "\nüîç Final Connectivity Test:"
          if curl -f -s --max-time 10 "http://localhost:4001/api/health" > /dev/null; then
            echo "‚úÖ Application is responding"
          else
            echo "‚ùå Application is not responding"
          fi
          
          echo "\n‚úÖ Deployment completed at $(date)"
          echo "=============================="